{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：<a href=\"https://qiita.com/inoory/items/e63ade6f21766c7c2393\">[Python] Keras-RLで簡単に強化学習(DQN)を試す</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "\n",
    "# 直線上を動く点の速度を操作し、目標(原点)に移動させることを目標とする環境\n",
    "class PointOnLine(gym.core.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.Discrete(3) # 行動空間。速度を下げる、そのまま、上げるの3種\n",
    "\n",
    "        high = np.array([1.0, 1.0]) # 観測空間(state)の次元 (位置と速度の2次元) とそれらの最大値\n",
    "        self.observation_space = gym.spaces.Box(low=-high, high=high) # 最小値は、最大値のマイナスがけ\n",
    "\n",
    "    # 各stepごとに呼ばれる\n",
    "    # actionを受け取り、次のstateとreward、episodeが終了したかどうかを返すように実装\n",
    "    def _step(self, action):\n",
    "        # actionを受け取り、次のstateを決定\n",
    "        dt = 0.1\n",
    "        acc = (action - 1) * 0.1\n",
    "        self._vel += acc * dt\n",
    "        self._vel = max(-1.0,  min(self._vel, 1.0))\n",
    "        self._pos += self._vel * dt\n",
    "        self._pos = max(-1.0,  min(self._pos, 1.0))\n",
    "\n",
    "        # 位置と速度の絶対値が十分小さくなったらepisode終了\n",
    "        done = abs(self._pos) < 0.1 and abs(self._vel) < 0.1\n",
    "\n",
    "        if done:\n",
    "            # 終了したときに正の報酬\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            # 時間経過ごとに負の報酬\n",
    "            # ゴールに近づくように、距離が近くなるほど絶対値を減らしておくと、学習が早く進む\n",
    "            reward = -0.01 * abs(self._pos)\n",
    "\n",
    "        # 次のstate、reward、終了したかどうか、追加情報の順に返す\n",
    "        # 追加情報は特にないので空dict\n",
    "        return np.array([self._pos, self._vel]), reward, done, {}\n",
    "\n",
    "    # 各episodeの開始時に呼ばれ、初期stateを返すように実装\n",
    "    def _reset(self):\n",
    "        # 初期stateは、位置はランダム、速度ゼロ\n",
    "        self._pos = np.random.rand()*2 - 1\n",
    "        self._vel = 0.0\n",
    "        return np.array([self._pos, self._vel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 643\n",
      "Trainable params: 643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n",
      "   300/50000: episode: 1, duration: 3.577s, episode steps: 300, steps per second: 84, episode reward: -2.430, mean reward: -0.008 [-0.010, -0.000], mean action: 1.067 [0.000, 2.000], mean observation: 0.458 [-0.479, 1.000], loss: 0.000188, mean_absolute_error: 0.054143, mean_q: -0.058164\n",
      "   600/50000: episode: 2, duration: 1.123s, episode steps: 300, steps per second: 267, episode reward: -1.378, mean reward: -0.005 [-0.010, -0.000], mean action: 1.087 [0.000, 2.000], mean observation: -0.006 [-0.623, 1.000], loss: 0.000028, mean_absolute_error: 0.055728, mean_q: -0.075482\n",
      "   900/50000: episode: 3, duration: 1.780s, episode steps: 300, steps per second: 169, episode reward: -2.423, mean reward: -0.008 [-0.010, -0.000], mean action: 1.080 [0.000, 2.000], mean observation: 0.392 [-0.951, 1.000], loss: 0.000019, mean_absolute_error: 0.062748, mean_q: -0.088152\n",
      "  1200/50000: episode: 4, duration: 1.819s, episode steps: 300, steps per second: 165, episode reward: -1.734, mean reward: -0.006 [-0.010, -0.000], mean action: 1.077 [0.000, 2.000], mean observation: 0.204 [-0.600, 1.000], loss: 0.000038, mean_absolute_error: 0.072205, mean_q: -0.102693\n",
      "  1309/50000: episode: 5, duration: 0.409s, episode steps: 109, steps per second: 267, episode reward: 0.627, mean reward: 0.006 [-0.005, 1.000], mean action: 0.945 [0.000, 1.000], mean observation: 0.153 [-0.060, 0.505], loss: 0.000043, mean_absolute_error: 0.077630, mean_q: -0.110215\n",
      "  1395/50000: episode: 6, duration: 0.252s, episode steps: 86, steps per second: 342, episode reward: 0.755, mean reward: 0.009 [-0.004, 1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.124 [-0.417, 0.090], loss: 0.000259, mean_absolute_error: 0.078227, mean_q: -0.109766\n",
      "  1695/50000: episode: 7, duration: 0.976s, episode steps: 300, steps per second: 307, episode reward: -2.632, mean reward: -0.009 [-0.010, -0.000], mean action: 1.110 [0.000, 2.000], mean observation: 0.492 [-0.789, 1.000], loss: 0.000913, mean_absolute_error: 0.082293, mean_q: -0.112338\n",
      "  1995/50000: episode: 8, duration: 1.157s, episode steps: 300, steps per second: 259, episode reward: -2.395, mean reward: -0.008 [-0.010, -0.000], mean action: 1.100 [0.000, 2.000], mean observation: 0.495 [-0.269, 1.000], loss: 0.000733, mean_absolute_error: 0.091130, mean_q: -0.124167\n",
      "  2295/50000: episode: 9, duration: 0.941s, episode steps: 300, steps per second: 319, episode reward: -2.624, mean reward: -0.009 [-0.010, -0.000], mean action: 1.120 [0.000, 2.000], mean observation: 0.537 [-0.664, 1.000], loss: 0.000336, mean_absolute_error: 0.101694, mean_q: -0.141125\n",
      "  2559/50000: episode: 10, duration: 0.809s, episode steps: 264, steps per second: 326, episode reward: 0.409, mean reward: 0.002 [-0.003, 1.000], mean action: 0.989 [0.000, 2.000], mean observation: 0.108 [-0.030, 0.309], loss: 0.000413, mean_absolute_error: 0.108473, mean_q: -0.147740\n",
      "  2859/50000: episode: 11, duration: 1.329s, episode steps: 300, steps per second: 226, episode reward: -2.566, mean reward: -0.009 [-0.010, -0.000], mean action: 1.110 [0.000, 2.000], mean observation: 0.401 [-0.923, 1.000], loss: 0.000583, mean_absolute_error: 0.111931, mean_q: -0.146326\n",
      "  3159/50000: episode: 12, duration: 1.253s, episode steps: 300, steps per second: 239, episode reward: -2.551, mean reward: -0.009 [-0.010, -0.000], mean action: 1.113 [0.000, 2.000], mean observation: 0.454 [-0.966, 1.000], loss: 0.000464, mean_absolute_error: 0.123328, mean_q: -0.160805\n",
      "  3160/50000: episode: 13, duration: 0.007s, episode steps: 1, steps per second: 146, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.008 [0.000, 0.017], loss: 0.000030, mean_absolute_error: 0.131821, mean_q: -0.170150\n",
      "  3460/50000: episode: 14, duration: 1.068s, episode steps: 300, steps per second: 281, episode reward: -2.012, mean reward: -0.007 [-0.010, -0.000], mean action: 1.080 [0.000, 2.000], mean observation: 0.381 [-0.266, 1.000], loss: 0.000408, mean_absolute_error: 0.133267, mean_q: -0.168472\n",
      "  3760/50000: episode: 15, duration: 0.906s, episode steps: 300, steps per second: 331, episode reward: -2.574, mean reward: -0.009 [-0.010, -0.000], mean action: 1.127 [0.000, 2.000], mean observation: 0.442 [-0.707, 1.000], loss: 0.000310, mean_absolute_error: 0.140893, mean_q: -0.173148\n",
      "  3948/50000: episode: 16, duration: 0.586s, episode steps: 188, steps per second: 321, episode reward: 0.538, mean reward: 0.003 [-0.007, 1.000], mean action: 1.016 [0.000, 2.000], mean observation: -0.006 [-0.182, 0.726], loss: 0.000176, mean_absolute_error: 0.148921, mean_q: -0.180230\n",
      "  4003/50000: episode: 17, duration: 0.167s, episode steps: 55, steps per second: 330, episode reward: 0.775, mean reward: 0.014 [-0.008, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.118 [-0.230, 0.793], loss: 0.000499, mean_absolute_error: 0.149523, mean_q: -0.175080\n",
      "  4303/50000: episode: 18, duration: 1.131s, episode steps: 300, steps per second: 265, episode reward: -2.625, mean reward: -0.009 [-0.010, -0.000], mean action: 1.080 [0.000, 2.000], mean observation: 0.514 [-0.706, 1.000], loss: 0.000561, mean_absolute_error: 0.157166, mean_q: -0.166917\n",
      "  4351/50000: episode: 19, duration: 0.166s, episode steps: 48, steps per second: 289, episode reward: 0.817, mean reward: 0.017 [-0.007, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.118 [-0.210, 0.691], loss: 0.000387, mean_absolute_error: 0.161170, mean_q: -0.165851\n",
      "  4651/50000: episode: 20, duration: 1.125s, episode steps: 300, steps per second: 267, episode reward: -2.584, mean reward: -0.009 [-0.010, -0.000], mean action: 1.010 [0.000, 2.000], mean observation: 0.536 [-0.280, 1.000], loss: 0.000604, mean_absolute_error: 0.175039, mean_q: -0.160927\n",
      "  4699/50000: episode: 21, duration: 0.264s, episode steps: 48, steps per second: 182, episode reward: 0.813, mean reward: 0.017 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.145 [-0.590, 0.170], loss: 0.000671, mean_absolute_error: 0.185904, mean_q: -0.174859\n",
      "  4803/50000: episode: 22, duration: 0.441s, episode steps: 104, steps per second: 236, episode reward: 0.622, mean reward: 0.006 [-0.009, 1.000], mean action: 0.923 [0.000, 2.000], mean observation: -0.048 [-0.893, 0.263], loss: 0.000441, mean_absolute_error: 0.188327, mean_q: -0.153622\n",
      "  4860/50000: episode: 23, duration: 0.200s, episode steps: 57, steps per second: 285, episode reward: 0.705, mean reward: 0.012 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.187 [-0.240, 0.887], loss: 0.000648, mean_absolute_error: 0.190252, mean_q: -0.154562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4861/50000: episode: 24, duration: 0.007s, episode steps: 1, steps per second: 149, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.020 [0.000, 0.039], loss: 0.000077, mean_absolute_error: 0.176891, mean_q: -0.190569\n",
      "  4912/50000: episode: 25, duration: 0.235s, episode steps: 51, steps per second: 217, episode reward: 0.728, mean reward: 0.014 [-0.009, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.188 [-0.280, 0.893], loss: 0.000334, mean_absolute_error: 0.190175, mean_q: -0.139767\n",
      "  4961/50000: episode: 26, duration: 0.244s, episode steps: 49, steps per second: 201, episode reward: 0.783, mean reward: 0.016 [-0.007, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.157 [-0.220, 0.730], loss: 0.000467, mean_absolute_error: 0.187123, mean_q: -0.142160\n",
      "  5011/50000: episode: 27, duration: 0.239s, episode steps: 50, steps per second: 210, episode reward: 0.757, mean reward: 0.015 [-0.008, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.172 [-0.240, 0.817], loss: 0.000616, mean_absolute_error: 0.190779, mean_q: -0.121331\n",
      "  5065/50000: episode: 28, duration: 0.217s, episode steps: 54, steps per second: 249, episode reward: 0.795, mean reward: 0.015 [-0.007, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.129 [-0.685, 0.190], loss: 0.000906, mean_absolute_error: 0.197137, mean_q: -0.128009\n",
      "  5138/50000: episode: 29, duration: 0.248s, episode steps: 73, steps per second: 294, episode reward: 0.698, mean reward: 0.010 [-0.006, 1.000], mean action: 1.096 [0.000, 2.000], mean observation: -0.172 [-0.614, 0.170], loss: 0.000556, mean_absolute_error: 0.201715, mean_q: -0.127748\n",
      "  5181/50000: episode: 30, duration: 0.134s, episode steps: 43, steps per second: 322, episode reward: 0.877, mean reward: 0.020 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.089 [-0.150, 0.502], loss: 0.000480, mean_absolute_error: 0.199505, mean_q: -0.116617\n",
      "  5248/50000: episode: 31, duration: 0.264s, episode steps: 67, steps per second: 254, episode reward: 0.713, mean reward: 0.011 [-0.008, 1.000], mean action: 1.104 [0.000, 2.000], mean observation: -0.165 [-0.763, 0.170], loss: 0.000615, mean_absolute_error: 0.204102, mean_q: -0.094504\n",
      "  5426/50000: episode: 32, duration: 0.620s, episode steps: 178, steps per second: 287, episode reward: -0.349, mean reward: -0.002 [-0.010, 1.000], mean action: 1.045 [0.000, 2.000], mean observation: -0.361 [-1.000, 0.170], loss: 0.000628, mean_absolute_error: 0.209337, mean_q: -0.096324\n",
      "  5566/50000: episode: 33, duration: 0.421s, episode steps: 140, steps per second: 333, episode reward: 0.348, mean reward: 0.002 [-0.006, 1.000], mean action: 1.057 [0.000, 2.000], mean observation: -0.217 [-0.629, 0.140], loss: 0.000320, mean_absolute_error: 0.220439, mean_q: -0.085136\n",
      "  5621/50000: episode: 34, duration: 0.232s, episode steps: 55, steps per second: 237, episode reward: 0.835, mean reward: 0.015 [-0.005, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.113 [-0.514, 0.120], loss: 0.000467, mean_absolute_error: 0.222116, mean_q: -0.080059\n",
      "  5663/50000: episode: 35, duration: 0.141s, episode steps: 42, steps per second: 299, episode reward: 0.900, mean reward: 0.021 [-0.004, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.087 [-0.374, 0.090], loss: 0.000534, mean_absolute_error: 0.226007, mean_q: -0.067647\n",
      "  5688/50000: episode: 36, duration: 0.088s, episode steps: 25, steps per second: 283, episode reward: 0.961, mean reward: 0.038 [-0.002, 1.000], mean action: 0.720 [0.000, 1.000], mean observation: 0.056 [-0.070, 0.209], loss: 0.000811, mean_absolute_error: 0.230548, mean_q: -0.074594\n",
      "  5786/50000: episode: 37, duration: 0.284s, episode steps: 98, steps per second: 345, episode reward: 0.543, mean reward: 0.006 [-0.008, 1.000], mean action: 1.071 [0.000, 2.000], mean observation: -0.197 [-0.806, 0.120], loss: 0.000457, mean_absolute_error: 0.232035, mean_q: -0.050779\n",
      "  5787/50000: episode: 38, duration: 0.008s, episode steps: 1, steps per second: 132, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.043 [-0.086, 0.000], loss: 0.000034, mean_absolute_error: 0.243097, mean_q: -0.056274\n",
      "  5819/50000: episode: 39, duration: 0.126s, episode steps: 32, steps per second: 254, episode reward: 0.955, mean reward: 0.030 [-0.002, 1.000], mean action: 1.125 [1.000, 2.000], mean observation: -0.058 [-0.191, 0.040], loss: 0.000287, mean_absolute_error: 0.241909, mean_q: -0.054327\n",
      "  5881/50000: episode: 40, duration: 0.189s, episode steps: 62, steps per second: 328, episode reward: 0.659, mean reward: 0.011 [-0.010, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.206 [-0.250, 0.955], loss: 0.000432, mean_absolute_error: 0.239180, mean_q: -0.060075\n",
      "  5882/50000: episode: 41, duration: 0.006s, episode steps: 1, steps per second: 164, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.027 [0.000, 0.054], loss: 0.000044, mean_absolute_error: 0.226890, mean_q: 0.085696\n",
      "  5934/50000: episode: 42, duration: 0.261s, episode steps: 52, steps per second: 199, episode reward: 0.932, mean reward: 0.018 [-0.001, 1.000], mean action: 1.038 [0.000, 2.000], mean observation: -0.062 [-0.141, 0.020], loss: 0.000323, mean_absolute_error: 0.239234, mean_q: -0.053617\n",
      "  5970/50000: episode: 43, duration: 0.159s, episode steps: 36, steps per second: 227, episode reward: 0.935, mean reward: 0.026 [-0.003, 1.000], mean action: 1.139 [0.000, 2.000], mean observation: -0.068 [-0.270, 0.070], loss: 0.000559, mean_absolute_error: 0.248377, mean_q: -0.015236\n",
      "  5986/50000: episode: 44, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 0.981, mean reward: 0.061 [-0.001, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: 0.047 [-0.040, 0.146], loss: 0.000624, mean_absolute_error: 0.256678, mean_q: -0.015944\n",
      "  6116/50000: episode: 45, duration: 0.459s, episode steps: 130, steps per second: 283, episode reward: 0.421, mean reward: 0.003 [-0.010, 1.000], mean action: 1.023 [0.000, 2.000], mean observation: -0.189 [-0.973, 0.150], loss: 0.000411, mean_absolute_error: 0.250322, mean_q: -0.009917\n",
      "  6117/50000: episode: 46, duration: 0.007s, episode steps: 1, steps per second: 143, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.017 [0.000, 0.034], loss: 0.000037, mean_absolute_error: 0.253047, mean_q: -0.052908\n",
      "  6136/50000: episode: 47, duration: 0.063s, episode steps: 19, steps per second: 300, episode reward: 0.976, mean reward: 0.051 [-0.002, 1.000], mean action: 1.211 [1.000, 2.000], mean observation: -0.049 [-0.163, 0.040], loss: 0.000282, mean_absolute_error: 0.245976, mean_q: -0.034472\n",
      "  6173/50000: episode: 48, duration: 0.155s, episode steps: 37, steps per second: 238, episode reward: 0.893, mean reward: 0.024 [-0.005, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.097 [-0.130, 0.462], loss: 0.000388, mean_absolute_error: 0.254421, mean_q: 0.015153\n",
      "  6208/50000: episode: 49, duration: 0.252s, episode steps: 35, steps per second: 139, episode reward: 0.900, mean reward: 0.026 [-0.005, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.094 [-0.140, 0.452], loss: 0.000220, mean_absolute_error: 0.264397, mean_q: 0.035189\n",
      "  6209/50000: episode: 50, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.007 [0.000, 0.013], loss: 0.003963, mean_absolute_error: 0.277117, mean_q: 0.009088\n",
      "  6291/50000: episode: 51, duration: 0.419s, episode steps: 82, steps per second: 196, episode reward: 0.694, mean reward: 0.008 [-0.007, 1.000], mean action: 1.085 [0.000, 2.000], mean observation: -0.152 [-0.678, 0.130], loss: 0.000377, mean_absolute_error: 0.265765, mean_q: 0.006420\n",
      "  6369/50000: episode: 52, duration: 0.269s, episode steps: 78, steps per second: 290, episode reward: 0.751, mean reward: 0.010 [-0.006, 1.000], mean action: 1.064 [0.000, 2.000], mean observation: -0.128 [-0.591, 0.110], loss: 0.000263, mean_absolute_error: 0.269026, mean_q: 0.042770\n",
      "  6404/50000: episode: 53, duration: 0.107s, episode steps: 35, steps per second: 328, episode reward: 0.921, mean reward: 0.026 [-0.003, 1.000], mean action: 0.771 [0.000, 1.000], mean observation: 0.078 [-0.080, 0.344], loss: 0.000238, mean_absolute_error: 0.264835, mean_q: 0.054551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6448/50000: episode: 54, duration: 0.127s, episode steps: 44, steps per second: 346, episode reward: 0.874, mean reward: 0.020 [-0.005, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.102 [-0.110, 0.461], loss: 0.000202, mean_absolute_error: 0.272792, mean_q: 0.035074\n",
      "  6480/50000: episode: 55, duration: 0.134s, episode steps: 32, steps per second: 239, episode reward: 0.942, mean reward: 0.029 [-0.003, 1.000], mean action: 1.219 [0.000, 2.000], mean observation: -0.067 [-0.253, 0.070], loss: 0.000720, mean_absolute_error: 0.275441, mean_q: 0.047237\n",
      "  6481/50000: episode: 56, duration: 0.008s, episode steps: 1, steps per second: 124, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.006 [0.000, 0.011], loss: 0.000059, mean_absolute_error: 0.248942, mean_q: -0.003999\n",
      "  6519/50000: episode: 57, duration: 0.146s, episode steps: 38, steps per second: 260, episode reward: 0.900, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.087 [-0.120, 0.425], loss: 0.000145, mean_absolute_error: 0.280044, mean_q: 0.050408\n",
      "  6591/50000: episode: 58, duration: 0.283s, episode steps: 72, steps per second: 255, episode reward: 0.787, mean reward: 0.011 [-0.005, 1.000], mean action: 1.069 [0.000, 2.000], mean observation: -0.119 [-0.524, 0.100], loss: 0.000518, mean_absolute_error: 0.280394, mean_q: 0.047958\n",
      "  6686/50000: episode: 59, duration: 0.422s, episode steps: 95, steps per second: 225, episode reward: 0.622, mean reward: 0.007 [-0.008, 1.000], mean action: 1.053 [0.000, 2.000], mean observation: -0.161 [-0.819, 0.140], loss: 0.000357, mean_absolute_error: 0.287218, mean_q: 0.078657\n",
      "  6687/50000: episode: 60, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.012 [-0.023, 0.000], loss: 0.000065, mean_absolute_error: 0.288311, mean_q: 0.059702\n",
      "  6760/50000: episode: 61, duration: 0.387s, episode steps: 73, steps per second: 189, episode reward: 0.624, mean reward: 0.009 [-0.009, 1.000], mean action: 0.904 [0.000, 2.000], mean observation: 0.203 [-0.160, 0.906], loss: 0.000253, mean_absolute_error: 0.288447, mean_q: 0.074302\n",
      "  6819/50000: episode: 62, duration: 0.279s, episode steps: 59, steps per second: 212, episode reward: 0.758, mean reward: 0.013 [-0.007, 1.000], mean action: 0.898 [0.000, 2.000], mean observation: 0.151 [-0.170, 0.742], loss: 0.000283, mean_absolute_error: 0.295059, mean_q: 0.082197\n",
      "  6873/50000: episode: 63, duration: 0.239s, episode steps: 54, steps per second: 226, episode reward: 0.936, mean reward: 0.017 [-0.001, 1.000], mean action: 0.963 [0.000, 2.000], mean observation: 0.058 [-0.020, 0.133], loss: 0.000408, mean_absolute_error: 0.299986, mean_q: 0.087251\n",
      "  6874/50000: episode: 64, duration: 0.007s, episode steps: 1, steps per second: 141, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.017 [0.000, 0.035], loss: 0.000068, mean_absolute_error: 0.308130, mean_q: 0.051161\n",
      "  6930/50000: episode: 65, duration: 0.259s, episode steps: 56, steps per second: 216, episode reward: 0.855, mean reward: 0.015 [-0.004, 1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.102 [-0.413, 0.090], loss: 0.000375, mean_absolute_error: 0.305505, mean_q: 0.134762\n",
      "  7034/50000: episode: 66, duration: 0.344s, episode steps: 104, steps per second: 303, episode reward: 0.432, mean reward: 0.004 [-0.010, 1.000], mean action: 1.058 [0.000, 2.000], mean observation: -0.230 [-0.998, 0.150], loss: 0.000187, mean_absolute_error: 0.302162, mean_q: 0.114503\n",
      "  7092/50000: episode: 67, duration: 0.183s, episode steps: 58, steps per second: 317, episode reward: 0.840, mean reward: 0.014 [-0.005, 1.000], mean action: 0.931 [0.000, 2.000], mean observation: 0.104 [-0.110, 0.507], loss: 0.000193, mean_absolute_error: 0.304759, mean_q: 0.108480\n",
      "  7170/50000: episode: 68, duration: 0.247s, episode steps: 78, steps per second: 316, episode reward: 0.589, mean reward: 0.008 [-0.010, 1.000], mean action: 0.923 [0.000, 2.000], mean observation: 0.209 [-0.150, 0.961], loss: 0.000154, mean_absolute_error: 0.307986, mean_q: 0.119390\n",
      "  7234/50000: episode: 69, duration: 0.190s, episode steps: 64, steps per second: 336, episode reward: 0.831, mean reward: 0.013 [-0.005, 1.000], mean action: 0.953 [0.000, 2.000], mean observation: 0.102 [-0.110, 0.493], loss: 0.000327, mean_absolute_error: 0.316106, mean_q: 0.149035\n",
      "  7300/50000: episode: 70, duration: 0.261s, episode steps: 66, steps per second: 253, episode reward: 0.788, mean reward: 0.012 [-0.006, 1.000], mean action: 0.924 [0.000, 2.000], mean observation: 0.125 [-0.110, 0.580], loss: 0.000469, mean_absolute_error: 0.310885, mean_q: 0.132524\n",
      "  7377/50000: episode: 71, duration: 0.215s, episode steps: 77, steps per second: 359, episode reward: 0.658, mean reward: 0.009 [-0.008, 1.000], mean action: 0.935 [0.000, 2.000], mean observation: 0.176 [-0.140, 0.806], loss: 0.000244, mean_absolute_error: 0.316663, mean_q: 0.151837\n",
      "  7402/50000: episode: 72, duration: 0.076s, episode steps: 25, steps per second: 327, episode reward: 0.962, mean reward: 0.038 [-0.002, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.056 [-0.205, 0.050], loss: 0.000316, mean_absolute_error: 0.317985, mean_q: 0.165896\n",
      "  7484/50000: episode: 73, duration: 0.248s, episode steps: 82, steps per second: 331, episode reward: 0.671, mean reward: 0.008 [-0.008, 1.000], mean action: 1.085 [0.000, 2.000], mean observation: -0.160 [-0.766, 0.120], loss: 0.000147, mean_absolute_error: 0.322172, mean_q: 0.158161\n",
      "  7516/50000: episode: 74, duration: 0.098s, episode steps: 32, steps per second: 327, episode reward: 0.933, mean reward: 0.029 [-0.003, 1.000], mean action: 0.781 [0.000, 2.000], mean observation: 0.071 [-0.090, 0.319], loss: 0.000264, mean_absolute_error: 0.326168, mean_q: 0.180261\n",
      "  7616/50000: episode: 75, duration: 0.412s, episode steps: 100, steps per second: 243, episode reward: 0.497, mean reward: 0.005 [-0.009, 1.000], mean action: 0.950 [0.000, 2.000], mean observation: 0.211 [-0.150, 0.913], loss: 0.000306, mean_absolute_error: 0.323049, mean_q: 0.165354\n",
      "  7648/50000: episode: 76, duration: 0.145s, episode steps: 32, steps per second: 221, episode reward: 0.951, mean reward: 0.030 [-0.002, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.059 [-0.060, 0.218], loss: 0.000151, mean_absolute_error: 0.320943, mean_q: 0.176368\n",
      "  7702/50000: episode: 77, duration: 0.276s, episode steps: 54, steps per second: 195, episode reward: 0.898, mean reward: 0.017 [-0.003, 1.000], mean action: 0.963 [0.000, 2.000], mean observation: 0.074 [-0.070, 0.327], loss: 0.000119, mean_absolute_error: 0.327129, mean_q: 0.191970\n",
      "  7721/50000: episode: 78, duration: 0.137s, episode steps: 19, steps per second: 139, episode reward: 0.977, mean reward: 0.051 [-0.001, 1.000], mean action: 1.158 [1.000, 2.000], mean observation: -0.049 [-0.147, 0.030], loss: 0.000594, mean_absolute_error: 0.340765, mean_q: 0.189394\n",
      "  7774/50000: episode: 79, duration: 0.223s, episode steps: 53, steps per second: 237, episode reward: 0.823, mean reward: 0.016 [-0.006, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.124 [-0.554, 0.120], loss: 0.000275, mean_absolute_error: 0.344177, mean_q: 0.232799\n",
      "  7844/50000: episode: 80, duration: 0.225s, episode steps: 70, steps per second: 311, episode reward: 0.709, mean reward: 0.010 [-0.008, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.152 [-0.807, 0.150], loss: 0.000261, mean_absolute_error: 0.338040, mean_q: 0.221543\n",
      "  7875/50000: episode: 81, duration: 0.101s, episode steps: 31, steps per second: 306, episode reward: 0.931, mean reward: 0.030 [-0.003, 1.000], mean action: 1.290 [1.000, 2.000], mean observation: -0.073 [-0.334, 0.090], loss: 0.000310, mean_absolute_error: 0.332434, mean_q: 0.215469\n",
      "  7978/50000: episode: 82, duration: 0.339s, episode steps: 103, steps per second: 303, episode reward: 0.547, mean reward: 0.005 [-0.008, 1.000], mean action: 0.932 [0.000, 2.000], mean observation: 0.189 [-0.140, 0.756], loss: 0.000277, mean_absolute_error: 0.340859, mean_q: 0.218118\n",
      "  8022/50000: episode: 83, duration: 0.139s, episode steps: 44, steps per second: 317, episode reward: 0.871, mean reward: 0.020 [-0.005, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.100 [-0.492, 0.120], loss: 0.000050, mean_absolute_error: 0.334542, mean_q: 0.188751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8041/50000: episode: 84, duration: 0.069s, episode steps: 19, steps per second: 275, episode reward: 0.980, mean reward: 0.052 [-0.001, 1.000], mean action: 1.053 [1.000, 2.000], mean observation: -0.049 [-0.117, 0.010], loss: 0.000062, mean_absolute_error: 0.337624, mean_q: 0.229573\n",
      "  8131/50000: episode: 85, duration: 0.325s, episode steps: 90, steps per second: 277, episode reward: 0.564, mean reward: 0.006 [-0.008, 1.000], mean action: 0.922 [0.000, 2.000], mean observation: 0.204 [-0.140, 0.786], loss: 0.000341, mean_absolute_error: 0.344114, mean_q: 0.231925\n",
      "  8193/50000: episode: 86, duration: 0.217s, episode steps: 62, steps per second: 286, episode reward: 0.703, mean reward: 0.011 [-0.008, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.181 [-0.839, 0.160], loss: 0.000129, mean_absolute_error: 0.345212, mean_q: 0.227804\n",
      "  8242/50000: episode: 87, duration: 0.160s, episode steps: 49, steps per second: 306, episode reward: 0.905, mean reward: 0.018 [-0.003, 1.000], mean action: 0.939 [0.000, 2.000], mean observation: 0.075 [-0.070, 0.321], loss: 0.000193, mean_absolute_error: 0.356641, mean_q: 0.259886\n",
      "  8351/50000: episode: 88, duration: 0.323s, episode steps: 109, steps per second: 337, episode reward: 0.421, mean reward: 0.004 [-0.009, 1.000], mean action: 0.972 [0.000, 2.000], mean observation: 0.230 [-0.170, 0.886], loss: 0.000235, mean_absolute_error: 0.349242, mean_q: 0.260772\n",
      "  8375/50000: episode: 89, duration: 0.082s, episode steps: 24, steps per second: 292, episode reward: 0.969, mean reward: 0.040 [-0.002, 1.000], mean action: 0.958 [0.000, 2.000], mean observation: 0.051 [-0.060, 0.177], loss: 0.000048, mean_absolute_error: 0.342522, mean_q: 0.280435\n",
      "  8423/50000: episode: 90, duration: 0.190s, episode steps: 48, steps per second: 253, episode reward: 0.873, mean reward: 0.018 [-0.004, 1.000], mean action: 0.896 [0.000, 2.000], mean observation: 0.099 [-0.080, 0.425], loss: 0.000065, mean_absolute_error: 0.350873, mean_q: 0.247192\n",
      "  8480/50000: episode: 91, duration: 0.170s, episode steps: 57, steps per second: 336, episode reward: 0.806, mean reward: 0.014 [-0.006, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.128 [-0.120, 0.578], loss: 0.000253, mean_absolute_error: 0.352283, mean_q: 0.270978\n",
      "  8520/50000: episode: 92, duration: 0.152s, episode steps: 40, steps per second: 264, episode reward: 0.949, mean reward: 0.024 [-0.002, 1.000], mean action: 0.975 [0.000, 2.000], mean observation: 0.052 [-0.070, 0.203], loss: 0.000179, mean_absolute_error: 0.370022, mean_q: 0.319697\n",
      "  8550/50000: episode: 93, duration: 0.111s, episode steps: 30, steps per second: 270, episode reward: 0.945, mean reward: 0.031 [-0.003, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.067 [-0.254, 0.090], loss: 0.000336, mean_absolute_error: 0.354935, mean_q: 0.276449\n",
      "  8588/50000: episode: 94, duration: 0.136s, episode steps: 38, steps per second: 279, episode reward: 0.898, mean reward: 0.024 [-0.004, 1.000], mean action: 1.211 [0.000, 2.000], mean observation: -0.094 [-0.406, 0.100], loss: 0.000161, mean_absolute_error: 0.358933, mean_q: 0.299191\n",
      "  8651/50000: episode: 95, duration: 0.177s, episode steps: 63, steps per second: 356, episode reward: 0.738, mean reward: 0.012 [-0.008, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.149 [-0.774, 0.150], loss: 0.000181, mean_absolute_error: 0.361002, mean_q: 0.295883\n",
      "  8708/50000: episode: 96, duration: 0.256s, episode steps: 57, steps per second: 223, episode reward: 0.864, mean reward: 0.015 [-0.005, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.067 [-0.483, 0.130], loss: 0.000219, mean_absolute_error: 0.356441, mean_q: 0.278781\n",
      "  8790/50000: episode: 97, duration: 0.261s, episode steps: 82, steps per second: 314, episode reward: 0.617, mean reward: 0.008 [-0.006, 1.000], mean action: 0.890 [0.000, 2.000], mean observation: 0.194 [-0.210, 0.646], loss: 0.000207, mean_absolute_error: 0.364457, mean_q: 0.302889\n",
      "  8813/50000: episode: 98, duration: 0.095s, episode steps: 23, steps per second: 243, episode reward: 0.962, mean reward: 0.042 [-0.003, 1.000], mean action: 1.391 [0.000, 2.000], mean observation: -0.041 [-0.251, 0.130], loss: 0.000058, mean_absolute_error: 0.351284, mean_q: 0.290327\n",
      "  8864/50000: episode: 99, duration: 0.179s, episode steps: 51, steps per second: 284, episode reward: 0.838, mean reward: 0.016 [-0.006, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.112 [-0.552, 0.120], loss: 0.000231, mean_absolute_error: 0.359758, mean_q: 0.301966\n",
      "  8888/50000: episode: 100, duration: 0.070s, episode steps: 24, steps per second: 344, episode reward: 0.952, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.061 [-0.292, 0.140], loss: 0.000302, mean_absolute_error: 0.365832, mean_q: 0.322378\n",
      "  8919/50000: episode: 101, duration: 0.091s, episode steps: 31, steps per second: 340, episode reward: 0.947, mean reward: 0.031 [-0.003, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.049 [-0.257, 0.130], loss: 0.000460, mean_absolute_error: 0.362931, mean_q: 0.317288\n",
      "  8994/50000: episode: 102, duration: 0.270s, episode steps: 75, steps per second: 278, episode reward: 0.623, mean reward: 0.008 [-0.010, 1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.189 [-0.957, 0.150], loss: 0.000283, mean_absolute_error: 0.365327, mean_q: 0.322625\n",
      "  9053/50000: episode: 103, duration: 0.207s, episode steps: 59, steps per second: 285, episode reward: 0.799, mean reward: 0.014 [-0.006, 1.000], mean action: 0.966 [0.000, 2.000], mean observation: 0.128 [-0.140, 0.604], loss: 0.000060, mean_absolute_error: 0.363630, mean_q: 0.330942\n",
      "  9077/50000: episode: 104, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 0.958, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.043 [-0.270, 0.140], loss: 0.000305, mean_absolute_error: 0.357244, mean_q: 0.322418\n",
      "  9084/50000: episode: 105, duration: 0.039s, episode steps: 7, steps per second: 182, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.041 [-0.030, 0.117], loss: 0.000086, mean_absolute_error: 0.379748, mean_q: 0.391576\n",
      "  9139/50000: episode: 106, duration: 0.256s, episode steps: 55, steps per second: 215, episode reward: 0.880, mean reward: 0.016 [-0.004, 1.000], mean action: 0.982 [0.000, 2.000], mean observation: 0.080 [-0.150, 0.419], loss: 0.000300, mean_absolute_error: 0.362466, mean_q: 0.323589\n",
      "  9179/50000: episode: 107, duration: 0.161s, episode steps: 40, steps per second: 248, episode reward: 0.897, mean reward: 0.022 [-0.004, 1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.093 [-0.394, 0.120], loss: 0.000165, mean_absolute_error: 0.374659, mean_q: 0.372772\n",
      "  9283/50000: episode: 108, duration: 0.438s, episode steps: 104, steps per second: 237, episode reward: 0.555, mean reward: 0.005 [-0.008, 1.000], mean action: 0.913 [0.000, 2.000], mean observation: 0.172 [-0.200, 0.777], loss: 0.000181, mean_absolute_error: 0.358417, mean_q: 0.344365\n",
      "  9284/50000: episode: 109, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [-0.010, 0.033], loss: 0.000055, mean_absolute_error: 0.350310, mean_q: 0.489741\n",
      "  9347/50000: episode: 110, duration: 0.219s, episode steps: 63, steps per second: 288, episode reward: 0.648, mean reward: 0.010 [-0.010, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.209 [-0.981, 0.180], loss: 0.000133, mean_absolute_error: 0.366023, mean_q: 0.366557\n",
      "  9390/50000: episode: 111, duration: 0.136s, episode steps: 43, steps per second: 316, episode reward: 0.890, mean reward: 0.021 [-0.004, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.092 [-0.110, 0.401], loss: 0.000111, mean_absolute_error: 0.366385, mean_q: 0.357744\n",
      "  9455/50000: episode: 112, duration: 0.259s, episode steps: 65, steps per second: 251, episode reward: 0.646, mean reward: 0.010 [-0.010, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.205 [-0.967, 0.200], loss: 0.000224, mean_absolute_error: 0.369845, mean_q: 0.375476\n",
      "  9501/50000: episode: 113, duration: 0.148s, episode steps: 46, steps per second: 311, episode reward: 0.852, mean reward: 0.019 [-0.005, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.112 [-0.523, 0.150], loss: 0.000220, mean_absolute_error: 0.363087, mean_q: 0.378086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9558/50000: episode: 114, duration: 0.203s, episode steps: 57, steps per second: 281, episode reward: 0.707, mean reward: 0.012 [-0.009, 1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.188 [-0.882, 0.220], loss: 0.000172, mean_absolute_error: 0.371738, mean_q: 0.387300\n",
      "  9559/50000: episode: 115, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [-0.010, 0.091], loss: 0.000235, mean_absolute_error: 0.373245, mean_q: 0.453195\n",
      "  9560/50000: episode: 116, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [-0.010, 0.076], loss: 0.000079, mean_absolute_error: 0.347057, mean_q: 0.384288\n",
      "  9589/50000: episode: 117, duration: 0.093s, episode steps: 29, steps per second: 313, episode reward: 0.938, mean reward: 0.032 [-0.003, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.057 [-0.341, 0.150], loss: 0.000130, mean_absolute_error: 0.363113, mean_q: 0.366244\n",
      "  9590/50000: episode: 118, duration: 0.006s, episode steps: 1, steps per second: 156, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.081, 0.010], loss: 0.000062, mean_absolute_error: 0.371903, mean_q: 0.385451\n",
      "  9639/50000: episode: 119, duration: 0.157s, episode steps: 49, steps per second: 311, episode reward: 0.781, mean reward: 0.016 [-0.008, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.148 [-0.230, 0.786], loss: 0.000127, mean_absolute_error: 0.360057, mean_q: 0.385700\n",
      "  9684/50000: episode: 120, duration: 0.167s, episode steps: 45, steps per second: 270, episode reward: 0.812, mean reward: 0.018 [-0.007, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.145 [-0.688, 0.190], loss: 0.000150, mean_absolute_error: 0.376912, mean_q: 0.415877\n",
      "  9742/50000: episode: 121, duration: 0.296s, episode steps: 58, steps per second: 196, episode reward: 0.697, mean reward: 0.012 [-0.009, 1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.190 [-0.897, 0.210], loss: 0.000160, mean_absolute_error: 0.372010, mean_q: 0.411232\n",
      "  9778/50000: episode: 122, duration: 0.107s, episode steps: 36, steps per second: 336, episode reward: 0.905, mean reward: 0.025 [-0.004, 1.000], mean action: 0.861 [0.000, 2.000], mean observation: 0.086 [-0.170, 0.435], loss: 0.000131, mean_absolute_error: 0.372417, mean_q: 0.424286\n",
      "  9830/50000: episode: 123, duration: 0.149s, episode steps: 52, steps per second: 349, episode reward: 0.769, mean reward: 0.015 [-0.008, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.152 [-0.788, 0.210], loss: 0.000328, mean_absolute_error: 0.361440, mean_q: 0.384025\n",
      "  9850/50000: episode: 124, duration: 0.060s, episode steps: 20, steps per second: 336, episode reward: 0.966, mean reward: 0.048 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.047 [-0.234, 0.130], loss: 0.000162, mean_absolute_error: 0.375993, mean_q: 0.455068\n",
      "  9907/50000: episode: 125, duration: 0.186s, episode steps: 57, steps per second: 306, episode reward: 0.718, mean reward: 0.013 [-0.009, 1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.180 [-0.877, 0.200], loss: 0.000188, mean_absolute_error: 0.371213, mean_q: 0.417532\n",
      "  9983/50000: episode: 126, duration: 0.250s, episode steps: 76, steps per second: 304, episode reward: 0.620, mean reward: 0.008 [-0.010, 1.000], mean action: 1.053 [0.000, 2.000], mean observation: -0.192 [-0.996, 0.220], loss: 0.000236, mean_absolute_error: 0.378647, mean_q: 0.445261\n",
      " 10046/50000: episode: 127, duration: 0.213s, episode steps: 63, steps per second: 295, episode reward: 0.677, mean reward: 0.011 [-0.009, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.190 [-0.940, 0.210], loss: 0.000170, mean_absolute_error: 0.372451, mean_q: 0.448034\n",
      " 10110/50000: episode: 128, duration: 0.306s, episode steps: 64, steps per second: 209, episode reward: 0.794, mean reward: 0.012 [-0.005, 1.000], mean action: 0.859 [0.000, 2.000], mean observation: 0.126 [-0.120, 0.546], loss: 0.000129, mean_absolute_error: 0.366270, mean_q: 0.450128\n",
      " 10136/50000: episode: 129, duration: 0.103s, episode steps: 26, steps per second: 252, episode reward: 0.943, mean reward: 0.036 [-0.003, 1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.068 [-0.319, 0.120], loss: 0.000069, mean_absolute_error: 0.369107, mean_q: 0.457821\n",
      " 10175/50000: episode: 130, duration: 0.129s, episode steps: 39, steps per second: 302, episode reward: 0.886, mean reward: 0.023 [-0.005, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.097 [-0.180, 0.454], loss: 0.000189, mean_absolute_error: 0.377523, mean_q: 0.456505\n",
      " 10200/50000: episode: 131, duration: 0.118s, episode steps: 25, steps per second: 211, episode reward: 0.962, mean reward: 0.038 [-0.002, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: 0.055 [-0.050, 0.207], loss: 0.000056, mean_absolute_error: 0.371026, mean_q: 0.457018\n",
      " 10219/50000: episode: 132, duration: 0.090s, episode steps: 19, steps per second: 211, episode reward: 0.968, mean reward: 0.051 [-0.002, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.045 [-0.238, 0.130], loss: 0.000239, mean_absolute_error: 0.359815, mean_q: 0.433108\n",
      " 10273/50000: episode: 133, duration: 0.191s, episode steps: 54, steps per second: 283, episode reward: 0.784, mean reward: 0.015 [-0.007, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.146 [-0.691, 0.170], loss: 0.000164, mean_absolute_error: 0.376768, mean_q: 0.471774\n",
      " 10302/50000: episode: 134, duration: 0.111s, episode steps: 29, steps per second: 261, episode reward: 0.942, mean reward: 0.032 [-0.003, 1.000], mean action: 0.759 [0.000, 2.000], mean observation: 0.069 [-0.110, 0.284], loss: 0.000077, mean_absolute_error: 0.375707, mean_q: 0.466089\n",
      " 10352/50000: episode: 135, duration: 0.147s, episode steps: 50, steps per second: 341, episode reward: 0.822, mean reward: 0.016 [-0.007, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.108 [-0.240, 0.675], loss: 0.000061, mean_absolute_error: 0.371192, mean_q: 0.467297\n",
      " 10412/50000: episode: 136, duration: 0.197s, episode steps: 60, steps per second: 305, episode reward: 0.674, mean reward: 0.011 [-0.010, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.192 [-0.985, 0.220], loss: 0.000116, mean_absolute_error: 0.376794, mean_q: 0.476958\n",
      " 10448/50000: episode: 137, duration: 0.106s, episode steps: 36, steps per second: 341, episode reward: 0.922, mean reward: 0.026 [-0.004, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.048 [-0.200, 0.397], loss: 0.000145, mean_absolute_error: 0.379429, mean_q: 0.480593\n",
      " 10491/50000: episode: 138, duration: 0.125s, episode steps: 43, steps per second: 343, episode reward: 0.885, mean reward: 0.021 [-0.004, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.097 [-0.090, 0.418], loss: 0.000122, mean_absolute_error: 0.391622, mean_q: 0.517145\n",
      " 10532/50000: episode: 139, duration: 0.122s, episode steps: 41, steps per second: 336, episode reward: 0.851, mean reward: 0.021 [-0.006, 1.000], mean action: 1.171 [0.000, 2.000], mean observation: -0.123 [-0.583, 0.170], loss: 0.000082, mean_absolute_error: 0.383350, mean_q: 0.499996\n",
      " 10588/50000: episode: 140, duration: 0.178s, episode steps: 56, steps per second: 314, episode reward: 0.724, mean reward: 0.013 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.167 [-0.887, 0.230], loss: 0.000161, mean_absolute_error: 0.377516, mean_q: 0.494986\n",
      " 10594/50000: episode: 141, duration: 0.024s, episode steps: 6, steps per second: 250, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.119, 0.060], loss: 0.000083, mean_absolute_error: 0.392147, mean_q: 0.518927\n",
      " 10634/50000: episode: 142, duration: 0.129s, episode steps: 40, steps per second: 310, episode reward: 0.910, mean reward: 0.023 [-0.004, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.053 [-0.200, 0.415], loss: 0.000140, mean_absolute_error: 0.374238, mean_q: 0.483748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10653/50000: episode: 143, duration: 0.078s, episode steps: 19, steps per second: 243, episode reward: 0.974, mean reward: 0.051 [-0.002, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.032 [-0.202, 0.130], loss: 0.000083, mean_absolute_error: 0.385346, mean_q: 0.522254\n",
      " 10694/50000: episode: 144, duration: 0.194s, episode steps: 41, steps per second: 211, episode reward: 0.873, mean reward: 0.021 [-0.005, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.090 [-0.220, 0.539], loss: 0.000112, mean_absolute_error: 0.379351, mean_q: 0.517121\n",
      " 10704/50000: episode: 145, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 0.990, mean reward: 0.099 [-0.001, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: 0.045 [-0.020, 0.115], loss: 0.000059, mean_absolute_error: 0.387682, mean_q: 0.528012\n",
      " 10720/50000: episode: 146, duration: 0.060s, episode steps: 16, steps per second: 266, episode reward: 0.978, mean reward: 0.061 [-0.002, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.044 [-0.190, 0.100], loss: 0.000107, mean_absolute_error: 0.389189, mean_q: 0.521564\n",
      " 10731/50000: episode: 147, duration: 0.046s, episode steps: 11, steps per second: 241, episode reward: 0.987, mean reward: 0.090 [-0.001, 1.000], mean action: 1.818 [0.000, 2.000], mean observation: -0.042 [-0.148, 0.090], loss: 0.000272, mean_absolute_error: 0.346752, mean_q: 0.453798\n",
      " 10775/50000: episode: 148, duration: 0.185s, episode steps: 44, steps per second: 238, episode reward: 0.869, mean reward: 0.020 [-0.005, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.093 [-0.521, 0.160], loss: 0.000172, mean_absolute_error: 0.382157, mean_q: 0.515504\n",
      " 10809/50000: episode: 149, duration: 0.151s, episode steps: 34, steps per second: 225, episode reward: 0.905, mean reward: 0.027 [-0.004, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.091 [-0.442, 0.160], loss: 0.000168, mean_absolute_error: 0.377431, mean_q: 0.515601\n",
      " 10855/50000: episode: 150, duration: 0.216s, episode steps: 46, steps per second: 213, episode reward: 0.840, mean reward: 0.018 [-0.006, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.120 [-0.190, 0.588], loss: 0.000058, mean_absolute_error: 0.386761, mean_q: 0.532840\n",
      " 10903/50000: episode: 151, duration: 0.246s, episode steps: 48, steps per second: 195, episode reward: 0.857, mean reward: 0.018 [-0.006, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.068 [-0.230, 0.599], loss: 0.000100, mean_absolute_error: 0.379833, mean_q: 0.528374\n",
      " 10924/50000: episode: 152, duration: 0.092s, episode steps: 21, steps per second: 228, episode reward: 0.975, mean reward: 0.046 [-0.001, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.052 [-0.040, 0.145], loss: 0.000073, mean_absolute_error: 0.381002, mean_q: 0.526473\n",
      " 10925/50000: episode: 153, duration: 0.008s, episode steps: 1, steps per second: 121, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.033 [-0.075, 0.010], loss: 0.000090, mean_absolute_error: 0.395867, mean_q: 0.590240\n",
      " 10988/50000: episode: 154, duration: 0.249s, episode steps: 63, steps per second: 253, episode reward: 0.635, mean reward: 0.010 [-0.010, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.216 [-0.984, 0.210], loss: 0.000189, mean_absolute_error: 0.381103, mean_q: 0.534625\n",
      " 10989/50000: episode: 155, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.016 [0.010, 0.021], loss: 0.000070, mean_absolute_error: 0.358752, mean_q: 0.480984\n",
      " 11043/50000: episode: 156, duration: 0.189s, episode steps: 54, steps per second: 285, episode reward: 0.732, mean reward: 0.014 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.171 [-0.260, 0.866], loss: 0.000149, mean_absolute_error: 0.387314, mean_q: 0.543163\n",
      " 11062/50000: episode: 157, duration: 0.067s, episode steps: 19, steps per second: 282, episode reward: 0.970, mean reward: 0.051 [-0.002, 1.000], mean action: 1.316 [0.000, 2.000], mean observation: -0.049 [-0.223, 0.110], loss: 0.000057, mean_absolute_error: 0.387958, mean_q: 0.553507\n",
      " 11078/50000: episode: 158, duration: 0.061s, episode steps: 16, steps per second: 261, episode reward: 0.983, mean reward: 0.061 [-0.001, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: 0.047 [-0.020, 0.129], loss: 0.000121, mean_absolute_error: 0.385119, mean_q: 0.549708\n",
      " 11079/50000: episode: 159, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [-0.010, 0.021], loss: 0.000148, mean_absolute_error: 0.341280, mean_q: 0.478047\n",
      " 11103/50000: episode: 160, duration: 0.084s, episode steps: 24, steps per second: 286, episode reward: 0.969, mean reward: 0.040 [-0.002, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.021 [-0.140, 0.217], loss: 0.000105, mean_absolute_error: 0.380654, mean_q: 0.535480\n",
      " 11141/50000: episode: 161, duration: 0.120s, episode steps: 38, steps per second: 316, episode reward: 0.903, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.080 [-0.140, 0.424], loss: 0.000142, mean_absolute_error: 0.385561, mean_q: 0.552941\n",
      " 11193/50000: episode: 162, duration: 0.188s, episode steps: 52, steps per second: 277, episode reward: 0.804, mean reward: 0.015 [-0.007, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.116 [-0.240, 0.730], loss: 0.000095, mean_absolute_error: 0.382362, mean_q: 0.552552\n",
      " 11236/50000: episode: 163, duration: 0.130s, episode steps: 43, steps per second: 330, episode reward: 0.894, mean reward: 0.021 [-0.005, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.070 [-0.465, 0.180], loss: 0.000074, mean_absolute_error: 0.383854, mean_q: 0.556590\n",
      " 11282/50000: episode: 164, duration: 0.133s, episode steps: 46, steps per second: 347, episode reward: 0.847, mean reward: 0.018 [-0.006, 1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.116 [-0.565, 0.160], loss: 0.000091, mean_absolute_error: 0.381738, mean_q: 0.554985\n",
      " 11332/50000: episode: 165, duration: 0.144s, episode steps: 50, steps per second: 347, episode reward: 0.829, mean reward: 0.017 [-0.006, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.110 [-0.190, 0.622], loss: 0.000106, mean_absolute_error: 0.382688, mean_q: 0.559573\n",
      " 11381/50000: episode: 166, duration: 0.148s, episode steps: 49, steps per second: 331, episode reward: 0.778, mean reward: 0.016 [-0.008, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.151 [-0.260, 0.792], loss: 0.000164, mean_absolute_error: 0.393762, mean_q: 0.575808\n",
      " 11446/50000: episode: 167, duration: 0.200s, episode steps: 65, steps per second: 324, episode reward: 0.689, mean reward: 0.011 [-0.009, 1.000], mean action: 1.092 [0.000, 2.000], mean observation: -0.178 [-0.899, 0.200], loss: 0.000052, mean_absolute_error: 0.383798, mean_q: 0.566900\n",
      " 11447/50000: episode: 168, duration: 0.008s, episode steps: 1, steps per second: 118, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.013 [0.000, 0.026], loss: 0.000037, mean_absolute_error: 0.377544, mean_q: 0.553204\n",
      " 11507/50000: episode: 169, duration: 0.193s, episode steps: 60, steps per second: 311, episode reward: 0.674, mean reward: 0.011 [-0.010, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.198 [-0.971, 0.220], loss: 0.000068, mean_absolute_error: 0.388063, mean_q: 0.573607\n",
      " 11576/50000: episode: 170, duration: 0.223s, episode steps: 69, steps per second: 310, episode reward: 0.711, mean reward: 0.010 [-0.008, 1.000], mean action: 1.029 [0.000, 2.000], mean observation: -0.159 [-0.806, 0.180], loss: 0.000064, mean_absolute_error: 0.389357, mean_q: 0.580413\n",
      " 11621/50000: episode: 171, duration: 0.154s, episode steps: 45, steps per second: 292, episode reward: 0.846, mean reward: 0.019 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.118 [-0.180, 0.570], loss: 0.000068, mean_absolute_error: 0.391070, mean_q: 0.585253\n",
      " 11622/50000: episode: 172, duration: 0.007s, episode steps: 1, steps per second: 144, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [-0.010, 0.024], loss: 0.000039, mean_absolute_error: 0.393372, mean_q: 0.584166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11687/50000: episode: 173, duration: 0.247s, episode steps: 65, steps per second: 263, episode reward: 0.768, mean reward: 0.012 [-0.007, 1.000], mean action: 1.062 [0.000, 2.000], mean observation: -0.135 [-0.678, 0.160], loss: 0.000059, mean_absolute_error: 0.385198, mean_q: 0.576524\n",
      " 11746/50000: episode: 174, duration: 0.177s, episode steps: 59, steps per second: 333, episode reward: 0.865, mean reward: 0.015 [-0.005, 1.000], mean action: 1.034 [0.000, 2.000], mean observation: -0.084 [-0.458, 0.140], loss: 0.000078, mean_absolute_error: 0.390565, mean_q: 0.586045\n",
      " 11835/50000: episode: 175, duration: 0.281s, episode steps: 89, steps per second: 317, episode reward: 0.748, mean reward: 0.008 [-0.007, 1.000], mean action: 1.034 [0.000, 2.000], mean observation: -0.109 [-0.679, 0.180], loss: 0.000052, mean_absolute_error: 0.391531, mean_q: 0.587685\n",
      " 11848/50000: episode: 176, duration: 0.040s, episode steps: 13, steps per second: 325, episode reward: 0.985, mean reward: 0.076 [-0.001, 1.000], mean action: 0.538 [0.000, 2.000], mean observation: 0.046 [-0.060, 0.143], loss: 0.000106, mean_absolute_error: 0.410705, mean_q: 0.618595\n",
      " 11902/50000: episode: 177, duration: 0.201s, episode steps: 54, steps per second: 269, episode reward: 0.839, mean reward: 0.016 [-0.006, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.095 [-0.200, 0.595], loss: 0.000093, mean_absolute_error: 0.394355, mean_q: 0.592045\n",
      " 11934/50000: episode: 178, duration: 0.095s, episode steps: 32, steps per second: 337, episode reward: 0.907, mean reward: 0.028 [-0.004, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.093 [-0.180, 0.440], loss: 0.000152, mean_absolute_error: 0.394724, mean_q: 0.594403\n",
      " 11962/50000: episode: 179, duration: 0.087s, episode steps: 28, steps per second: 323, episode reward: 0.940, mean reward: 0.034 [-0.003, 1.000], mean action: 0.714 [0.000, 2.000], mean observation: 0.069 [-0.130, 0.318], loss: 0.000094, mean_absolute_error: 0.394923, mean_q: 0.595248\n",
      " 11994/50000: episode: 180, duration: 0.119s, episode steps: 32, steps per second: 270, episode reward: 0.933, mean reward: 0.029 [-0.003, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.069 [-0.333, 0.130], loss: 0.000085, mean_absolute_error: 0.387282, mean_q: 0.583574\n",
      " 12053/50000: episode: 181, duration: 0.182s, episode steps: 59, steps per second: 324, episode reward: 0.765, mean reward: 0.013 [-0.007, 1.000], mean action: 1.068 [0.000, 2.000], mean observation: -0.147 [-0.723, 0.160], loss: 0.000088, mean_absolute_error: 0.395291, mean_q: 0.596271\n",
      " 12074/50000: episode: 182, duration: 0.068s, episode steps: 21, steps per second: 309, episode reward: 0.971, mean reward: 0.046 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.031 [-0.140, 0.199], loss: 0.000100, mean_absolute_error: 0.402903, mean_q: 0.607133\n",
      " 12126/50000: episode: 183, duration: 0.239s, episode steps: 52, steps per second: 218, episode reward: 0.823, mean reward: 0.016 [-0.006, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: -0.122 [-0.612, 0.180], loss: 0.000047, mean_absolute_error: 0.392649, mean_q: 0.592892\n",
      " 12152/50000: episode: 184, duration: 0.120s, episode steps: 26, steps per second: 217, episode reward: 0.956, mean reward: 0.037 [-0.002, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.058 [-0.242, 0.110], loss: 0.000041, mean_absolute_error: 0.396281, mean_q: 0.596284\n",
      " 12205/50000: episode: 185, duration: 0.260s, episode steps: 53, steps per second: 204, episode reward: 0.783, mean reward: 0.015 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.146 [-0.220, 0.732], loss: 0.000110, mean_absolute_error: 0.398981, mean_q: 0.602751\n",
      " 12235/50000: episode: 186, duration: 0.102s, episode steps: 30, steps per second: 295, episode reward: 0.935, mean reward: 0.031 [-0.003, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.072 [-0.327, 0.130], loss: 0.000076, mean_absolute_error: 0.390349, mean_q: 0.591471\n",
      " 12265/50000: episode: 187, duration: 0.087s, episode steps: 30, steps per second: 345, episode reward: 0.949, mean reward: 0.032 [-0.003, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.060 [-0.261, 0.100], loss: 0.000078, mean_absolute_error: 0.411577, mean_q: 0.621965\n",
      " 12316/50000: episode: 188, duration: 0.147s, episode steps: 51, steps per second: 347, episode reward: 0.829, mean reward: 0.016 [-0.006, 1.000], mean action: 1.078 [0.000, 2.000], mean observation: -0.121 [-0.582, 0.150], loss: 0.000111, mean_absolute_error: 0.397940, mean_q: 0.601130\n",
      " 12342/50000: episode: 189, duration: 0.076s, episode steps: 26, steps per second: 341, episode reward: 0.949, mean reward: 0.036 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.060 [-0.130, 0.281], loss: 0.000122, mean_absolute_error: 0.400984, mean_q: 0.605634\n",
      " 12381/50000: episode: 190, duration: 0.129s, episode steps: 39, steps per second: 302, episode reward: 0.892, mean reward: 0.023 [-0.005, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: -0.093 [-0.462, 0.150], loss: 0.000048, mean_absolute_error: 0.405966, mean_q: 0.614582\n",
      " 12418/50000: episode: 191, duration: 0.127s, episode steps: 37, steps per second: 291, episode reward: 0.912, mean reward: 0.025 [-0.004, 1.000], mean action: 1.108 [0.000, 2.000], mean observation: -0.080 [-0.394, 0.130], loss: 0.000039, mean_absolute_error: 0.406848, mean_q: 0.615018\n",
      " 12469/50000: episode: 192, duration: 0.188s, episode steps: 51, steps per second: 271, episode reward: 0.804, mean reward: 0.016 [-0.007, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.139 [-0.656, 0.150], loss: 0.000064, mean_absolute_error: 0.409123, mean_q: 0.618303\n",
      " 12496/50000: episode: 193, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 0.960, mean reward: 0.036 [-0.002, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.062 [-0.090, 0.173], loss: 0.000071, mean_absolute_error: 0.405398, mean_q: 0.613419\n",
      " 12537/50000: episode: 194, duration: 0.215s, episode steps: 41, steps per second: 191, episode reward: 0.903, mean reward: 0.022 [-0.003, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.088 [-0.090, 0.349], loss: 0.000039, mean_absolute_error: 0.409303, mean_q: 0.619712\n",
      " 12538/50000: episode: 195, duration: 0.008s, episode steps: 1, steps per second: 124, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.093, 0.010], loss: 0.000081, mean_absolute_error: 0.355586, mean_q: 0.537718\n",
      " 12615/50000: episode: 196, duration: 0.385s, episode steps: 77, steps per second: 200, episode reward: 0.677, mean reward: 0.009 [-0.009, 1.000], mean action: 1.052 [0.000, 2.000], mean observation: -0.161 [-0.866, 0.190], loss: 0.000066, mean_absolute_error: 0.404291, mean_q: 0.612161\n",
      " 12662/50000: episode: 197, duration: 0.271s, episode steps: 47, steps per second: 174, episode reward: 0.802, mean reward: 0.017 [-0.007, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.146 [-0.230, 0.717], loss: 0.000077, mean_absolute_error: 0.407123, mean_q: 0.615247\n",
      " 12710/50000: episode: 198, duration: 0.279s, episode steps: 48, steps per second: 172, episode reward: 0.788, mean reward: 0.016 [-0.007, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.154 [-0.210, 0.749], loss: 0.000060, mean_absolute_error: 0.399848, mean_q: 0.605298\n",
      " 12731/50000: episode: 199, duration: 0.097s, episode steps: 21, steps per second: 216, episode reward: 0.971, mean reward: 0.046 [-0.002, 1.000], mean action: 1.095 [0.000, 2.000], mean observation: -0.053 [-0.175, 0.060], loss: 0.000065, mean_absolute_error: 0.394347, mean_q: 0.596711\n",
      " 12732/50000: episode: 200, duration: 0.008s, episode steps: 1, steps per second: 129, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.005 [-0.010, 0.001], loss: 0.000035, mean_absolute_error: 0.345741, mean_q: 0.525378\n",
      " 12785/50000: episode: 201, duration: 0.219s, episode steps: 53, steps per second: 242, episode reward: 0.768, mean reward: 0.014 [-0.008, 1.000], mean action: 0.906 [0.000, 2.000], mean observation: 0.156 [-0.240, 0.771], loss: 0.000050, mean_absolute_error: 0.396672, mean_q: 0.600526\n",
      " 12826/50000: episode: 202, duration: 0.170s, episode steps: 41, steps per second: 241, episode reward: 0.890, mean reward: 0.022 [-0.005, 1.000], mean action: 1.122 [0.000, 2.000], mean observation: -0.092 [-0.456, 0.140], loss: 0.000038, mean_absolute_error: 0.396596, mean_q: 0.600702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12870/50000: episode: 203, duration: 0.208s, episode steps: 44, steps per second: 212, episode reward: 0.846, mean reward: 0.019 [-0.006, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.115 [-0.200, 0.593], loss: 0.000075, mean_absolute_error: 0.405757, mean_q: 0.613817\n",
      " 12923/50000: episode: 204, duration: 0.223s, episode steps: 53, steps per second: 238, episode reward: 0.876, mean reward: 0.017 [-0.005, 1.000], mean action: 1.057 [0.000, 2.000], mean observation: -0.084 [-0.456, 0.150], loss: 0.000033, mean_absolute_error: 0.402594, mean_q: 0.609841\n",
      " 13002/50000: episode: 205, duration: 0.457s, episode steps: 79, steps per second: 173, episode reward: 0.718, mean reward: 0.009 [-0.008, 1.000], mean action: 1.013 [0.000, 2.000], mean observation: -0.135 [-0.789, 0.180], loss: 0.000064, mean_absolute_error: 0.404756, mean_q: 0.612915\n",
      " 13003/50000: episode: 206, duration: 0.008s, episode steps: 1, steps per second: 123, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.023 [-0.037, -0.010], loss: 0.000101, mean_absolute_error: 0.467222, mean_q: 0.706945\n",
      " 13004/50000: episode: 207, duration: 0.008s, episode steps: 1, steps per second: 132, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [-0.010, 0.046], loss: 0.000090, mean_absolute_error: 0.426249, mean_q: 0.645042\n",
      " 13033/50000: episode: 208, duration: 0.119s, episode steps: 29, steps per second: 244, episode reward: 0.930, mean reward: 0.032 [-0.004, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.075 [-0.130, 0.367], loss: 0.000046, mean_absolute_error: 0.405674, mean_q: 0.614438\n",
      " 13063/50000: episode: 209, duration: 0.119s, episode steps: 30, steps per second: 253, episode reward: 0.949, mean reward: 0.032 [-0.003, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.061 [-0.252, 0.100], loss: 0.000049, mean_absolute_error: 0.399088, mean_q: 0.604553\n",
      " 13106/50000: episode: 210, duration: 0.280s, episode steps: 43, steps per second: 153, episode reward: 0.823, mean reward: 0.019 [-0.007, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.138 [-0.220, 0.678], loss: 0.000050, mean_absolute_error: 0.410278, mean_q: 0.621381\n",
      " 13175/50000: episode: 211, duration: 0.297s, episode steps: 69, steps per second: 232, episode reward: 0.732, mean reward: 0.011 [-0.008, 1.000], mean action: 1.058 [0.000, 2.000], mean observation: -0.147 [-0.764, 0.170], loss: 0.000062, mean_absolute_error: 0.405285, mean_q: 0.613656\n",
      " 13241/50000: episode: 212, duration: 0.272s, episode steps: 66, steps per second: 242, episode reward: 0.750, mean reward: 0.011 [-0.007, 1.000], mean action: 1.045 [0.000, 2.000], mean observation: -0.142 [-0.739, 0.170], loss: 0.000054, mean_absolute_error: 0.411693, mean_q: 0.622180\n",
      " 13243/50000: episode: 213, duration: 0.027s, episode steps: 2, steps per second: 74, episode reward: 0.999, mean reward: 0.499 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.042 [-0.101, 0.020], loss: 0.000037, mean_absolute_error: 0.388371, mean_q: 0.590462\n",
      " 13311/50000: episode: 214, duration: 0.274s, episode steps: 68, steps per second: 248, episode reward: 0.640, mean reward: 0.009 [-0.010, 1.000], mean action: 1.088 [0.000, 2.000], mean observation: -0.201 [-0.977, 0.200], loss: 0.000041, mean_absolute_error: 0.404141, mean_q: 0.612230\n",
      " 13370/50000: episode: 215, duration: 0.169s, episode steps: 59, steps per second: 348, episode reward: 0.686, mean reward: 0.012 [-0.009, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.196 [-0.230, 0.920], loss: 0.000049, mean_absolute_error: 0.407526, mean_q: 0.617186\n",
      " 13445/50000: episode: 216, duration: 0.251s, episode steps: 75, steps per second: 298, episode reward: 0.658, mean reward: 0.009 [-0.009, 1.000], mean action: 1.040 [0.000, 2.000], mean observation: -0.174 [-0.927, 0.210], loss: 0.000039, mean_absolute_error: 0.413202, mean_q: 0.625820\n",
      " 13499/50000: episode: 217, duration: 0.160s, episode steps: 54, steps per second: 337, episode reward: 0.734, mean reward: 0.014 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.177 [-0.220, 0.857], loss: 0.000058, mean_absolute_error: 0.398620, mean_q: 0.603616\n",
      " 13536/50000: episode: 218, duration: 0.114s, episode steps: 37, steps per second: 325, episode reward: 0.917, mean reward: 0.025 [-0.004, 1.000], mean action: 1.135 [0.000, 2.000], mean observation: -0.077 [-0.365, 0.130], loss: 0.000074, mean_absolute_error: 0.413791, mean_q: 0.626831\n",
      " 13537/50000: episode: 219, duration: 0.008s, episode steps: 1, steps per second: 127, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [-0.010, 0.062], loss: 0.000056, mean_absolute_error: 0.442216, mean_q: 0.668055\n",
      " 13544/50000: episode: 220, duration: 0.032s, episode steps: 7, steps per second: 222, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.286 [0.000, 2.000], mean observation: 0.038 [-0.050, 0.120], loss: 0.000040, mean_absolute_error: 0.377723, mean_q: 0.573861\n",
      " 13606/50000: episode: 221, duration: 0.270s, episode steps: 62, steps per second: 230, episode reward: 0.736, mean reward: 0.012 [-0.008, 1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.158 [-0.783, 0.170], loss: 0.000075, mean_absolute_error: 0.412843, mean_q: 0.624836\n",
      " 13653/50000: episode: 222, duration: 0.235s, episode steps: 47, steps per second: 200, episode reward: 0.835, mean reward: 0.018 [-0.006, 1.000], mean action: 0.936 [0.000, 2.000], mean observation: 0.123 [-0.210, 0.603], loss: 0.000037, mean_absolute_error: 0.406972, mean_q: 0.617318\n",
      " 13686/50000: episode: 223, duration: 0.102s, episode steps: 33, steps per second: 323, episode reward: 0.902, mean reward: 0.027 [-0.005, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.094 [-0.150, 0.459], loss: 0.000028, mean_absolute_error: 0.417438, mean_q: 0.632218\n",
      " 13687/50000: episode: 224, duration: 0.007s, episode steps: 1, steps per second: 148, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.014 [-0.037, 0.010], loss: 0.000010, mean_absolute_error: 0.436216, mean_q: 0.661399\n",
      " 13741/50000: episode: 225, duration: 0.199s, episode steps: 54, steps per second: 272, episode reward: 0.731, mean reward: 0.014 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.178 [-0.220, 0.865], loss: 0.000037, mean_absolute_error: 0.402789, mean_q: 0.610950\n",
      " 13790/50000: episode: 226, duration: 0.153s, episode steps: 49, steps per second: 320, episode reward: 0.788, mean reward: 0.016 [-0.007, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.150 [-0.210, 0.735], loss: 0.000066, mean_absolute_error: 0.417913, mean_q: 0.633443\n",
      " 13808/50000: episode: 227, duration: 0.058s, episode steps: 18, steps per second: 312, episode reward: 0.976, mean reward: 0.054 [-0.002, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.045 [-0.177, 0.080], loss: 0.000074, mean_absolute_error: 0.394294, mean_q: 0.597745\n",
      " 13842/50000: episode: 228, duration: 0.129s, episode steps: 34, steps per second: 263, episode reward: 0.922, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.062 [-0.150, 0.387], loss: 0.000074, mean_absolute_error: 0.414692, mean_q: 0.627528\n",
      " 13884/50000: episode: 229, duration: 0.125s, episode steps: 42, steps per second: 337, episode reward: 0.862, mean reward: 0.021 [-0.005, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.111 [-0.180, 0.548], loss: 0.000085, mean_absolute_error: 0.411001, mean_q: 0.622474\n",
      " 13916/50000: episode: 230, duration: 0.096s, episode steps: 32, steps per second: 334, episode reward: 0.936, mean reward: 0.029 [-0.003, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.065 [-0.324, 0.130], loss: 0.000037, mean_absolute_error: 0.408761, mean_q: 0.619498\n",
      " 13944/50000: episode: 231, duration: 0.090s, episode steps: 28, steps per second: 310, episode reward: 0.939, mean reward: 0.034 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.069 [-0.100, 0.325], loss: 0.000030, mean_absolute_error: 0.399951, mean_q: 0.606072\n",
      " 13945/50000: episode: 232, duration: 0.006s, episode steps: 1, steps per second: 166, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.002 [-0.015, 0.010], loss: 0.000177, mean_absolute_error: 0.393615, mean_q: 0.594988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13998/50000: episode: 233, duration: 0.196s, episode steps: 53, steps per second: 271, episode reward: 0.802, mean reward: 0.015 [-0.007, 1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.135 [-0.665, 0.170], loss: 0.000031, mean_absolute_error: 0.409642, mean_q: 0.621026\n",
      " 13999/50000: episode: 234, duration: 0.007s, episode steps: 1, steps per second: 138, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.015 [0.010, 0.020], loss: 0.000025, mean_absolute_error: 0.436329, mean_q: 0.661216\n",
      " 14043/50000: episode: 235, duration: 0.144s, episode steps: 44, steps per second: 305, episode reward: 0.889, mean reward: 0.020 [-0.004, 1.000], mean action: 1.068 [0.000, 2.000], mean observation: -0.089 [-0.435, 0.140], loss: 0.000035, mean_absolute_error: 0.403224, mean_q: 0.611042\n",
      " 14101/50000: episode: 236, duration: 0.185s, episode steps: 58, steps per second: 313, episode reward: 0.764, mean reward: 0.013 [-0.008, 1.000], mean action: 1.069 [0.000, 2.000], mean observation: -0.148 [-0.752, 0.180], loss: 0.000040, mean_absolute_error: 0.412483, mean_q: 0.624877\n",
      " 14159/50000: episode: 237, duration: 0.193s, episode steps: 58, steps per second: 301, episode reward: 0.785, mean reward: 0.014 [-0.007, 1.000], mean action: 1.069 [0.000, 2.000], mean observation: -0.134 [-0.695, 0.190], loss: 0.000038, mean_absolute_error: 0.409601, mean_q: 0.620999\n",
      " 14214/50000: episode: 238, duration: 0.242s, episode steps: 55, steps per second: 227, episode reward: 0.714, mean reward: 0.013 [-0.009, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.189 [-0.260, 0.885], loss: 0.000054, mean_absolute_error: 0.406710, mean_q: 0.615857\n",
      " 14273/50000: episode: 239, duration: 0.218s, episode steps: 59, steps per second: 270, episode reward: 0.754, mean reward: 0.013 [-0.007, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.158 [-0.699, 0.180], loss: 0.000040, mean_absolute_error: 0.421646, mean_q: 0.637905\n",
      " 14330/50000: episode: 240, duration: 0.366s, episode steps: 57, steps per second: 156, episode reward: 0.695, mean reward: 0.012 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.192 [-0.240, 0.946], loss: 0.000032, mean_absolute_error: 0.418374, mean_q: 0.633653\n",
      " 14360/50000: episode: 241, duration: 0.250s, episode steps: 30, steps per second: 120, episode reward: 0.936, mean reward: 0.031 [-0.003, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.069 [-0.110, 0.330], loss: 0.000033, mean_absolute_error: 0.396610, mean_q: 0.601033\n",
      " 14413/50000: episode: 242, duration: 0.418s, episode steps: 53, steps per second: 127, episode reward: 0.796, mean reward: 0.015 [-0.007, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.139 [-0.669, 0.160], loss: 0.000038, mean_absolute_error: 0.416521, mean_q: 0.630896\n",
      " 14469/50000: episode: 243, duration: 0.425s, episode steps: 56, steps per second: 132, episode reward: 0.716, mean reward: 0.013 [-0.009, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.183 [-0.250, 0.898], loss: 0.000038, mean_absolute_error: 0.413289, mean_q: 0.626274\n",
      " 14521/50000: episode: 244, duration: 0.575s, episode steps: 52, steps per second: 90, episode reward: 0.785, mean reward: 0.015 [-0.007, 1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.147 [-0.734, 0.200], loss: 0.000049, mean_absolute_error: 0.408714, mean_q: 0.618224\n",
      " 14574/50000: episode: 245, duration: 0.346s, episode steps: 53, steps per second: 153, episode reward: 0.744, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.173 [-0.230, 0.825], loss: 0.000032, mean_absolute_error: 0.412024, mean_q: 0.623736\n",
      " 14628/50000: episode: 246, duration: 0.594s, episode steps: 54, steps per second: 91, episode reward: 0.752, mean reward: 0.014 [-0.008, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.167 [-0.230, 0.786], loss: 0.000029, mean_absolute_error: 0.408994, mean_q: 0.619434\n",
      " 14677/50000: episode: 247, duration: 0.288s, episode steps: 49, steps per second: 170, episode reward: 0.852, mean reward: 0.017 [-0.005, 1.000], mean action: 1.061 [0.000, 2.000], mean observation: -0.107 [-0.540, 0.170], loss: 0.000033, mean_absolute_error: 0.411750, mean_q: 0.622365\n",
      " 14707/50000: episode: 248, duration: 0.205s, episode steps: 30, steps per second: 147, episode reward: 0.941, mean reward: 0.031 [-0.003, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.058 [-0.100, 0.306], loss: 0.000033, mean_absolute_error: 0.419032, mean_q: 0.633770\n",
      " 14776/50000: episode: 249, duration: 0.617s, episode steps: 69, steps per second: 112, episode reward: 0.657, mean reward: 0.010 [-0.009, 1.000], mean action: 1.072 [0.000, 2.000], mean observation: -0.191 [-0.902, 0.180], loss: 0.000033, mean_absolute_error: 0.416960, mean_q: 0.631341\n",
      " 14802/50000: episode: 250, duration: 0.122s, episode steps: 26, steps per second: 213, episode reward: 0.953, mean reward: 0.037 [-0.003, 1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.060 [-0.260, 0.120], loss: 0.000023, mean_absolute_error: 0.419223, mean_q: 0.634337\n",
      " 14822/50000: episode: 251, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 0.965, mean reward: 0.048 [-0.002, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.054 [-0.243, 0.100], loss: 0.000028, mean_absolute_error: 0.409544, mean_q: 0.619214\n",
      " 14845/50000: episode: 252, duration: 0.222s, episode steps: 23, steps per second: 104, episode reward: 0.964, mean reward: 0.042 [-0.002, 1.000], mean action: 0.783 [0.000, 2.000], mean observation: 0.056 [-0.060, 0.210], loss: 0.000039, mean_absolute_error: 0.421635, mean_q: 0.637734\n",
      " 14891/50000: episode: 253, duration: 0.344s, episode steps: 46, steps per second: 134, episode reward: 0.820, mean reward: 0.018 [-0.007, 1.000], mean action: 0.848 [0.000, 2.000], mean observation: 0.134 [-0.220, 0.670], loss: 0.000034, mean_absolute_error: 0.417248, mean_q: 0.631721\n",
      " 14943/50000: episode: 254, duration: 0.396s, episode steps: 52, steps per second: 131, episode reward: 0.762, mean reward: 0.015 [-0.008, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.162 [-0.240, 0.804], loss: 0.000066, mean_absolute_error: 0.419493, mean_q: 0.633895\n",
      " 14980/50000: episode: 255, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 0.892, mean reward: 0.024 [-0.005, 1.000], mean action: 1.135 [0.000, 2.000], mean observation: -0.096 [-0.476, 0.170], loss: 0.000033, mean_absolute_error: 0.418219, mean_q: 0.632061\n",
      " 15017/50000: episode: 256, duration: 0.260s, episode steps: 37, steps per second: 142, episode reward: 0.910, mean reward: 0.025 [-0.004, 1.000], mean action: 1.162 [0.000, 2.000], mean observation: -0.083 [-0.391, 0.140], loss: 0.000029, mean_absolute_error: 0.414931, mean_q: 0.627385\n",
      " 15035/50000: episode: 257, duration: 0.139s, episode steps: 18, steps per second: 130, episode reward: 0.970, mean reward: 0.054 [-0.002, 1.000], mean action: 0.611 [0.000, 2.000], mean observation: 0.049 [-0.110, 0.228], loss: 0.000032, mean_absolute_error: 0.411002, mean_q: 0.621060\n",
      " 15080/50000: episode: 258, duration: 0.367s, episode steps: 45, steps per second: 123, episode reward: 0.849, mean reward: 0.019 [-0.006, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.116 [-0.571, 0.150], loss: 0.000028, mean_absolute_error: 0.416611, mean_q: 0.629992\n",
      " 15140/50000: episode: 259, duration: 0.315s, episode steps: 60, steps per second: 191, episode reward: 0.761, mean reward: 0.013 [-0.007, 1.000], mean action: 1.067 [0.000, 2.000], mean observation: -0.145 [-0.748, 0.180], loss: 0.000026, mean_absolute_error: 0.414003, mean_q: 0.625831\n",
      " 15159/50000: episode: 260, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 0.969, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.051 [-0.090, 0.223], loss: 0.000034, mean_absolute_error: 0.420106, mean_q: 0.634975\n",
      " 15227/50000: episode: 261, duration: 0.356s, episode steps: 68, steps per second: 191, episode reward: 0.632, mean reward: 0.009 [-0.010, 1.000], mean action: 1.103 [0.000, 2.000], mean observation: -0.207 [-0.976, 0.190], loss: 0.000033, mean_absolute_error: 0.411879, mean_q: 0.622646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15266/50000: episode: 262, duration: 0.300s, episode steps: 39, steps per second: 130, episode reward: 0.887, mean reward: 0.023 [-0.005, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.096 [-0.479, 0.160], loss: 0.000031, mean_absolute_error: 0.416523, mean_q: 0.630079\n",
      " 15302/50000: episode: 263, duration: 0.272s, episode steps: 36, steps per second: 132, episode reward: 0.900, mean reward: 0.025 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.092 [-0.439, 0.150], loss: 0.000029, mean_absolute_error: 0.416446, mean_q: 0.630687\n",
      " 15362/50000: episode: 264, duration: 0.312s, episode steps: 60, steps per second: 192, episode reward: 0.731, mean reward: 0.012 [-0.008, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.167 [-0.788, 0.180], loss: 0.000068, mean_absolute_error: 0.423806, mean_q: 0.640426\n",
      " 15406/50000: episode: 265, duration: 0.210s, episode steps: 44, steps per second: 210, episode reward: 0.823, mean reward: 0.019 [-0.007, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.138 [-0.210, 0.659], loss: 0.000072, mean_absolute_error: 0.414131, mean_q: 0.625058\n",
      " 15449/50000: episode: 266, duration: 0.155s, episode steps: 43, steps per second: 277, episode reward: 0.864, mean reward: 0.020 [-0.005, 1.000], mean action: 1.163 [0.000, 2.000], mean observation: -0.109 [-0.532, 0.180], loss: 0.000026, mean_absolute_error: 0.409741, mean_q: 0.619187\n",
      " 15506/50000: episode: 267, duration: 0.203s, episode steps: 57, steps per second: 281, episode reward: 0.752, mean reward: 0.013 [-0.008, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.161 [-0.752, 0.180], loss: 0.000035, mean_absolute_error: 0.416560, mean_q: 0.629604\n",
      " 15572/50000: episode: 268, duration: 0.404s, episode steps: 66, steps per second: 163, episode reward: 0.654, mean reward: 0.010 [-0.009, 1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.199 [-0.937, 0.210], loss: 0.000033, mean_absolute_error: 0.422765, mean_q: 0.639035\n",
      " 15615/50000: episode: 269, duration: 0.262s, episode steps: 43, steps per second: 164, episode reward: 0.845, mean reward: 0.020 [-0.006, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.124 [-0.190, 0.589], loss: 0.000042, mean_absolute_error: 0.417646, mean_q: 0.631116\n",
      " 15684/50000: episode: 270, duration: 0.394s, episode steps: 69, steps per second: 175, episode reward: 0.645, mean reward: 0.009 [-0.009, 1.000], mean action: 1.101 [0.000, 2.000], mean observation: -0.201 [-0.893, 0.180], loss: 0.000055, mean_absolute_error: 0.421527, mean_q: 0.636570\n",
      " 15735/50000: episode: 271, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 0.828, mean reward: 0.016 [-0.006, 1.000], mean action: 1.078 [0.000, 2.000], mean observation: -0.122 [-0.583, 0.150], loss: 0.000035, mean_absolute_error: 0.415989, mean_q: 0.628751\n",
      " 15736/50000: episode: 272, duration: 0.008s, episode steps: 1, steps per second: 118, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.021 [-0.052, 0.010], loss: 0.000046, mean_absolute_error: 0.402233, mean_q: 0.606686\n",
      " 15751/50000: episode: 273, duration: 0.060s, episode steps: 15, steps per second: 252, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.046 [-0.184, 0.090], loss: 0.000042, mean_absolute_error: 0.425520, mean_q: 0.642354\n",
      " 15755/50000: episode: 274, duration: 0.017s, episode steps: 4, steps per second: 231, episode reward: 0.997, mean reward: 0.249 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.039 [-0.107, 0.040], loss: 0.000030, mean_absolute_error: 0.436289, mean_q: 0.658489\n",
      " 15814/50000: episode: 275, duration: 0.231s, episode steps: 59, steps per second: 256, episode reward: 0.737, mean reward: 0.012 [-0.008, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.167 [-0.764, 0.160], loss: 0.000024, mean_absolute_error: 0.416142, mean_q: 0.628175\n",
      " 15854/50000: episode: 276, duration: 0.172s, episode steps: 40, steps per second: 233, episode reward: 0.886, mean reward: 0.022 [-0.005, 1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.099 [-0.451, 0.140], loss: 0.000024, mean_absolute_error: 0.432299, mean_q: 0.652927\n",
      " 15924/50000: episode: 277, duration: 0.394s, episode steps: 70, steps per second: 178, episode reward: 0.633, mean reward: 0.009 [-0.010, 1.000], mean action: 1.071 [0.000, 2.000], mean observation: -0.202 [-0.953, 0.200], loss: 0.000041, mean_absolute_error: 0.415798, mean_q: 0.627476\n",
      " 15964/50000: episode: 278, duration: 0.218s, episode steps: 40, steps per second: 184, episode reward: 0.868, mean reward: 0.022 [-0.006, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.104 [-0.200, 0.555], loss: 0.000065, mean_absolute_error: 0.427173, mean_q: 0.643735\n",
      " 15988/50000: episode: 279, duration: 0.209s, episode steps: 24, steps per second: 115, episode reward: 0.957, mean reward: 0.040 [-0.003, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.058 [-0.256, 0.100], loss: 0.000067, mean_absolute_error: 0.414591, mean_q: 0.625252\n",
      " 16000/50000: episode: 280, duration: 0.092s, episode steps: 12, steps per second: 131, episode reward: 0.985, mean reward: 0.082 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.040 [-0.159, 0.080], loss: 0.000030, mean_absolute_error: 0.421223, mean_q: 0.635022\n",
      " 16045/50000: episode: 281, duration: 0.318s, episode steps: 45, steps per second: 141, episode reward: 0.822, mean reward: 0.018 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.137 [-0.200, 0.648], loss: 0.000053, mean_absolute_error: 0.427829, mean_q: 0.646018\n",
      " 16091/50000: episode: 282, duration: 0.268s, episode steps: 46, steps per second: 172, episode reward: 0.842, mean reward: 0.018 [-0.006, 1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.121 [-0.576, 0.150], loss: 0.000037, mean_absolute_error: 0.418967, mean_q: 0.630847\n",
      " 16092/50000: episode: 283, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.012 [-0.014, -0.010], loss: 0.000059, mean_absolute_error: 0.446076, mean_q: 0.672558\n",
      " 16156/50000: episode: 284, duration: 0.347s, episode steps: 64, steps per second: 184, episode reward: 0.676, mean reward: 0.011 [-0.009, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.191 [-0.889, 0.170], loss: 0.000033, mean_absolute_error: 0.419620, mean_q: 0.633572\n",
      " 16201/50000: episode: 285, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 0.850, mean reward: 0.019 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.117 [-0.554, 0.160], loss: 0.000025, mean_absolute_error: 0.411857, mean_q: 0.620626\n",
      " 16237/50000: episode: 286, duration: 0.273s, episode steps: 36, steps per second: 132, episode reward: 0.895, mean reward: 0.025 [-0.005, 1.000], mean action: 0.806 [0.000, 2.000], mean observation: 0.096 [-0.170, 0.461], loss: 0.000030, mean_absolute_error: 0.404386, mean_q: 0.609725\n",
      " 16282/50000: episode: 287, duration: 0.554s, episode steps: 45, steps per second: 81, episode reward: 0.828, mean reward: 0.018 [-0.006, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.131 [-0.220, 0.644], loss: 0.000033, mean_absolute_error: 0.418819, mean_q: 0.630730\n",
      " 16289/50000: episode: 288, duration: 0.122s, episode steps: 7, steps per second: 58, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.040 [-0.050, 0.121], loss: 0.000051, mean_absolute_error: 0.415053, mean_q: 0.623840\n",
      " 16327/50000: episode: 289, duration: 0.368s, episode steps: 38, steps per second: 103, episode reward: 0.877, mean reward: 0.023 [-0.005, 1.000], mean action: 0.789 [0.000, 2.000], mean observation: 0.105 [-0.200, 0.529], loss: 0.000028, mean_absolute_error: 0.423685, mean_q: 0.637745\n",
      " 16359/50000: episode: 290, duration: 0.243s, episode steps: 32, steps per second: 132, episode reward: 0.907, mean reward: 0.028 [-0.004, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.090 [-0.180, 0.449], loss: 0.000028, mean_absolute_error: 0.425442, mean_q: 0.641682\n",
      " 16360/50000: episode: 291, duration: 0.008s, episode steps: 1, steps per second: 127, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.021 [-0.052, 0.010], loss: 0.000012, mean_absolute_error: 0.390854, mean_q: 0.590067\n",
      " 16363/50000: episode: 292, duration: 0.016s, episode steps: 3, steps per second: 191, episode reward: 0.998, mean reward: 0.333 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [-0.030, 0.102], loss: 0.000055, mean_absolute_error: 0.428004, mean_q: 0.646276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16415/50000: episode: 293, duration: 0.527s, episode steps: 52, steps per second: 99, episode reward: 0.787, mean reward: 0.015 [-0.007, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.147 [-0.706, 0.190], loss: 0.000030, mean_absolute_error: 0.421795, mean_q: 0.635572\n",
      " 16416/50000: episode: 294, duration: 0.051s, episode steps: 1, steps per second: 20, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.011 [-0.032, 0.010], loss: 0.000070, mean_absolute_error: 0.396817, mean_q: 0.586562\n",
      " 16481/50000: episode: 295, duration: 0.411s, episode steps: 65, steps per second: 158, episode reward: 0.673, mean reward: 0.010 [-0.009, 1.000], mean action: 1.108 [0.000, 2.000], mean observation: -0.189 [-0.911, 0.230], loss: 0.000047, mean_absolute_error: 0.421730, mean_q: 0.635499\n",
      " 16530/50000: episode: 296, duration: 0.255s, episode steps: 49, steps per second: 192, episode reward: 0.821, mean reward: 0.017 [-0.006, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.130 [-0.616, 0.150], loss: 0.000027, mean_absolute_error: 0.417196, mean_q: 0.628368\n",
      " 16585/50000: episode: 297, duration: 0.162s, episode steps: 55, steps per second: 339, episode reward: 0.752, mean reward: 0.014 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.165 [-0.780, 0.200], loss: 0.000057, mean_absolute_error: 0.418722, mean_q: 0.630720\n",
      " 16641/50000: episode: 298, duration: 0.218s, episode steps: 56, steps per second: 257, episode reward: 0.691, mean reward: 0.012 [-0.010, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.198 [-0.290, 0.980], loss: 0.000043, mean_absolute_error: 0.424458, mean_q: 0.639534\n",
      " 16675/50000: episode: 299, duration: 0.114s, episode steps: 34, steps per second: 299, episode reward: 0.924, mean reward: 0.027 [-0.003, 1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.078 [-0.320, 0.110], loss: 0.000048, mean_absolute_error: 0.419978, mean_q: 0.631255\n",
      " 16684/50000: episode: 300, duration: 0.029s, episode steps: 9, steps per second: 313, episode reward: 0.991, mean reward: 0.110 [-0.001, 1.000], mean action: 1.444 [0.000, 2.000], mean observation: -0.041 [-0.121, 0.040], loss: 0.000038, mean_absolute_error: 0.416161, mean_q: 0.626095\n",
      " 16714/50000: episode: 301, duration: 0.117s, episode steps: 30, steps per second: 256, episode reward: 0.926, mean reward: 0.031 [-0.004, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.079 [-0.160, 0.370], loss: 0.000045, mean_absolute_error: 0.419365, mean_q: 0.630496\n",
      " 16768/50000: episode: 302, duration: 0.307s, episode steps: 54, steps per second: 176, episode reward: 0.744, mean reward: 0.014 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.169 [-0.837, 0.220], loss: 0.000035, mean_absolute_error: 0.411611, mean_q: 0.619875\n",
      " 16816/50000: episode: 303, duration: 0.157s, episode steps: 48, steps per second: 306, episode reward: 0.785, mean reward: 0.016 [-0.008, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.155 [-0.250, 0.772], loss: 0.000030, mean_absolute_error: 0.408413, mean_q: 0.614604\n",
      " 16861/50000: episode: 304, duration: 0.169s, episode steps: 45, steps per second: 267, episode reward: 0.850, mean reward: 0.019 [-0.006, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.117 [-0.554, 0.170], loss: 0.000033, mean_absolute_error: 0.421551, mean_q: 0.634505\n",
      " 16901/50000: episode: 305, duration: 0.119s, episode steps: 40, steps per second: 336, episode reward: 0.857, mean reward: 0.021 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.119 [-0.200, 0.585], loss: 0.000039, mean_absolute_error: 0.421777, mean_q: 0.634953\n",
      " 16902/50000: episode: 306, duration: 0.006s, episode steps: 1, steps per second: 177, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [-0.010, 0.047], loss: 0.000014, mean_absolute_error: 0.393241, mean_q: 0.592428\n",
      " 16942/50000: episode: 307, duration: 0.135s, episode steps: 40, steps per second: 297, episode reward: 0.850, mean reward: 0.021 [-0.006, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.125 [-0.220, 0.607], loss: 0.000030, mean_absolute_error: 0.418584, mean_q: 0.629569\n",
      " 16973/50000: episode: 308, duration: 0.095s, episode steps: 31, steps per second: 327, episode reward: 0.922, mean reward: 0.030 [-0.004, 1.000], mean action: 0.774 [0.000, 2.000], mean observation: 0.084 [-0.160, 0.370], loss: 0.000030, mean_absolute_error: 0.436127, mean_q: 0.657019\n",
      " 17001/50000: episode: 309, duration: 0.094s, episode steps: 28, steps per second: 299, episode reward: 0.932, mean reward: 0.033 [-0.004, 1.000], mean action: 0.714 [0.000, 2.000], mean observation: 0.077 [-0.160, 0.356], loss: 0.000027, mean_absolute_error: 0.421417, mean_q: 0.633827\n",
      " 17051/50000: episode: 310, duration: 0.174s, episode steps: 50, steps per second: 288, episode reward: 0.785, mean reward: 0.016 [-0.007, 1.000], mean action: 0.900 [0.000, 2.000], mean observation: 0.159 [-0.230, 0.671], loss: 0.000078, mean_absolute_error: 0.425596, mean_q: 0.639479\n",
      " 17110/50000: episode: 311, duration: 0.277s, episode steps: 59, steps per second: 213, episode reward: 0.670, mean reward: 0.011 [-0.010, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.204 [-0.280, 0.995], loss: 0.000048, mean_absolute_error: 0.422919, mean_q: 0.635644\n",
      " 17111/50000: episode: 312, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.008 [-0.027, 0.010], loss: 0.000026, mean_absolute_error: 0.399524, mean_q: 0.607629\n",
      " 17140/50000: episode: 313, duration: 0.132s, episode steps: 29, steps per second: 220, episode reward: 0.931, mean reward: 0.032 [-0.004, 1.000], mean action: 0.759 [0.000, 2.000], mean observation: 0.075 [-0.160, 0.367], loss: 0.000027, mean_absolute_error: 0.420022, mean_q: 0.631602\n",
      " 17195/50000: episode: 314, duration: 0.230s, episode steps: 55, steps per second: 239, episode reward: 0.737, mean reward: 0.013 [-0.008, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.174 [-0.825, 0.240], loss: 0.000029, mean_absolute_error: 0.415709, mean_q: 0.625261\n",
      " 17209/50000: episode: 315, duration: 0.047s, episode steps: 14, steps per second: 297, episode reward: 0.981, mean reward: 0.070 [-0.002, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.044 [-0.080, 0.175], loss: 0.000029, mean_absolute_error: 0.437457, mean_q: 0.659440\n",
      " 17272/50000: episode: 316, duration: 0.196s, episode steps: 63, steps per second: 321, episode reward: 0.673, mean reward: 0.011 [-0.009, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.194 [-0.932, 0.240], loss: 0.000036, mean_absolute_error: 0.427763, mean_q: 0.642728\n",
      " 17323/50000: episode: 317, duration: 0.222s, episode steps: 51, steps per second: 230, episode reward: 0.729, mean reward: 0.014 [-0.009, 1.000], mean action: 0.843 [0.000, 2.000], mean observation: 0.188 [-0.270, 0.900], loss: 0.000028, mean_absolute_error: 0.428245, mean_q: 0.643684\n",
      " 17376/50000: episode: 318, duration: 0.163s, episode steps: 53, steps per second: 326, episode reward: 0.760, mean reward: 0.014 [-0.008, 1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.162 [-0.789, 0.240], loss: 0.000038, mean_absolute_error: 0.422451, mean_q: 0.634907\n",
      " 17377/50000: episode: 319, duration: 0.006s, episode steps: 1, steps per second: 172, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [-0.010, 0.042], loss: 0.000022, mean_absolute_error: 0.435276, mean_q: 0.655756\n",
      " 17378/50000: episode: 320, duration: 0.006s, episode steps: 1, steps per second: 170, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [-0.010, 0.043], loss: 0.000018, mean_absolute_error: 0.444312, mean_q: 0.671361\n",
      " 17391/50000: episode: 321, duration: 0.040s, episode steps: 13, steps per second: 321, episode reward: 0.982, mean reward: 0.076 [-0.002, 1.000], mean action: 1.692 [0.000, 2.000], mean observation: -0.041 [-0.177, 0.100], loss: 0.000022, mean_absolute_error: 0.418448, mean_q: 0.629612\n",
      " 17454/50000: episode: 322, duration: 0.200s, episode steps: 63, steps per second: 314, episode reward: 0.685, mean reward: 0.011 [-0.009, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.188 [-0.887, 0.230], loss: 0.000028, mean_absolute_error: 0.415658, mean_q: 0.623813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17499/50000: episode: 323, duration: 0.163s, episode steps: 45, steps per second: 276, episode reward: 0.842, mean reward: 0.019 [-0.006, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.121 [-0.595, 0.180], loss: 0.000053, mean_absolute_error: 0.419439, mean_q: 0.629358\n",
      " 17500/50000: episode: 324, duration: 0.007s, episode steps: 1, steps per second: 148, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.044 [-0.098, 0.010], loss: 0.000035, mean_absolute_error: 0.345216, mean_q: 0.507321\n",
      " 17544/50000: episode: 325, duration: 0.126s, episode steps: 44, steps per second: 348, episode reward: 0.798, mean reward: 0.018 [-0.007, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.157 [-0.260, 0.745], loss: 0.000034, mean_absolute_error: 0.420431, mean_q: 0.631164\n",
      " 17613/50000: episode: 326, duration: 0.202s, episode steps: 69, steps per second: 342, episode reward: 0.618, mean reward: 0.009 [-0.010, 1.000], mean action: 1.101 [0.000, 2.000], mean observation: -0.213 [-0.986, 0.240], loss: 0.000027, mean_absolute_error: 0.415141, mean_q: 0.623338\n",
      " 17614/50000: episode: 327, duration: 0.006s, episode steps: 1, steps per second: 167, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [-0.010, 0.030], loss: 0.000037, mean_absolute_error: 0.430578, mean_q: 0.646637\n",
      " 17640/50000: episode: 328, duration: 0.090s, episode steps: 26, steps per second: 290, episode reward: 0.944, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.066 [-0.311, 0.120], loss: 0.000039, mean_absolute_error: 0.413213, mean_q: 0.618651\n",
      " 17641/50000: episode: 329, duration: 0.007s, episode steps: 1, steps per second: 149, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [-0.010, 0.063], loss: 0.000043, mean_absolute_error: 0.416356, mean_q: 0.629779\n",
      " 17655/50000: episode: 330, duration: 0.043s, episode steps: 14, steps per second: 323, episode reward: 0.981, mean reward: 0.070 [-0.002, 1.000], mean action: 1.571 [1.000, 2.000], mean observation: -0.042 [-0.178, 0.080], loss: 0.000035, mean_absolute_error: 0.415783, mean_q: 0.623745\n",
      " 17701/50000: episode: 331, duration: 0.174s, episode steps: 46, steps per second: 264, episode reward: 0.834, mean reward: 0.018 [-0.006, 1.000], mean action: 1.174 [0.000, 2.000], mean observation: -0.127 [-0.602, 0.160], loss: 0.000025, mean_absolute_error: 0.426313, mean_q: 0.640682\n",
      " 17750/50000: episode: 332, duration: 0.152s, episode steps: 49, steps per second: 323, episode reward: 0.825, mean reward: 0.017 [-0.006, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.126 [-0.609, 0.180], loss: 0.000030, mean_absolute_error: 0.424973, mean_q: 0.637065\n",
      " 17751/50000: episode: 333, duration: 0.006s, episode steps: 1, steps per second: 176, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.010 [0.000, 0.020], loss: 0.000027, mean_absolute_error: 0.436882, mean_q: 0.657651\n",
      " 17752/50000: episode: 334, duration: 0.006s, episode steps: 1, steps per second: 166, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.011 [-0.032, 0.010], loss: 0.000020, mean_absolute_error: 0.458001, mean_q: 0.688487\n",
      " 17791/50000: episode: 335, duration: 0.119s, episode steps: 39, steps per second: 327, episode reward: 0.899, mean reward: 0.023 [-0.004, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.089 [-0.423, 0.130], loss: 0.000038, mean_absolute_error: 0.417293, mean_q: 0.625009\n",
      " 17842/50000: episode: 336, duration: 0.176s, episode steps: 51, steps per second: 290, episode reward: 0.788, mean reward: 0.015 [-0.007, 1.000], mean action: 1.098 [0.000, 2.000], mean observation: -0.147 [-0.730, 0.220], loss: 0.000031, mean_absolute_error: 0.419033, mean_q: 0.628845\n",
      " 17867/50000: episode: 337, duration: 0.087s, episode steps: 25, steps per second: 287, episode reward: 0.952, mean reward: 0.038 [-0.003, 1.000], mean action: 1.320 [0.000, 2.000], mean observation: -0.061 [-0.270, 0.110], loss: 0.000018, mean_absolute_error: 0.419082, mean_q: 0.628473\n",
      " 17878/50000: episode: 338, duration: 0.039s, episode steps: 11, steps per second: 283, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 1.727 [0.000, 2.000], mean observation: -0.037 [-0.150, 0.090], loss: 0.000029, mean_absolute_error: 0.424004, mean_q: 0.637041\n",
      " 17879/50000: episode: 339, duration: 0.006s, episode steps: 1, steps per second: 170, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [-0.010, 0.077], loss: 0.000035, mean_absolute_error: 0.485536, mean_q: 0.731222\n",
      " 17880/50000: episode: 340, duration: 0.006s, episode steps: 1, steps per second: 173, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.007 [-0.024, 0.010], loss: 0.000023, mean_absolute_error: 0.461111, mean_q: 0.695961\n",
      " 17909/50000: episode: 341, duration: 0.083s, episode steps: 29, steps per second: 347, episode reward: 0.931, mean reward: 0.032 [-0.003, 1.000], mean action: 1.276 [0.000, 2.000], mean observation: -0.077 [-0.348, 0.120], loss: 0.000041, mean_absolute_error: 0.421833, mean_q: 0.631507\n",
      " 17943/50000: episode: 342, duration: 0.099s, episode steps: 34, steps per second: 345, episode reward: 0.909, mean reward: 0.027 [-0.004, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.085 [-0.170, 0.429], loss: 0.000039, mean_absolute_error: 0.438727, mean_q: 0.656521\n",
      " 18000/50000: episode: 343, duration: 0.182s, episode steps: 57, steps per second: 314, episode reward: 0.718, mean reward: 0.013 [-0.010, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.152 [-0.310, 0.954], loss: 0.000037, mean_absolute_error: 0.425038, mean_q: 0.635391\n",
      " 18048/50000: episode: 344, duration: 0.147s, episode steps: 48, steps per second: 327, episode reward: 0.810, mean reward: 0.017 [-0.007, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.140 [-0.662, 0.210], loss: 0.000039, mean_absolute_error: 0.425953, mean_q: 0.636137\n",
      " 18099/50000: episode: 345, duration: 0.162s, episode steps: 51, steps per second: 314, episode reward: 0.726, mean reward: 0.014 [-0.009, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.185 [-0.300, 0.928], loss: 0.000041, mean_absolute_error: 0.422727, mean_q: 0.630582\n",
      " 18142/50000: episode: 346, duration: 0.136s, episode steps: 43, steps per second: 316, episode reward: 0.822, mean reward: 0.019 [-0.007, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.138 [-0.230, 0.685], loss: 0.000026, mean_absolute_error: 0.423536, mean_q: 0.633199\n",
      " 18143/50000: episode: 347, duration: 0.007s, episode steps: 1, steps per second: 142, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.007 [0.000, 0.014], loss: 0.000039, mean_absolute_error: 0.450252, mean_q: 0.663594\n",
      " 18159/50000: episode: 348, duration: 0.053s, episode steps: 16, steps per second: 303, episode reward: 0.978, mean reward: 0.061 [-0.002, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.044 [-0.080, 0.181], loss: 0.000088, mean_absolute_error: 0.424672, mean_q: 0.635881\n",
      " 18195/50000: episode: 349, duration: 0.142s, episode steps: 36, steps per second: 254, episode reward: 0.877, mean reward: 0.024 [-0.005, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.109 [-0.210, 0.544], loss: 0.000046, mean_absolute_error: 0.430943, mean_q: 0.643062\n",
      " 18236/50000: episode: 350, duration: 0.125s, episode steps: 41, steps per second: 328, episode reward: 0.871, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.110 [-0.484, 0.140], loss: 0.000029, mean_absolute_error: 0.431193, mean_q: 0.643159\n",
      " 18291/50000: episode: 351, duration: 0.163s, episode steps: 55, steps per second: 338, episode reward: 0.715, mean reward: 0.013 [-0.009, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.180 [-0.290, 0.901], loss: 0.000029, mean_absolute_error: 0.429886, mean_q: 0.641669\n",
      " 18343/50000: episode: 352, duration: 0.188s, episode steps: 52, steps per second: 277, episode reward: 0.753, mean reward: 0.014 [-0.008, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.172 [-0.240, 0.794], loss: 0.000023, mean_absolute_error: 0.424426, mean_q: 0.631959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18405/50000: episode: 353, duration: 0.216s, episode steps: 62, steps per second: 287, episode reward: 0.684, mean reward: 0.011 [-0.010, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.186 [-0.953, 0.250], loss: 0.000025, mean_absolute_error: 0.427919, mean_q: 0.637190\n",
      " 18414/50000: episode: 354, duration: 0.032s, episode steps: 9, steps per second: 279, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.035 [-0.136, 0.090], loss: 0.000017, mean_absolute_error: 0.452925, mean_q: 0.678207\n",
      " 18452/50000: episode: 355, duration: 0.112s, episode steps: 38, steps per second: 339, episode reward: 0.879, mean reward: 0.023 [-0.005, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.107 [-0.190, 0.488], loss: 0.000028, mean_absolute_error: 0.428303, mean_q: 0.638405\n",
      " 18498/50000: episode: 356, duration: 0.139s, episode steps: 46, steps per second: 332, episode reward: 0.847, mean reward: 0.018 [-0.005, 1.000], mean action: 1.174 [0.000, 2.000], mean observation: -0.118 [-0.545, 0.140], loss: 0.000026, mean_absolute_error: 0.434213, mean_q: 0.648171\n",
      " 18511/50000: episode: 357, duration: 0.049s, episode steps: 13, steps per second: 268, episode reward: 0.982, mean reward: 0.076 [-0.002, 1.000], mean action: 0.308 [0.000, 2.000], mean observation: 0.042 [-0.100, 0.179], loss: 0.000039, mean_absolute_error: 0.429242, mean_q: 0.637943\n",
      " 18576/50000: episode: 358, duration: 0.234s, episode steps: 65, steps per second: 277, episode reward: 0.647, mean reward: 0.010 [-0.010, 1.000], mean action: 1.123 [0.000, 2.000], mean observation: -0.205 [-0.969, 0.240], loss: 0.000041, mean_absolute_error: 0.439659, mean_q: 0.654551\n",
      " 18605/50000: episode: 359, duration: 0.088s, episode steps: 29, steps per second: 328, episode reward: 0.930, mean reward: 0.032 [-0.004, 1.000], mean action: 0.793 [0.000, 2.000], mean observation: 0.075 [-0.160, 0.370], loss: 0.000035, mean_absolute_error: 0.425796, mean_q: 0.633385\n",
      " 18652/50000: episode: 360, duration: 0.157s, episode steps: 47, steps per second: 300, episode reward: 0.790, mean reward: 0.017 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.153 [-0.250, 0.762], loss: 0.000039, mean_absolute_error: 0.422665, mean_q: 0.625232\n",
      " 18694/50000: episode: 361, duration: 0.121s, episode steps: 42, steps per second: 348, episode reward: 0.829, mean reward: 0.020 [-0.007, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.136 [-0.230, 0.670], loss: 0.000041, mean_absolute_error: 0.423541, mean_q: 0.627866\n",
      " 18715/50000: episode: 362, duration: 0.063s, episode steps: 21, steps per second: 335, episode reward: 0.961, mean reward: 0.046 [-0.003, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.055 [-0.261, 0.120], loss: 0.000032, mean_absolute_error: 0.426712, mean_q: 0.633404\n",
      " 18736/50000: episode: 363, duration: 0.084s, episode steps: 21, steps per second: 249, episode reward: 0.966, mean reward: 0.046 [-0.002, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.055 [-0.090, 0.211], loss: 0.000033, mean_absolute_error: 0.423308, mean_q: 0.626077\n",
      " 18777/50000: episode: 364, duration: 0.180s, episode steps: 41, steps per second: 228, episode reward: 0.870, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.105 [-0.526, 0.150], loss: 0.000032, mean_absolute_error: 0.431977, mean_q: 0.642494\n",
      " 18837/50000: episode: 365, duration: 0.203s, episode steps: 60, steps per second: 295, episode reward: 0.717, mean reward: 0.012 [-0.008, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.174 [-0.826, 0.210], loss: 0.000024, mean_absolute_error: 0.428680, mean_q: 0.635280\n",
      " 18892/50000: episode: 366, duration: 0.199s, episode steps: 55, steps per second: 277, episode reward: 0.770, mean reward: 0.014 [-0.007, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.152 [-0.725, 0.190], loss: 0.000048, mean_absolute_error: 0.437023, mean_q: 0.648732\n",
      " 18924/50000: episode: 367, duration: 0.111s, episode steps: 32, steps per second: 288, episode reward: 0.920, mean reward: 0.029 [-0.004, 1.000], mean action: 0.781 [0.000, 2.000], mean observation: 0.081 [-0.160, 0.391], loss: 0.000026, mean_absolute_error: 0.440360, mean_q: 0.653580\n",
      " 18956/50000: episode: 368, duration: 0.100s, episode steps: 32, steps per second: 320, episode reward: 0.922, mean reward: 0.029 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.081 [-0.365, 0.120], loss: 0.000017, mean_absolute_error: 0.426810, mean_q: 0.633373\n",
      " 18992/50000: episode: 369, duration: 0.124s, episode steps: 36, steps per second: 290, episode reward: 0.894, mean reward: 0.025 [-0.005, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.099 [-0.180, 0.453], loss: 0.000062, mean_absolute_error: 0.441540, mean_q: 0.655394\n",
      " 19058/50000: episode: 370, duration: 0.194s, episode steps: 66, steps per second: 339, episode reward: 0.659, mean reward: 0.010 [-0.009, 1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.197 [-0.922, 0.240], loss: 0.000036, mean_absolute_error: 0.430530, mean_q: 0.635502\n",
      " 19096/50000: episode: 371, duration: 0.109s, episode steps: 38, steps per second: 348, episode reward: 0.873, mean reward: 0.023 [-0.005, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.112 [-0.180, 0.525], loss: 0.000033, mean_absolute_error: 0.435735, mean_q: 0.645532\n",
      " 19150/50000: episode: 372, duration: 0.163s, episode steps: 54, steps per second: 331, episode reward: 0.709, mean reward: 0.013 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.194 [-0.270, 0.922], loss: 0.000035, mean_absolute_error: 0.438416, mean_q: 0.649948\n",
      " 19205/50000: episode: 373, duration: 0.204s, episode steps: 55, steps per second: 270, episode reward: 0.725, mean reward: 0.013 [-0.009, 1.000], mean action: 0.873 [0.000, 2.000], mean observation: 0.179 [-0.270, 0.883], loss: 0.000042, mean_absolute_error: 0.435199, mean_q: 0.644335\n",
      " 19264/50000: episode: 374, duration: 0.222s, episode steps: 59, steps per second: 266, episode reward: 0.714, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.178 [-0.862, 0.220], loss: 0.000031, mean_absolute_error: 0.431009, mean_q: 0.637298\n",
      " 19306/50000: episode: 375, duration: 0.131s, episode steps: 42, steps per second: 320, episode reward: 0.860, mean reward: 0.020 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.113 [-0.547, 0.160], loss: 0.000023, mean_absolute_error: 0.439698, mean_q: 0.650462\n",
      " 19347/50000: episode: 376, duration: 0.121s, episode steps: 41, steps per second: 339, episode reward: 0.838, mean reward: 0.020 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.131 [-0.230, 0.648], loss: 0.000028, mean_absolute_error: 0.429872, mean_q: 0.636497\n",
      " 19407/50000: episode: 377, duration: 0.171s, episode steps: 60, steps per second: 352, episode reward: 0.713, mean reward: 0.012 [-0.009, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.175 [-0.869, 0.210], loss: 0.000020, mean_absolute_error: 0.430486, mean_q: 0.635936\n",
      " 19461/50000: episode: 378, duration: 0.175s, episode steps: 54, steps per second: 308, episode reward: 0.708, mean reward: 0.013 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.195 [-0.260, 0.917], loss: 0.000020, mean_absolute_error: 0.437668, mean_q: 0.647423\n",
      " 19491/50000: episode: 379, duration: 0.090s, episode steps: 30, steps per second: 332, episode reward: 0.931, mean reward: 0.031 [-0.003, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.075 [-0.341, 0.120], loss: 0.000017, mean_absolute_error: 0.432384, mean_q: 0.640800\n",
      " 19503/50000: episode: 380, duration: 0.036s, episode steps: 12, steps per second: 337, episode reward: 0.984, mean reward: 0.082 [-0.002, 1.000], mean action: 0.333 [0.000, 2.000], mean observation: 0.040 [-0.090, 0.166], loss: 0.000019, mean_absolute_error: 0.441572, mean_q: 0.654045\n",
      " 19560/50000: episode: 381, duration: 0.167s, episode steps: 57, steps per second: 341, episode reward: 0.673, mean reward: 0.012 [-0.010, 1.000], mean action: 0.877 [0.000, 2.000], mean observation: 0.210 [-0.270, 0.982], loss: 0.000021, mean_absolute_error: 0.434735, mean_q: 0.642027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19605/50000: episode: 382, duration: 0.204s, episode steps: 45, steps per second: 221, episode reward: 0.850, mean reward: 0.019 [-0.005, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.118 [-0.537, 0.150], loss: 0.000027, mean_absolute_error: 0.430354, mean_q: 0.633632\n",
      " 19635/50000: episode: 383, duration: 0.101s, episode steps: 30, steps per second: 296, episode reward: 0.933, mean reward: 0.031 [-0.003, 1.000], mean action: 1.233 [0.000, 2.000], mean observation: -0.071 [-0.342, 0.120], loss: 0.000020, mean_absolute_error: 0.435960, mean_q: 0.643387\n",
      " 19636/50000: episode: 384, duration: 0.007s, episode steps: 1, steps per second: 148, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.010 [-0.020, 0.000], loss: 0.000021, mean_absolute_error: 0.398980, mean_q: 0.590033\n",
      " 19696/50000: episode: 385, duration: 0.176s, episode steps: 60, steps per second: 342, episode reward: 0.737, mean reward: 0.012 [-0.008, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.161 [-0.801, 0.210], loss: 0.000036, mean_absolute_error: 0.435247, mean_q: 0.639811\n",
      " 19755/50000: episode: 386, duration: 0.175s, episode steps: 59, steps per second: 337, episode reward: 0.743, mean reward: 0.013 [-0.008, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.161 [-0.776, 0.170], loss: 0.000020, mean_absolute_error: 0.437546, mean_q: 0.645504\n",
      " 19814/50000: episode: 387, duration: 0.186s, episode steps: 59, steps per second: 317, episode reward: 0.736, mean reward: 0.012 [-0.008, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.166 [-0.792, 0.200], loss: 0.000024, mean_absolute_error: 0.438577, mean_q: 0.648709\n",
      " 19833/50000: episode: 388, duration: 0.058s, episode steps: 19, steps per second: 328, episode reward: 0.967, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.050 [-0.130, 0.243], loss: 0.000016, mean_absolute_error: 0.434126, mean_q: 0.639194\n",
      " 19870/50000: episode: 389, duration: 0.111s, episode steps: 37, steps per second: 333, episode reward: 0.894, mean reward: 0.024 [-0.004, 1.000], mean action: 0.811 [0.000, 2.000], mean observation: 0.098 [-0.160, 0.443], loss: 0.000020, mean_absolute_error: 0.439907, mean_q: 0.648646\n",
      " 19909/50000: episode: 390, duration: 0.114s, episode steps: 39, steps per second: 342, episode reward: 0.870, mean reward: 0.022 [-0.006, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.109 [-0.210, 0.551], loss: 0.000020, mean_absolute_error: 0.433978, mean_q: 0.636868\n",
      " 19956/50000: episode: 391, duration: 0.139s, episode steps: 47, steps per second: 337, episode reward: 0.828, mean reward: 0.018 [-0.006, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.129 [-0.618, 0.160], loss: 0.000022, mean_absolute_error: 0.440044, mean_q: 0.649486\n",
      " 19999/50000: episode: 392, duration: 0.212s, episode steps: 43, steps per second: 203, episode reward: 0.826, mean reward: 0.019 [-0.007, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.138 [-0.210, 0.669], loss: 0.000020, mean_absolute_error: 0.441868, mean_q: 0.651746\n",
      " 20033/50000: episode: 393, duration: 0.164s, episode steps: 34, steps per second: 208, episode reward: 0.894, mean reward: 0.026 [-0.005, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.100 [-0.190, 0.490], loss: 0.000034, mean_absolute_error: 0.450185, mean_q: 0.667322\n",
      " 20089/50000: episode: 394, duration: 0.278s, episode steps: 56, steps per second: 202, episode reward: 0.692, mean reward: 0.012 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.200 [-0.260, 0.942], loss: 0.000035, mean_absolute_error: 0.447229, mean_q: 0.662916\n",
      " 20155/50000: episode: 395, duration: 0.266s, episode steps: 66, steps per second: 248, episode reward: 0.649, mean reward: 0.010 [-0.009, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.202 [-0.944, 0.210], loss: 0.000019, mean_absolute_error: 0.442360, mean_q: 0.653158\n",
      " 20204/50000: episode: 396, duration: 0.200s, episode steps: 49, steps per second: 245, episode reward: 0.811, mean reward: 0.017 [-0.007, 1.000], mean action: 1.163 [0.000, 2.000], mean observation: -0.136 [-0.662, 0.170], loss: 0.000029, mean_absolute_error: 0.442748, mean_q: 0.651654\n",
      " 20231/50000: episode: 397, duration: 0.109s, episode steps: 27, steps per second: 249, episode reward: 0.935, mean reward: 0.035 [-0.004, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.074 [-0.150, 0.362], loss: 0.000021, mean_absolute_error: 0.439336, mean_q: 0.648449\n",
      " 20285/50000: episode: 398, duration: 0.192s, episode steps: 54, steps per second: 282, episode reward: 0.774, mean reward: 0.014 [-0.007, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.151 [-0.730, 0.180], loss: 0.000022, mean_absolute_error: 0.434684, mean_q: 0.639969\n",
      " 20326/50000: episode: 399, duration: 0.148s, episode steps: 41, steps per second: 276, episode reward: 0.865, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.112 [-0.542, 0.180], loss: 0.000025, mean_absolute_error: 0.439489, mean_q: 0.647039\n",
      " 20383/50000: episode: 400, duration: 0.232s, episode steps: 57, steps per second: 246, episode reward: 0.666, mean reward: 0.012 [-0.010, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.216 [-0.260, 0.979], loss: 0.000026, mean_absolute_error: 0.444288, mean_q: 0.657610\n",
      " 20411/50000: episode: 401, duration: 0.092s, episode steps: 28, steps per second: 305, episode reward: 0.951, mean reward: 0.034 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.039 [-0.294, 0.140], loss: 0.000092, mean_absolute_error: 0.431488, mean_q: 0.632309\n",
      " 20473/50000: episode: 402, duration: 0.209s, episode steps: 62, steps per second: 296, episode reward: 0.695, mean reward: 0.011 [-0.009, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.186 [-0.853, 0.200], loss: 0.000027, mean_absolute_error: 0.441823, mean_q: 0.650590\n",
      " 20522/50000: episode: 403, duration: 0.142s, episode steps: 49, steps per second: 344, episode reward: 0.783, mean reward: 0.016 [-0.007, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.158 [-0.220, 0.734], loss: 0.000017, mean_absolute_error: 0.440183, mean_q: 0.646571\n",
      " 20548/50000: episode: 404, duration: 0.099s, episode steps: 26, steps per second: 263, episode reward: 0.950, mean reward: 0.037 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.061 [-0.287, 0.110], loss: 0.000021, mean_absolute_error: 0.436511, mean_q: 0.641433\n",
      " 20571/50000: episode: 405, duration: 0.085s, episode steps: 23, steps per second: 271, episode reward: 0.956, mean reward: 0.042 [-0.003, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.060 [-0.130, 0.269], loss: 0.000024, mean_absolute_error: 0.451007, mean_q: 0.667542\n",
      " 20613/50000: episode: 406, duration: 0.161s, episode steps: 42, steps per second: 261, episode reward: 0.844, mean reward: 0.020 [-0.006, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.126 [-0.190, 0.610], loss: 0.000019, mean_absolute_error: 0.439476, mean_q: 0.647359\n",
      " 20677/50000: episode: 407, duration: 0.185s, episode steps: 64, steps per second: 346, episode reward: 0.691, mean reward: 0.011 [-0.009, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.177 [-0.900, 0.200], loss: 0.000026, mean_absolute_error: 0.442103, mean_q: 0.651676\n",
      " 20691/50000: episode: 408, duration: 0.050s, episode steps: 14, steps per second: 279, episode reward: 0.981, mean reward: 0.070 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.044 [-0.169, 0.080], loss: 0.000018, mean_absolute_error: 0.436943, mean_q: 0.644230\n",
      " 20724/50000: episode: 409, duration: 0.098s, episode steps: 33, steps per second: 338, episode reward: 0.916, mean reward: 0.028 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.084 [-0.390, 0.120], loss: 0.000030, mean_absolute_error: 0.431366, mean_q: 0.632246\n",
      " 20764/50000: episode: 410, duration: 0.136s, episode steps: 40, steps per second: 294, episode reward: 0.857, mean reward: 0.021 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.118 [-0.210, 0.589], loss: 0.000015, mean_absolute_error: 0.436032, mean_q: 0.641558\n",
      " 20828/50000: episode: 411, duration: 0.197s, episode steps: 64, steps per second: 324, episode reward: 0.676, mean reward: 0.011 [-0.009, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.191 [-0.895, 0.220], loss: 0.000040, mean_absolute_error: 0.436843, mean_q: 0.642336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20829/50000: episode: 412, duration: 0.006s, episode steps: 1, steps per second: 154, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [-0.010, 0.037], loss: 0.000013, mean_absolute_error: 0.450564, mean_q: 0.681333\n",
      " 20876/50000: episode: 413, duration: 0.138s, episode steps: 47, steps per second: 341, episode reward: 0.799, mean reward: 0.017 [-0.007, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.147 [-0.210, 0.727], loss: 0.000018, mean_absolute_error: 0.440070, mean_q: 0.648474\n",
      " 20877/50000: episode: 414, duration: 0.007s, episode steps: 1, steps per second: 153, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.012 [-0.034, 0.010], loss: 0.000010, mean_absolute_error: 0.375375, mean_q: 0.538777\n",
      " 20878/50000: episode: 415, duration: 0.007s, episode steps: 1, steps per second: 149, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [-0.010, 0.099], loss: 0.000006, mean_absolute_error: 0.422281, mean_q: 0.609929\n",
      " 20929/50000: episode: 416, duration: 0.154s, episode steps: 51, steps per second: 332, episode reward: 0.761, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.166 [-0.220, 0.805], loss: 0.000023, mean_absolute_error: 0.447254, mean_q: 0.659343\n",
      " 20988/50000: episode: 417, duration: 0.169s, episode steps: 59, steps per second: 350, episode reward: 0.735, mean reward: 0.012 [-0.008, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.168 [-0.777, 0.210], loss: 0.000021, mean_absolute_error: 0.443434, mean_q: 0.654703\n",
      " 20989/50000: episode: 418, duration: 0.006s, episode steps: 1, steps per second: 167, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [-0.010, 0.022], loss: 0.000064, mean_absolute_error: 0.499559, mean_q: 0.743614\n",
      " 21042/50000: episode: 419, duration: 0.165s, episode steps: 53, steps per second: 322, episode reward: 0.792, mean reward: 0.015 [-0.007, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.142 [-0.666, 0.180], loss: 0.000021, mean_absolute_error: 0.443896, mean_q: 0.654752\n",
      " 21083/50000: episode: 420, duration: 0.126s, episode steps: 41, steps per second: 325, episode reward: 0.873, mean reward: 0.021 [-0.004, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.114 [-0.447, 0.130], loss: 0.000021, mean_absolute_error: 0.442174, mean_q: 0.652711\n",
      " 21100/50000: episode: 421, duration: 0.050s, episode steps: 17, steps per second: 343, episode reward: 0.973, mean reward: 0.057 [-0.002, 1.000], mean action: 0.588 [0.000, 2.000], mean observation: 0.046 [-0.110, 0.213], loss: 0.000017, mean_absolute_error: 0.437114, mean_q: 0.644916\n",
      " 21156/50000: episode: 422, duration: 0.229s, episode steps: 56, steps per second: 244, episode reward: 0.724, mean reward: 0.013 [-0.009, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.177 [-0.874, 0.240], loss: 0.000027, mean_absolute_error: 0.447220, mean_q: 0.658934\n",
      " 21164/50000: episode: 423, duration: 0.029s, episode steps: 8, steps per second: 280, episode reward: 0.991, mean reward: 0.124 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.080, 0.134], loss: 0.000039, mean_absolute_error: 0.458429, mean_q: 0.680041\n",
      " 21214/50000: episode: 424, duration: 0.152s, episode steps: 50, steps per second: 330, episode reward: 0.772, mean reward: 0.015 [-0.008, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.160 [-0.210, 0.784], loss: 0.000022, mean_absolute_error: 0.445781, mean_q: 0.657014\n",
      " 21250/50000: episode: 425, duration: 0.106s, episode steps: 36, steps per second: 338, episode reward: 0.900, mean reward: 0.025 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.088 [-0.446, 0.140], loss: 0.000023, mean_absolute_error: 0.450065, mean_q: 0.662409\n",
      " 21305/50000: episode: 426, duration: 0.157s, episode steps: 55, steps per second: 349, episode reward: 0.744, mean reward: 0.014 [-0.008, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.165 [-0.220, 0.828], loss: 0.000023, mean_absolute_error: 0.439392, mean_q: 0.646691\n",
      " 21346/50000: episode: 427, duration: 0.118s, episode steps: 41, steps per second: 347, episode reward: 0.877, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.104 [-0.478, 0.150], loss: 0.000028, mean_absolute_error: 0.452547, mean_q: 0.669275\n",
      " 21410/50000: episode: 428, duration: 0.191s, episode steps: 64, steps per second: 335, episode reward: 0.651, mean reward: 0.010 [-0.010, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.205 [-0.975, 0.250], loss: 0.000030, mean_absolute_error: 0.443740, mean_q: 0.655144\n",
      " 21411/50000: episode: 429, duration: 0.007s, episode steps: 1, steps per second: 142, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.011 [-0.031, 0.010], loss: 0.000018, mean_absolute_error: 0.419707, mean_q: 0.617977\n",
      " 21466/50000: episode: 430, duration: 0.157s, episode steps: 55, steps per second: 349, episode reward: 0.773, mean reward: 0.014 [-0.007, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.154 [-0.681, 0.200], loss: 0.000022, mean_absolute_error: 0.443507, mean_q: 0.652871\n",
      " 21467/50000: episode: 431, duration: 0.006s, episode steps: 1, steps per second: 175, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.010, 0.082], loss: 0.000021, mean_absolute_error: 0.441231, mean_q: 0.654407\n",
      " 21492/50000: episode: 432, duration: 0.076s, episode steps: 25, steps per second: 330, episode reward: 0.948, mean reward: 0.038 [-0.003, 1.000], mean action: 0.640 [0.000, 2.000], mean observation: 0.065 [-0.130, 0.305], loss: 0.000011, mean_absolute_error: 0.442365, mean_q: 0.651084\n",
      " 21545/50000: episode: 433, duration: 0.211s, episode steps: 53, steps per second: 251, episode reward: 0.753, mean reward: 0.014 [-0.008, 1.000], mean action: 0.849 [0.000, 2.000], mean observation: 0.168 [-0.200, 0.800], loss: 0.000020, mean_absolute_error: 0.442314, mean_q: 0.654644\n",
      " 21572/50000: episode: 434, duration: 0.103s, episode steps: 27, steps per second: 261, episode reward: 0.943, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.059 [-0.324, 0.150], loss: 0.000044, mean_absolute_error: 0.434834, mean_q: 0.640349\n",
      " 21618/50000: episode: 435, duration: 0.135s, episode steps: 46, steps per second: 341, episode reward: 0.824, mean reward: 0.018 [-0.006, 1.000], mean action: 0.848 [0.000, 2.000], mean observation: 0.132 [-0.210, 0.645], loss: 0.000024, mean_absolute_error: 0.448033, mean_q: 0.662490\n",
      " 21667/50000: episode: 436, duration: 0.140s, episode steps: 49, steps per second: 351, episode reward: 0.788, mean reward: 0.016 [-0.007, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.152 [-0.230, 0.738], loss: 0.000017, mean_absolute_error: 0.452822, mean_q: 0.670741\n",
      " 21697/50000: episode: 437, duration: 0.087s, episode steps: 30, steps per second: 344, episode reward: 0.928, mean reward: 0.031 [-0.004, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.074 [-0.361, 0.140], loss: 0.000033, mean_absolute_error: 0.450790, mean_q: 0.668269\n",
      " 21713/50000: episode: 438, duration: 0.048s, episode steps: 16, steps per second: 330, episode reward: 0.976, mean reward: 0.061 [-0.002, 1.000], mean action: 1.562 [0.000, 2.000], mean observation: -0.047 [-0.198, 0.100], loss: 0.000018, mean_absolute_error: 0.442520, mean_q: 0.652081\n",
      " 21763/50000: episode: 439, duration: 0.177s, episode steps: 50, steps per second: 282, episode reward: 0.777, mean reward: 0.016 [-0.008, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.157 [-0.200, 0.766], loss: 0.000025, mean_absolute_error: 0.450862, mean_q: 0.669043\n",
      " 21780/50000: episode: 440, duration: 0.050s, episode steps: 17, steps per second: 337, episode reward: 0.971, mean reward: 0.057 [-0.002, 1.000], mean action: 1.529 [0.000, 2.000], mean observation: -0.049 [-0.230, 0.130], loss: 0.000016, mean_absolute_error: 0.436786, mean_q: 0.641030\n",
      " 21802/50000: episode: 441, duration: 0.065s, episode steps: 22, steps per second: 341, episode reward: 0.962, mean reward: 0.044 [-0.002, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.050 [-0.120, 0.246], loss: 0.000017, mean_absolute_error: 0.448473, mean_q: 0.660286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21855/50000: episode: 442, duration: 0.180s, episode steps: 53, steps per second: 294, episode reward: 0.752, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.166 [-0.220, 0.821], loss: 0.000018, mean_absolute_error: 0.442154, mean_q: 0.654444\n",
      " 21870/50000: episode: 443, duration: 0.065s, episode steps: 15, steps per second: 231, episode reward: 0.978, mean reward: 0.065 [-0.002, 1.000], mean action: 1.533 [0.000, 2.000], mean observation: -0.043 [-0.196, 0.110], loss: 0.000020, mean_absolute_error: 0.444328, mean_q: 0.658600\n",
      " 21914/50000: episode: 444, duration: 0.162s, episode steps: 44, steps per second: 271, episode reward: 0.831, mean reward: 0.019 [-0.006, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.130 [-0.647, 0.200], loss: 0.000024, mean_absolute_error: 0.443442, mean_q: 0.655934\n",
      " 21954/50000: episode: 445, duration: 0.184s, episode steps: 40, steps per second: 217, episode reward: 0.880, mean reward: 0.022 [-0.005, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.088 [-0.529, 0.190], loss: 0.000021, mean_absolute_error: 0.448684, mean_q: 0.660743\n",
      " 21997/50000: episode: 446, duration: 0.173s, episode steps: 43, steps per second: 248, episode reward: 0.850, mean reward: 0.020 [-0.006, 1.000], mean action: 0.814 [0.000, 2.000], mean observation: 0.118 [-0.210, 0.590], loss: 0.000024, mean_absolute_error: 0.452652, mean_q: 0.670115\n",
      " 22014/50000: episode: 447, duration: 0.054s, episode steps: 17, steps per second: 317, episode reward: 0.973, mean reward: 0.057 [-0.002, 1.000], mean action: 0.471 [0.000, 2.000], mean observation: 0.048 [-0.100, 0.218], loss: 0.000034, mean_absolute_error: 0.448534, mean_q: 0.658962\n",
      " 22015/50000: episode: 448, duration: 0.006s, episode steps: 1, steps per second: 172, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.029 [-0.067, 0.010], loss: 0.000025, mean_absolute_error: 0.432581, mean_q: 0.636469\n",
      " 22016/50000: episode: 449, duration: 0.006s, episode steps: 1, steps per second: 156, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.045 [-0.100, 0.010], loss: 0.000073, mean_absolute_error: 0.464916, mean_q: 0.693746\n",
      " 22051/50000: episode: 450, duration: 0.112s, episode steps: 35, steps per second: 313, episode reward: 0.918, mean reward: 0.026 [-0.004, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.071 [-0.140, 0.389], loss: 0.000054, mean_absolute_error: 0.447295, mean_q: 0.661954\n",
      " 22112/50000: episode: 451, duration: 0.201s, episode steps: 61, steps per second: 303, episode reward: 0.647, mean reward: 0.011 [-0.010, 1.000], mean action: 1.131 [0.000, 2.000], mean observation: -0.218 [-0.976, 0.220], loss: 0.000030, mean_absolute_error: 0.450500, mean_q: 0.667465\n",
      " 22118/50000: episode: 452, duration: 0.021s, episode steps: 6, steps per second: 284, episode reward: 0.995, mean reward: 0.166 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.060, 0.115], loss: 0.000020, mean_absolute_error: 0.450803, mean_q: 0.670014\n",
      " 22171/50000: episode: 453, duration: 0.178s, episode steps: 53, steps per second: 299, episode reward: 0.743, mean reward: 0.014 [-0.009, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.172 [-0.260, 0.851], loss: 0.000074, mean_absolute_error: 0.441971, mean_q: 0.651274\n",
      " 22228/50000: episode: 454, duration: 0.172s, episode steps: 57, steps per second: 332, episode reward: 0.698, mean reward: 0.012 [-0.009, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.191 [-0.933, 0.230], loss: 0.000020, mean_absolute_error: 0.450705, mean_q: 0.670317\n",
      " 22283/50000: episode: 455, duration: 0.170s, episode steps: 55, steps per second: 324, episode reward: 0.759, mean reward: 0.014 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.154 [-0.797, 0.230], loss: 0.000020, mean_absolute_error: 0.443779, mean_q: 0.654880\n",
      " 22341/50000: episode: 456, duration: 0.167s, episode steps: 58, steps per second: 347, episode reward: 0.679, mean reward: 0.012 [-0.010, 1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.202 [-0.955, 0.220], loss: 0.000023, mean_absolute_error: 0.443694, mean_q: 0.656867\n",
      " 22381/50000: episode: 457, duration: 0.121s, episode steps: 40, steps per second: 331, episode reward: 0.869, mean reward: 0.022 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.109 [-0.190, 0.543], loss: 0.000014, mean_absolute_error: 0.444912, mean_q: 0.658933\n",
      " 22438/50000: episode: 458, duration: 0.186s, episode steps: 57, steps per second: 306, episode reward: 0.706, mean reward: 0.012 [-0.009, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.189 [-0.260, 0.887], loss: 0.000020, mean_absolute_error: 0.449051, mean_q: 0.666246\n",
      " 22461/50000: episode: 459, duration: 0.067s, episode steps: 23, steps per second: 345, episode reward: 0.957, mean reward: 0.042 [-0.003, 1.000], mean action: 0.696 [0.000, 2.000], mean observation: 0.056 [-0.110, 0.269], loss: 0.000019, mean_absolute_error: 0.440461, mean_q: 0.653480\n",
      " 22504/50000: episode: 460, duration: 0.182s, episode steps: 43, steps per second: 237, episode reward: 0.833, mean reward: 0.019 [-0.006, 1.000], mean action: 0.814 [0.000, 2.000], mean observation: 0.132 [-0.220, 0.635], loss: 0.000025, mean_absolute_error: 0.448833, mean_q: 0.667837\n",
      " 22552/50000: episode: 461, duration: 0.179s, episode steps: 48, steps per second: 268, episode reward: 0.811, mean reward: 0.017 [-0.007, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.136 [-0.220, 0.690], loss: 0.000015, mean_absolute_error: 0.447596, mean_q: 0.663131\n",
      " 22603/50000: episode: 462, duration: 0.160s, episode steps: 51, steps per second: 319, episode reward: 0.803, mean reward: 0.016 [-0.007, 1.000], mean action: 0.882 [0.000, 2.000], mean observation: 0.139 [-0.210, 0.656], loss: 0.000022, mean_absolute_error: 0.452694, mean_q: 0.672438\n",
      " 22604/50000: episode: 463, duration: 0.006s, episode steps: 1, steps per second: 155, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.006 [-0.010, -0.003], loss: 0.000012, mean_absolute_error: 0.436192, mean_q: 0.641608\n",
      " 22605/50000: episode: 464, duration: 0.007s, episode steps: 1, steps per second: 149, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.041 [-0.010, 0.092], loss: 0.000013, mean_absolute_error: 0.403822, mean_q: 0.588879\n",
      " 22644/50000: episode: 465, duration: 0.112s, episode steps: 39, steps per second: 348, episode reward: 0.901, mean reward: 0.023 [-0.004, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.095 [-0.130, 0.364], loss: 0.000026, mean_absolute_error: 0.445189, mean_q: 0.658663\n",
      " 22646/50000: episode: 466, duration: 0.009s, episode steps: 2, steps per second: 232, episode reward: 0.999, mean reward: 0.499 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.042 [-0.020, 0.101], loss: 0.000033, mean_absolute_error: 0.395876, mean_q: 0.583141\n",
      " 22691/50000: episode: 467, duration: 0.130s, episode steps: 45, steps per second: 346, episode reward: 0.836, mean reward: 0.019 [-0.006, 1.000], mean action: 0.822 [0.000, 2.000], mean observation: 0.126 [-0.200, 0.611], loss: 0.000014, mean_absolute_error: 0.449194, mean_q: 0.664533\n",
      " 22745/50000: episode: 468, duration: 0.157s, episode steps: 54, steps per second: 344, episode reward: 0.755, mean reward: 0.014 [-0.008, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.165 [-0.250, 0.783], loss: 0.000057, mean_absolute_error: 0.449473, mean_q: 0.666290\n",
      " 22772/50000: episode: 469, duration: 0.088s, episode steps: 27, steps per second: 306, episode reward: 0.939, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.072 [-0.329, 0.140], loss: 0.000045, mean_absolute_error: 0.442370, mean_q: 0.657206\n",
      " 22794/50000: episode: 470, duration: 0.066s, episode steps: 22, steps per second: 335, episode reward: 0.960, mean reward: 0.044 [-0.003, 1.000], mean action: 1.364 [0.000, 2.000], mean observation: -0.057 [-0.251, 0.100], loss: 0.000019, mean_absolute_error: 0.451229, mean_q: 0.669131\n",
      " 22831/50000: episode: 471, duration: 0.106s, episode steps: 37, steps per second: 348, episode reward: 0.876, mean reward: 0.024 [-0.005, 1.000], mean action: 1.216 [0.000, 2.000], mean observation: -0.109 [-0.536, 0.200], loss: 0.000028, mean_absolute_error: 0.445229, mean_q: 0.661505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22873/50000: episode: 472, duration: 0.133s, episode steps: 42, steps per second: 315, episode reward: 0.857, mean reward: 0.020 [-0.006, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.116 [-0.190, 0.560], loss: 0.000014, mean_absolute_error: 0.451528, mean_q: 0.670682\n",
      " 22935/50000: episode: 473, duration: 0.224s, episode steps: 62, steps per second: 277, episode reward: 0.647, mean reward: 0.010 [-0.010, 1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.213 [-0.998, 0.220], loss: 0.000030, mean_absolute_error: 0.442414, mean_q: 0.657003\n",
      " 22959/50000: episode: 474, duration: 0.081s, episode steps: 24, steps per second: 298, episode reward: 0.952, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.063 [-0.284, 0.130], loss: 0.000027, mean_absolute_error: 0.454670, mean_q: 0.679845\n",
      " 23018/50000: episode: 475, duration: 0.195s, episode steps: 59, steps per second: 302, episode reward: 0.664, mean reward: 0.011 [-0.010, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.209 [-0.240, 0.986], loss: 0.000026, mean_absolute_error: 0.455304, mean_q: 0.677940\n",
      " 23068/50000: episode: 476, duration: 0.148s, episode steps: 50, steps per second: 339, episode reward: 0.781, mean reward: 0.016 [-0.007, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.157 [-0.726, 0.210], loss: 0.000061, mean_absolute_error: 0.447912, mean_q: 0.666034\n",
      " 23081/50000: episode: 477, duration: 0.040s, episode steps: 13, steps per second: 322, episode reward: 0.983, mean reward: 0.076 [-0.002, 1.000], mean action: 0.462 [0.000, 2.000], mean observation: 0.042 [-0.080, 0.168], loss: 0.000044, mean_absolute_error: 0.455699, mean_q: 0.682244\n",
      " 23130/50000: episode: 478, duration: 0.154s, episode steps: 49, steps per second: 319, episode reward: 0.812, mean reward: 0.017 [-0.007, 1.000], mean action: 0.878 [0.000, 2.000], mean observation: 0.133 [-0.220, 0.679], loss: 0.000028, mean_absolute_error: 0.460910, mean_q: 0.686085\n",
      " 23142/50000: episode: 479, duration: 0.036s, episode steps: 12, steps per second: 336, episode reward: 0.985, mean reward: 0.082 [-0.002, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.041 [-0.080, 0.159], loss: 0.000012, mean_absolute_error: 0.452577, mean_q: 0.675241\n",
      " 23176/50000: episode: 480, duration: 0.100s, episode steps: 34, steps per second: 338, episode reward: 0.909, mean reward: 0.027 [-0.004, 1.000], mean action: 0.794 [0.000, 2.000], mean observation: 0.086 [-0.150, 0.427], loss: 0.000031, mean_absolute_error: 0.437167, mean_q: 0.649257\n",
      " 23220/50000: episode: 481, duration: 0.139s, episode steps: 44, steps per second: 318, episode reward: 0.825, mean reward: 0.019 [-0.007, 1.000], mean action: 0.841 [0.000, 2.000], mean observation: 0.135 [-0.220, 0.669], loss: 0.000023, mean_absolute_error: 0.450377, mean_q: 0.672137\n",
      " 23272/50000: episode: 482, duration: 0.159s, episode steps: 52, steps per second: 327, episode reward: 0.758, mean reward: 0.015 [-0.008, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.164 [-0.250, 0.816], loss: 0.000016, mean_absolute_error: 0.451387, mean_q: 0.672084\n",
      " 23293/50000: episode: 483, duration: 0.114s, episode steps: 21, steps per second: 183, episode reward: 0.961, mean reward: 0.046 [-0.003, 1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.055 [-0.264, 0.130], loss: 0.000013, mean_absolute_error: 0.453863, mean_q: 0.676711\n",
      " 23341/50000: episode: 484, duration: 0.158s, episode steps: 48, steps per second: 304, episode reward: 0.799, mean reward: 0.017 [-0.007, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.149 [-0.688, 0.200], loss: 0.000014, mean_absolute_error: 0.444728, mean_q: 0.660776\n",
      " 23364/50000: episode: 485, duration: 0.068s, episode steps: 23, steps per second: 337, episode reward: 0.959, mean reward: 0.042 [-0.003, 1.000], mean action: 0.696 [0.000, 2.000], mean observation: 0.056 [-0.110, 0.261], loss: 0.000092, mean_absolute_error: 0.445593, mean_q: 0.661720\n",
      " 23399/50000: episode: 486, duration: 0.101s, episode steps: 35, steps per second: 347, episode reward: 0.891, mean reward: 0.025 [-0.005, 1.000], mean action: 1.171 [0.000, 2.000], mean observation: -0.100 [-0.496, 0.180], loss: 0.000033, mean_absolute_error: 0.455912, mean_q: 0.680767\n",
      " 23450/50000: episode: 487, duration: 0.180s, episode steps: 51, steps per second: 283, episode reward: 0.774, mean reward: 0.015 [-0.008, 1.000], mean action: 1.118 [0.000, 2.000], mean observation: -0.156 [-0.774, 0.200], loss: 0.000021, mean_absolute_error: 0.441809, mean_q: 0.656774\n",
      " 23489/50000: episode: 488, duration: 0.167s, episode steps: 39, steps per second: 233, episode reward: 0.880, mean reward: 0.023 [-0.005, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.106 [-0.170, 0.483], loss: 0.000022, mean_absolute_error: 0.457903, mean_q: 0.685156\n",
      " 23541/50000: episode: 489, duration: 0.181s, episode steps: 52, steps per second: 287, episode reward: 0.728, mean reward: 0.014 [-0.009, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.187 [-0.250, 0.883], loss: 0.000018, mean_absolute_error: 0.447757, mean_q: 0.668308\n",
      " 23600/50000: episode: 490, duration: 0.261s, episode steps: 59, steps per second: 226, episode reward: 0.693, mean reward: 0.012 [-0.009, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.191 [-0.917, 0.220], loss: 0.000016, mean_absolute_error: 0.450258, mean_q: 0.673231\n",
      " 23643/50000: episode: 491, duration: 0.179s, episode steps: 43, steps per second: 240, episode reward: 0.854, mean reward: 0.020 [-0.006, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.115 [-0.190, 0.578], loss: 0.000052, mean_absolute_error: 0.448933, mean_q: 0.670063\n",
      " 23698/50000: episode: 492, duration: 0.184s, episode steps: 55, steps per second: 299, episode reward: 0.749, mean reward: 0.014 [-0.008, 1.000], mean action: 0.891 [0.000, 2.000], mean observation: 0.163 [-0.240, 0.812], loss: 0.000021, mean_absolute_error: 0.448187, mean_q: 0.669695\n",
      " 23745/50000: episode: 493, duration: 0.134s, episode steps: 47, steps per second: 350, episode reward: 0.812, mean reward: 0.017 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.140 [-0.220, 0.675], loss: 0.000024, mean_absolute_error: 0.455914, mean_q: 0.680425\n",
      " 23800/50000: episode: 494, duration: 0.158s, episode steps: 55, steps per second: 347, episode reward: 0.741, mean reward: 0.013 [-0.008, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.172 [-0.806, 0.200], loss: 0.000020, mean_absolute_error: 0.454117, mean_q: 0.678363\n",
      " 23835/50000: episode: 495, duration: 0.108s, episode steps: 35, steps per second: 324, episode reward: 0.913, mean reward: 0.026 [-0.004, 1.000], mean action: 0.829 [0.000, 2.000], mean observation: 0.082 [-0.150, 0.398], loss: 0.000035, mean_absolute_error: 0.446455, mean_q: 0.666638\n",
      " 23852/50000: episode: 496, duration: 0.051s, episode steps: 17, steps per second: 334, episode reward: 0.975, mean reward: 0.057 [-0.002, 1.000], mean action: 1.412 [0.000, 2.000], mean observation: -0.048 [-0.190, 0.080], loss: 0.000020, mean_absolute_error: 0.460190, mean_q: 0.687139\n",
      " 23886/50000: episode: 497, duration: 0.113s, episode steps: 34, steps per second: 302, episode reward: 0.909, mean reward: 0.027 [-0.004, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.087 [-0.160, 0.423], loss: 0.000024, mean_absolute_error: 0.458052, mean_q: 0.686260\n",
      " 23940/50000: episode: 498, duration: 0.161s, episode steps: 54, steps per second: 335, episode reward: 0.765, mean reward: 0.014 [-0.008, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.155 [-0.230, 0.776], loss: 0.000025, mean_absolute_error: 0.444723, mean_q: 0.663435\n",
      " 23976/50000: episode: 499, duration: 0.147s, episode steps: 36, steps per second: 244, episode reward: 0.901, mean reward: 0.025 [-0.004, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.091 [-0.160, 0.447], loss: 0.000015, mean_absolute_error: 0.447113, mean_q: 0.665989\n",
      " 23988/50000: episode: 500, duration: 0.044s, episode steps: 12, steps per second: 276, episode reward: 0.986, mean reward: 0.082 [-0.001, 1.000], mean action: 0.583 [0.000, 2.000], mean observation: 0.040 [-0.060, 0.145], loss: 0.000152, mean_absolute_error: 0.450617, mean_q: 0.674289\n",
      " 24018/50000: episode: 501, duration: 0.097s, episode steps: 30, steps per second: 310, episode reward: 0.929, mean reward: 0.031 [-0.004, 1.000], mean action: 1.233 [0.000, 2.000], mean observation: -0.075 [-0.357, 0.140], loss: 0.000043, mean_absolute_error: 0.447402, mean_q: 0.667591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24035/50000: episode: 502, duration: 0.056s, episode steps: 17, steps per second: 306, episode reward: 0.974, mean reward: 0.057 [-0.002, 1.000], mean action: 1.412 [0.000, 2.000], mean observation: -0.046 [-0.200, 0.090], loss: 0.000032, mean_absolute_error: 0.449253, mean_q: 0.673989\n",
      " 24055/50000: episode: 503, duration: 0.061s, episode steps: 20, steps per second: 325, episode reward: 0.966, mean reward: 0.048 [-0.002, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.052 [-0.239, 0.110], loss: 0.000140, mean_absolute_error: 0.466475, mean_q: 0.698592\n",
      " 24095/50000: episode: 504, duration: 0.113s, episode steps: 40, steps per second: 354, episode reward: 0.892, mean reward: 0.022 [-0.005, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.092 [-0.160, 0.452], loss: 0.000024, mean_absolute_error: 0.448417, mean_q: 0.670833\n",
      " 24157/50000: episode: 505, duration: 0.179s, episode steps: 62, steps per second: 347, episode reward: 0.657, mean reward: 0.011 [-0.010, 1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.206 [-0.980, 0.210], loss: 0.000017, mean_absolute_error: 0.447017, mean_q: 0.669178\n",
      " 24212/50000: episode: 506, duration: 0.161s, episode steps: 55, steps per second: 341, episode reward: 0.753, mean reward: 0.014 [-0.008, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.162 [-0.799, 0.200], loss: 0.000023, mean_absolute_error: 0.449902, mean_q: 0.673656\n",
      " 24250/50000: episode: 507, duration: 0.119s, episode steps: 38, steps per second: 320, episode reward: 0.894, mean reward: 0.024 [-0.005, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.092 [-0.461, 0.170], loss: 0.000040, mean_absolute_error: 0.449546, mean_q: 0.673256\n",
      " 24302/50000: episode: 508, duration: 0.157s, episode steps: 52, steps per second: 331, episode reward: 0.796, mean reward: 0.015 [-0.007, 1.000], mean action: 0.923 [0.000, 2.000], mean observation: 0.140 [-0.220, 0.691], loss: 0.000027, mean_absolute_error: 0.448502, mean_q: 0.669837\n",
      " 24339/50000: episode: 509, duration: 0.127s, episode steps: 37, steps per second: 291, episode reward: 0.896, mean reward: 0.024 [-0.004, 1.000], mean action: 1.162 [0.000, 2.000], mean observation: -0.094 [-0.449, 0.160], loss: 0.000020, mean_absolute_error: 0.455821, mean_q: 0.683682\n",
      " 24383/50000: episode: 510, duration: 0.148s, episode steps: 44, steps per second: 297, episode reward: 0.860, mean reward: 0.020 [-0.005, 1.000], mean action: 1.159 [0.000, 2.000], mean observation: -0.113 [-0.516, 0.170], loss: 0.000018, mean_absolute_error: 0.445997, mean_q: 0.668441\n",
      " 24419/50000: episode: 511, duration: 0.118s, episode steps: 36, steps per second: 305, episode reward: 0.898, mean reward: 0.025 [-0.005, 1.000], mean action: 1.139 [0.000, 2.000], mean observation: -0.093 [-0.459, 0.170], loss: 0.000020, mean_absolute_error: 0.452515, mean_q: 0.678542\n",
      " 24468/50000: episode: 512, duration: 0.172s, episode steps: 49, steps per second: 285, episode reward: 0.740, mean reward: 0.015 [-0.009, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.186 [-0.270, 0.885], loss: 0.000028, mean_absolute_error: 0.448666, mean_q: 0.671711\n",
      " 24520/50000: episode: 513, duration: 0.149s, episode steps: 52, steps per second: 349, episode reward: 0.769, mean reward: 0.015 [-0.008, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.158 [-0.230, 0.776], loss: 0.000020, mean_absolute_error: 0.446752, mean_q: 0.671349\n",
      " 24521/50000: episode: 514, duration: 0.006s, episode steps: 1, steps per second: 157, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.043 [-0.096, 0.010], loss: 0.000027, mean_absolute_error: 0.466136, mean_q: 0.700418\n",
      " 24541/50000: episode: 515, duration: 0.059s, episode steps: 20, steps per second: 337, episode reward: 0.965, mean reward: 0.048 [-0.002, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.055 [-0.236, 0.100], loss: 0.000022, mean_absolute_error: 0.445409, mean_q: 0.667147\n",
      " 24584/50000: episode: 516, duration: 0.140s, episode steps: 43, steps per second: 307, episode reward: 0.875, mean reward: 0.020 [-0.005, 1.000], mean action: 0.884 [0.000, 2.000], mean observation: 0.100 [-0.170, 0.501], loss: 0.000029, mean_absolute_error: 0.452961, mean_q: 0.678934\n",
      " 24596/50000: episode: 517, duration: 0.038s, episode steps: 12, steps per second: 316, episode reward: 0.986, mean reward: 0.082 [-0.002, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-0.060, 0.152], loss: 0.000019, mean_absolute_error: 0.449126, mean_q: 0.672578\n",
      " 24597/50000: episode: 518, duration: 0.007s, episode steps: 1, steps per second: 145, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.037 [-0.085, 0.010], loss: 0.000012, mean_absolute_error: 0.448052, mean_q: 0.668194\n",
      " 24652/50000: episode: 519, duration: 0.168s, episode steps: 55, steps per second: 328, episode reward: 0.723, mean reward: 0.013 [-0.009, 1.000], mean action: 0.927 [0.000, 2.000], mean observation: 0.181 [-0.260, 0.880], loss: 0.000022, mean_absolute_error: 0.455743, mean_q: 0.683290\n",
      " 24710/50000: episode: 520, duration: 0.177s, episode steps: 58, steps per second: 327, episode reward: 0.733, mean reward: 0.013 [-0.008, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.166 [-0.840, 0.210], loss: 0.000023, mean_absolute_error: 0.449302, mean_q: 0.674709\n",
      " 24765/50000: episode: 521, duration: 0.170s, episode steps: 55, steps per second: 324, episode reward: 0.756, mean reward: 0.014 [-0.008, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.161 [-0.775, 0.220], loss: 0.000020, mean_absolute_error: 0.453848, mean_q: 0.681852\n",
      " 24774/50000: episode: 522, duration: 0.027s, episode steps: 9, steps per second: 332, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 1.778 [1.000, 2.000], mean observation: -0.039 [-0.131, 0.070], loss: 0.000029, mean_absolute_error: 0.449201, mean_q: 0.672965\n",
      " 24827/50000: episode: 523, duration: 0.166s, episode steps: 53, steps per second: 319, episode reward: 0.760, mean reward: 0.014 [-0.008, 1.000], mean action: 0.906 [0.000, 2.000], mean observation: 0.160 [-0.250, 0.808], loss: 0.000030, mean_absolute_error: 0.451974, mean_q: 0.678976\n",
      " 24869/50000: episode: 524, duration: 0.121s, episode steps: 42, steps per second: 347, episode reward: 0.879, mean reward: 0.021 [-0.005, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.100 [-0.150, 0.474], loss: 0.000030, mean_absolute_error: 0.452569, mean_q: 0.676678\n",
      " 24870/50000: episode: 525, duration: 0.006s, episode steps: 1, steps per second: 168, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [-0.010, 0.055], loss: 0.000010, mean_absolute_error: 0.441282, mean_q: 0.662115\n",
      " 24897/50000: episode: 526, duration: 0.101s, episode steps: 27, steps per second: 268, episode reward: 0.945, mean reward: 0.035 [-0.003, 1.000], mean action: 0.704 [0.000, 2.000], mean observation: 0.066 [-0.100, 0.296], loss: 0.000025, mean_absolute_error: 0.454671, mean_q: 0.680484\n",
      " 24898/50000: episode: 527, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.039 [0.010, 0.068], loss: 0.000028, mean_absolute_error: 0.455257, mean_q: 0.676242\n",
      " 24963/50000: episode: 528, duration: 0.275s, episode steps: 65, steps per second: 237, episode reward: 0.656, mean reward: 0.010 [-0.010, 1.000], mean action: 1.092 [0.000, 2.000], mean observation: -0.198 [-0.969, 0.220], loss: 0.000023, mean_absolute_error: 0.458203, mean_q: 0.688873\n",
      " 24964/50000: episode: 529, duration: 0.007s, episode steps: 1, steps per second: 137, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [-0.010, 0.067], loss: 0.000023, mean_absolute_error: 0.445086, mean_q: 0.672424\n",
      " 24994/50000: episode: 530, duration: 0.141s, episode steps: 30, steps per second: 212, episode reward: 0.934, mean reward: 0.031 [-0.003, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.074 [-0.325, 0.130], loss: 0.000025, mean_absolute_error: 0.458811, mean_q: 0.689313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25034/50000: episode: 531, duration: 0.161s, episode steps: 40, steps per second: 248, episode reward: 0.899, mean reward: 0.022 [-0.004, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.088 [-0.150, 0.405], loss: 0.000023, mean_absolute_error: 0.457289, mean_q: 0.686705\n",
      " 25059/50000: episode: 532, duration: 0.085s, episode steps: 25, steps per second: 296, episode reward: 0.951, mean reward: 0.038 [-0.003, 1.000], mean action: 1.280 [0.000, 2.000], mean observation: -0.063 [-0.281, 0.120], loss: 0.000029, mean_absolute_error: 0.447046, mean_q: 0.670451\n",
      " 25115/50000: episode: 533, duration: 0.195s, episode steps: 56, steps per second: 287, episode reward: 0.747, mean reward: 0.013 [-0.008, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.163 [-0.808, 0.230], loss: 0.000023, mean_absolute_error: 0.458552, mean_q: 0.689352\n",
      " 25159/50000: episode: 534, duration: 0.128s, episode steps: 44, steps per second: 344, episode reward: 0.864, mean reward: 0.020 [-0.005, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.108 [-0.522, 0.180], loss: 0.000035, mean_absolute_error: 0.456895, mean_q: 0.686927\n",
      " 25199/50000: episode: 535, duration: 0.121s, episode steps: 40, steps per second: 332, episode reward: 0.893, mean reward: 0.022 [-0.005, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.090 [-0.170, 0.456], loss: 0.000025, mean_absolute_error: 0.457564, mean_q: 0.687647\n",
      " 25227/50000: episode: 536, duration: 0.096s, episode steps: 28, steps per second: 291, episode reward: 0.942, mean reward: 0.034 [-0.003, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.067 [-0.120, 0.310], loss: 0.000028, mean_absolute_error: 0.466448, mean_q: 0.702544\n",
      " 25284/50000: episode: 537, duration: 0.387s, episode steps: 57, steps per second: 147, episode reward: 0.709, mean reward: 0.012 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.169 [-0.270, 0.946], loss: 0.000031, mean_absolute_error: 0.458288, mean_q: 0.687471\n",
      " 25339/50000: episode: 538, duration: 0.289s, episode steps: 55, steps per second: 190, episode reward: 0.697, mean reward: 0.013 [-0.010, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.192 [-0.260, 0.966], loss: 0.000026, mean_absolute_error: 0.454341, mean_q: 0.683718\n",
      " 25388/50000: episode: 539, duration: 0.335s, episode steps: 49, steps per second: 146, episode reward: 0.758, mean reward: 0.015 [-0.009, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.167 [-0.270, 0.850], loss: 0.000024, mean_absolute_error: 0.462073, mean_q: 0.694671\n",
      " 25406/50000: episode: 540, duration: 0.133s, episode steps: 18, steps per second: 136, episode reward: 0.972, mean reward: 0.054 [-0.002, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.049 [-0.215, 0.100], loss: 0.000037, mean_absolute_error: 0.438713, mean_q: 0.656971\n",
      " 25426/50000: episode: 541, duration: 0.164s, episode steps: 20, steps per second: 122, episode reward: 0.971, mean reward: 0.049 [-0.002, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.052 [-0.080, 0.193], loss: 0.000041, mean_absolute_error: 0.450578, mean_q: 0.676205\n",
      " 25460/50000: episode: 542, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 0.926, mean reward: 0.027 [-0.003, 1.000], mean action: 0.853 [0.000, 2.000], mean observation: 0.073 [-0.120, 0.347], loss: 0.000018, mean_absolute_error: 0.453859, mean_q: 0.682329\n",
      " 25511/50000: episode: 543, duration: 0.219s, episode steps: 51, steps per second: 233, episode reward: 0.721, mean reward: 0.014 [-0.009, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.192 [-0.290, 0.923], loss: 0.000046, mean_absolute_error: 0.452856, mean_q: 0.682387\n",
      " 25512/50000: episode: 544, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.082, 0.010], loss: 0.000048, mean_absolute_error: 0.506656, mean_q: 0.764859\n",
      " 25551/50000: episode: 545, duration: 0.227s, episode steps: 39, steps per second: 172, episode reward: 0.859, mean reward: 0.022 [-0.006, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.118 [-0.220, 0.590], loss: 0.000036, mean_absolute_error: 0.451314, mean_q: 0.678316\n",
      " 25598/50000: episode: 546, duration: 0.234s, episode steps: 47, steps per second: 201, episode reward: 0.841, mean reward: 0.018 [-0.006, 1.000], mean action: 1.128 [0.000, 2.000], mean observation: -0.118 [-0.591, 0.180], loss: 0.000025, mean_absolute_error: 0.454010, mean_q: 0.683118\n",
      " 25643/50000: episode: 547, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 0.841, mean reward: 0.019 [-0.006, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.120 [-0.210, 0.610], loss: 0.000047, mean_absolute_error: 0.454077, mean_q: 0.682806\n",
      " 25685/50000: episode: 548, duration: 0.220s, episode steps: 42, steps per second: 191, episode reward: 0.873, mean reward: 0.021 [-0.005, 1.000], mean action: 0.905 [0.000, 2.000], mean observation: 0.101 [-0.190, 0.528], loss: 0.000024, mean_absolute_error: 0.462304, mean_q: 0.695859\n",
      " 25732/50000: episode: 549, duration: 0.173s, episode steps: 47, steps per second: 271, episode reward: 0.830, mean reward: 0.018 [-0.006, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.126 [-0.622, 0.190], loss: 0.000037, mean_absolute_error: 0.464431, mean_q: 0.699000\n",
      " 25765/50000: episode: 550, duration: 0.151s, episode steps: 33, steps per second: 219, episode reward: 0.910, mean reward: 0.028 [-0.004, 1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.088 [-0.432, 0.150], loss: 0.000042, mean_absolute_error: 0.458146, mean_q: 0.688806\n",
      " 25773/50000: episode: 551, duration: 0.033s, episode steps: 8, steps per second: 245, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.039 [-0.040, 0.122], loss: 0.000027, mean_absolute_error: 0.445627, mean_q: 0.668955\n",
      " 25833/50000: episode: 552, duration: 0.281s, episode steps: 60, steps per second: 214, episode reward: 0.725, mean reward: 0.012 [-0.008, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.172 [-0.793, 0.210], loss: 0.000028, mean_absolute_error: 0.457780, mean_q: 0.689856\n",
      " 25834/50000: episode: 553, duration: 0.006s, episode steps: 1, steps per second: 155, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.015 [-0.029, 0.000], loss: 0.000013, mean_absolute_error: 0.528787, mean_q: 0.796058\n",
      " 25850/50000: episode: 554, duration: 0.064s, episode steps: 16, steps per second: 250, episode reward: 0.981, mean reward: 0.061 [-0.001, 1.000], mean action: 1.438 [1.000, 2.000], mean observation: -0.048 [-0.145, 0.070], loss: 0.000045, mean_absolute_error: 0.448208, mean_q: 0.674180\n",
      " 25851/50000: episode: 555, duration: 0.007s, episode steps: 1, steps per second: 144, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.021 [0.010, 0.031], loss: 0.000019, mean_absolute_error: 0.502091, mean_q: 0.757583\n",
      " 25852/50000: episode: 556, duration: 0.006s, episode steps: 1, steps per second: 160, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.020 [-0.049, 0.010], loss: 0.000014, mean_absolute_error: 0.512383, mean_q: 0.773089\n",
      " 25894/50000: episode: 557, duration: 0.141s, episode steps: 42, steps per second: 298, episode reward: 0.880, mean reward: 0.021 [-0.005, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.096 [-0.160, 0.492], loss: 0.000044, mean_absolute_error: 0.458230, mean_q: 0.688855\n",
      " 25909/50000: episode: 558, duration: 0.060s, episode steps: 15, steps per second: 251, episode reward: 0.980, mean reward: 0.065 [-0.002, 1.000], mean action: 1.533 [0.000, 2.000], mean observation: -0.047 [-0.166, 0.090], loss: 0.000031, mean_absolute_error: 0.468410, mean_q: 0.706420\n",
      " 25990/50000: episode: 559, duration: 0.422s, episode steps: 81, steps per second: 192, episode reward: 0.675, mean reward: 0.008 [-0.009, 1.000], mean action: 1.074 [0.000, 2.000], mean observation: 0.099 [-0.290, 0.934], loss: 0.000026, mean_absolute_error: 0.459546, mean_q: 0.692105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26041/50000: episode: 560, duration: 0.231s, episode steps: 51, steps per second: 221, episode reward: 0.796, mean reward: 0.016 [-0.007, 1.000], mean action: 1.157 [0.000, 2.000], mean observation: -0.142 [-0.692, 0.180], loss: 0.000025, mean_absolute_error: 0.457132, mean_q: 0.688566\n",
      " 26060/50000: episode: 561, duration: 0.168s, episode steps: 19, steps per second: 113, episode reward: 0.971, mean reward: 0.051 [-0.002, 1.000], mean action: 1.368 [0.000, 2.000], mean observation: -0.049 [-0.212, 0.080], loss: 0.000033, mean_absolute_error: 0.458083, mean_q: 0.687429\n",
      " 26100/50000: episode: 562, duration: 0.223s, episode steps: 40, steps per second: 180, episode reward: 0.884, mean reward: 0.022 [-0.005, 1.000], mean action: 0.950 [0.000, 2.000], mean observation: 0.096 [-0.170, 0.497], loss: 0.000025, mean_absolute_error: 0.458831, mean_q: 0.691639\n",
      " 26141/50000: episode: 563, duration: 0.176s, episode steps: 41, steps per second: 232, episode reward: 0.875, mean reward: 0.021 [-0.005, 1.000], mean action: 0.829 [0.000, 2.000], mean observation: 0.103 [-0.170, 0.513], loss: 0.000035, mean_absolute_error: 0.468857, mean_q: 0.706605\n",
      " 26178/50000: episode: 564, duration: 0.270s, episode steps: 37, steps per second: 137, episode reward: 0.907, mean reward: 0.025 [-0.004, 1.000], mean action: 0.838 [0.000, 2.000], mean observation: 0.088 [-0.130, 0.381], loss: 0.000030, mean_absolute_error: 0.466902, mean_q: 0.702249\n",
      " 26234/50000: episode: 565, duration: 0.202s, episode steps: 56, steps per second: 278, episode reward: 0.736, mean reward: 0.013 [-0.009, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.153 [-0.280, 0.886], loss: 0.000028, mean_absolute_error: 0.457191, mean_q: 0.689524\n",
      " 26262/50000: episode: 566, duration: 0.083s, episode steps: 28, steps per second: 339, episode reward: 0.950, mean reward: 0.034 [-0.003, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.060 [-0.090, 0.271], loss: 0.000025, mean_absolute_error: 0.465419, mean_q: 0.701315\n",
      " 26305/50000: episode: 567, duration: 0.154s, episode steps: 43, steps per second: 280, episode reward: 0.857, mean reward: 0.020 [-0.006, 1.000], mean action: 1.186 [0.000, 2.000], mean observation: -0.115 [-0.553, 0.160], loss: 0.000026, mean_absolute_error: 0.457356, mean_q: 0.688347\n",
      " 26367/50000: episode: 568, duration: 0.274s, episode steps: 62, steps per second: 226, episode reward: 0.711, mean reward: 0.011 [-0.009, 1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.172 [-0.864, 0.210], loss: 0.000032, mean_absolute_error: 0.456632, mean_q: 0.686696\n",
      " 26459/50000: episode: 569, duration: 0.444s, episode steps: 92, steps per second: 207, episode reward: 0.633, mean reward: 0.007 [-0.010, 1.000], mean action: 1.054 [0.000, 2.000], mean observation: 0.092 [-0.290, 0.975], loss: 0.000042, mean_absolute_error: 0.456120, mean_q: 0.687565\n",
      " 26487/50000: episode: 570, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 0.937, mean reward: 0.033 [-0.003, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.072 [-0.331, 0.130], loss: 0.000027, mean_absolute_error: 0.463260, mean_q: 0.698573\n",
      " 26513/50000: episode: 571, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 0.949, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.063 [-0.287, 0.100], loss: 0.000045, mean_absolute_error: 0.457160, mean_q: 0.686849\n",
      " 26564/50000: episode: 572, duration: 0.225s, episode steps: 51, steps per second: 227, episode reward: 0.799, mean reward: 0.016 [-0.007, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.130 [-0.210, 0.697], loss: 0.000044, mean_absolute_error: 0.454664, mean_q: 0.684901\n",
      " 26624/50000: episode: 573, duration: 0.389s, episode steps: 60, steps per second: 154, episode reward: 0.723, mean reward: 0.012 [-0.008, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.171 [-0.814, 0.190], loss: 0.000046, mean_absolute_error: 0.450739, mean_q: 0.679058\n",
      " 26677/50000: episode: 574, duration: 0.304s, episode steps: 53, steps per second: 174, episode reward: 0.763, mean reward: 0.014 [-0.008, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.161 [-0.768, 0.190], loss: 0.000029, mean_absolute_error: 0.457611, mean_q: 0.688484\n",
      " 26707/50000: episode: 575, duration: 0.194s, episode steps: 30, steps per second: 154, episode reward: 0.940, mean reward: 0.031 [-0.003, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.066 [-0.308, 0.130], loss: 0.000045, mean_absolute_error: 0.470196, mean_q: 0.707834\n",
      " 26754/50000: episode: 576, duration: 0.216s, episode steps: 47, steps per second: 217, episode reward: 0.819, mean reward: 0.017 [-0.007, 1.000], mean action: 0.936 [0.000, 2.000], mean observation: 0.134 [-0.210, 0.657], loss: 0.000047, mean_absolute_error: 0.454006, mean_q: 0.683784\n",
      " 26792/50000: episode: 577, duration: 0.149s, episode steps: 38, steps per second: 255, episode reward: 0.868, mean reward: 0.023 [-0.006, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.114 [-0.210, 0.567], loss: 0.000036, mean_absolute_error: 0.455727, mean_q: 0.686882\n",
      " 26807/50000: episode: 578, duration: 0.059s, episode steps: 15, steps per second: 253, episode reward: 0.982, mean reward: 0.065 [-0.002, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.045 [-0.151, 0.060], loss: 0.000040, mean_absolute_error: 0.465592, mean_q: 0.703985\n",
      " 26911/50000: episode: 579, duration: 0.495s, episode steps: 104, steps per second: 210, episode reward: 0.588, mean reward: 0.006 [-0.009, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: 0.003 [-0.382, 0.924], loss: 0.000042, mean_absolute_error: 0.456131, mean_q: 0.687475\n",
      " 27024/50000: episode: 580, duration: 0.572s, episode steps: 113, steps per second: 198, episode reward: 0.557, mean reward: 0.005 [-0.009, 1.000], mean action: 1.044 [0.000, 2.000], mean observation: -0.009 [-0.416, 0.922], loss: 0.000033, mean_absolute_error: 0.456208, mean_q: 0.687695\n",
      " 27106/50000: episode: 581, duration: 0.459s, episode steps: 82, steps per second: 179, episode reward: 0.761, mean reward: 0.009 [-0.007, 1.000], mean action: 1.110 [0.000, 2.000], mean observation: 0.038 [-0.280, 0.732], loss: 0.000046, mean_absolute_error: 0.459034, mean_q: 0.691515\n",
      " 27144/50000: episode: 582, duration: 0.157s, episode steps: 38, steps per second: 242, episode reward: 0.892, mean reward: 0.023 [-0.005, 1.000], mean action: 0.895 [0.000, 2.000], mean observation: 0.095 [-0.180, 0.462], loss: 0.000028, mean_absolute_error: 0.460986, mean_q: 0.695046\n",
      " 27202/50000: episode: 583, duration: 0.229s, episode steps: 58, steps per second: 253, episode reward: 0.727, mean reward: 0.013 [-0.009, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.142 [-0.310, 0.910], loss: 0.000029, mean_absolute_error: 0.452971, mean_q: 0.682157\n",
      " 27240/50000: episode: 584, duration: 0.171s, episode steps: 38, steps per second: 222, episode reward: 0.896, mean reward: 0.024 [-0.004, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.093 [-0.160, 0.435], loss: 0.000036, mean_absolute_error: 0.461621, mean_q: 0.694529\n",
      " 27298/50000: episode: 585, duration: 0.236s, episode steps: 58, steps per second: 245, episode reward: 0.716, mean reward: 0.012 [-0.009, 1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.178 [-0.854, 0.200], loss: 0.000042, mean_absolute_error: 0.453268, mean_q: 0.683990\n",
      " 27344/50000: episode: 586, duration: 0.194s, episode steps: 46, steps per second: 238, episode reward: 0.878, mean reward: 0.019 [-0.005, 1.000], mean action: 0.848 [0.000, 2.000], mean observation: 0.093 [-0.160, 0.462], loss: 0.000038, mean_absolute_error: 0.464980, mean_q: 0.700774\n",
      " 27381/50000: episode: 587, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 0.878, mean reward: 0.024 [-0.005, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.110 [-0.200, 0.507], loss: 0.000053, mean_absolute_error: 0.464938, mean_q: 0.700647\n",
      " 27437/50000: episode: 588, duration: 0.200s, episode steps: 56, steps per second: 280, episode reward: 0.759, mean reward: 0.014 [-0.008, 1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.154 [-0.786, 0.200], loss: 0.000030, mean_absolute_error: 0.460136, mean_q: 0.694755\n",
      " 27438/50000: episode: 589, duration: 0.007s, episode steps: 1, steps per second: 148, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.030 [-0.010, 0.071], loss: 0.000012, mean_absolute_error: 0.499598, mean_q: 0.755483\n",
      " 27468/50000: episode: 590, duration: 0.111s, episode steps: 30, steps per second: 270, episode reward: 0.939, mean reward: 0.031 [-0.003, 1.000], mean action: 0.733 [0.000, 2.000], mean observation: 0.068 [-0.130, 0.300], loss: 0.000032, mean_absolute_error: 0.460222, mean_q: 0.695143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27497/50000: episode: 591, duration: 0.091s, episode steps: 29, steps per second: 318, episode reward: 0.927, mean reward: 0.032 [-0.004, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.078 [-0.140, 0.384], loss: 0.000030, mean_absolute_error: 0.451593, mean_q: 0.681044\n",
      " 27529/50000: episode: 592, duration: 0.097s, episode steps: 32, steps per second: 332, episode reward: 0.910, mean reward: 0.028 [-0.004, 1.000], mean action: 1.281 [0.000, 2.000], mean observation: -0.092 [-0.414, 0.180], loss: 0.000039, mean_absolute_error: 0.452945, mean_q: 0.682811\n",
      " 27582/50000: episode: 593, duration: 0.188s, episode steps: 53, steps per second: 282, episode reward: 0.762, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.153 [-0.230, 0.795], loss: 0.000048, mean_absolute_error: 0.454606, mean_q: 0.684420\n",
      " 27617/50000: episode: 594, duration: 0.142s, episode steps: 35, steps per second: 246, episode reward: 0.893, mean reward: 0.026 [-0.005, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.099 [-0.488, 0.190], loss: 0.000043, mean_absolute_error: 0.468236, mean_q: 0.705526\n",
      " 27647/50000: episode: 595, duration: 0.094s, episode steps: 30, steps per second: 321, episode reward: 0.931, mean reward: 0.031 [-0.003, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.074 [-0.350, 0.130], loss: 0.000041, mean_absolute_error: 0.463488, mean_q: 0.698663\n",
      " 27722/50000: episode: 596, duration: 0.228s, episode steps: 75, steps per second: 328, episode reward: 0.773, mean reward: 0.010 [-0.007, 1.000], mean action: 1.093 [0.000, 2.000], mean observation: 0.040 [-0.260, 0.722], loss: 0.000048, mean_absolute_error: 0.458721, mean_q: 0.692299\n",
      " 27750/50000: episode: 597, duration: 0.105s, episode steps: 28, steps per second: 266, episode reward: 0.955, mean reward: 0.034 [-0.002, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.058 [-0.070, 0.232], loss: 0.000056, mean_absolute_error: 0.465677, mean_q: 0.700532\n",
      " 27778/50000: episode: 598, duration: 0.102s, episode steps: 28, steps per second: 276, episode reward: 0.943, mean reward: 0.034 [-0.003, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.066 [-0.120, 0.309], loss: 0.000030, mean_absolute_error: 0.458905, mean_q: 0.692010\n",
      " 27828/50000: episode: 599, duration: 0.145s, episode steps: 50, steps per second: 345, episode reward: 0.837, mean reward: 0.017 [-0.006, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.093 [-0.210, 0.606], loss: 0.000034, mean_absolute_error: 0.461713, mean_q: 0.696928\n",
      " 27865/50000: episode: 600, duration: 0.107s, episode steps: 37, steps per second: 346, episode reward: 0.895, mean reward: 0.024 [-0.004, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.097 [-0.180, 0.435], loss: 0.000042, mean_absolute_error: 0.454801, mean_q: 0.686085\n",
      " 27883/50000: episode: 601, duration: 0.054s, episode steps: 18, steps per second: 336, episode reward: 0.980, mean reward: 0.054 [-0.001, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.047 [-0.040, 0.142], loss: 0.000045, mean_absolute_error: 0.465329, mean_q: 0.702333\n",
      " 27940/50000: episode: 602, duration: 0.164s, episode steps: 57, steps per second: 348, episode reward: 0.697, mean reward: 0.012 [-0.009, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.195 [-0.900, 0.200], loss: 0.000033, mean_absolute_error: 0.460874, mean_q: 0.695090\n",
      " 27976/50000: episode: 603, duration: 0.107s, episode steps: 36, steps per second: 337, episode reward: 0.896, mean reward: 0.025 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.083 [-0.210, 0.480], loss: 0.000055, mean_absolute_error: 0.458983, mean_q: 0.691004\n",
      " 28019/50000: episode: 604, duration: 0.140s, episode steps: 43, steps per second: 308, episode reward: 0.863, mean reward: 0.020 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.097 [-0.200, 0.549], loss: 0.000087, mean_absolute_error: 0.461323, mean_q: 0.695527\n",
      " 28061/50000: episode: 605, duration: 0.122s, episode steps: 42, steps per second: 343, episode reward: 0.915, mean reward: 0.022 [-0.003, 1.000], mean action: 0.929 [0.000, 2.000], mean observation: 0.073 [-0.110, 0.343], loss: 0.000055, mean_absolute_error: 0.454792, mean_q: 0.685780\n",
      " 28106/50000: episode: 606, duration: 0.193s, episode steps: 45, steps per second: 233, episode reward: 0.872, mean reward: 0.019 [-0.005, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.101 [-0.150, 0.473], loss: 0.000035, mean_absolute_error: 0.457948, mean_q: 0.691144\n",
      " 28144/50000: episode: 607, duration: 0.157s, episode steps: 38, steps per second: 242, episode reward: 0.920, mean reward: 0.024 [-0.003, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.076 [-0.110, 0.331], loss: 0.000038, mean_absolute_error: 0.459648, mean_q: 0.693651\n",
      " 28174/50000: episode: 608, duration: 0.086s, episode steps: 30, steps per second: 347, episode reward: 0.950, mean reward: 0.032 [-0.002, 1.000], mean action: 0.900 [0.000, 2.000], mean observation: 0.060 [-0.070, 0.249], loss: 0.000060, mean_absolute_error: 0.463844, mean_q: 0.698379\n",
      " 28201/50000: episode: 609, duration: 0.077s, episode steps: 27, steps per second: 350, episode reward: 0.944, mean reward: 0.035 [-0.003, 1.000], mean action: 1.296 [0.000, 2.000], mean observation: -0.067 [-0.302, 0.100], loss: 0.000037, mean_absolute_error: 0.459310, mean_q: 0.693298\n",
      " 28202/50000: episode: 610, duration: 0.006s, episode steps: 1, steps per second: 175, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.042 [-0.095, 0.010], loss: 0.000014, mean_absolute_error: 0.408122, mean_q: 0.621084\n",
      " 28267/50000: episode: 611, duration: 0.185s, episode steps: 65, steps per second: 350, episode reward: 0.627, mean reward: 0.010 [-0.010, 1.000], mean action: 1.123 [0.000, 2.000], mean observation: -0.219 [-0.986, 0.210], loss: 0.000039, mean_absolute_error: 0.455232, mean_q: 0.686546\n",
      " 28298/50000: episode: 612, duration: 0.091s, episode steps: 31, steps per second: 342, episode reward: 0.940, mean reward: 0.030 [-0.003, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.064 [-0.110, 0.300], loss: 0.000026, mean_absolute_error: 0.458873, mean_q: 0.692102\n",
      " 28335/50000: episode: 613, duration: 0.113s, episode steps: 37, steps per second: 328, episode reward: 0.899, mean reward: 0.024 [-0.004, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.091 [-0.434, 0.150], loss: 0.000040, mean_absolute_error: 0.459754, mean_q: 0.693645\n",
      " 28336/50000: episode: 614, duration: 0.007s, episode steps: 1, steps per second: 144, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.004 [-0.019, 0.010], loss: 0.000016, mean_absolute_error: 0.458930, mean_q: 0.684250\n",
      " 28373/50000: episode: 615, duration: 0.122s, episode steps: 37, steps per second: 304, episode reward: 0.885, mean reward: 0.024 [-0.005, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.098 [-0.509, 0.190], loss: 0.000032, mean_absolute_error: 0.448057, mean_q: 0.676088\n",
      " 28396/50000: episode: 616, duration: 0.075s, episode steps: 23, steps per second: 305, episode reward: 0.967, mean reward: 0.042 [-0.002, 1.000], mean action: 1.391 [0.000, 2.000], mean observation: -0.032 [-0.223, 0.120], loss: 0.000051, mean_absolute_error: 0.464038, mean_q: 0.700037\n",
      " 28419/50000: episode: 617, duration: 0.102s, episode steps: 23, steps per second: 225, episode reward: 0.952, mean reward: 0.041 [-0.003, 1.000], mean action: 1.348 [0.000, 2.000], mean observation: -0.060 [-0.300, 0.140], loss: 0.000067, mean_absolute_error: 0.468269, mean_q: 0.705066\n",
      " 28517/50000: episode: 618, duration: 0.371s, episode steps: 98, steps per second: 264, episode reward: 0.661, mean reward: 0.007 [-0.008, 1.000], mean action: 1.092 [0.000, 2.000], mean observation: 0.005 [-0.330, 0.799], loss: 0.000041, mean_absolute_error: 0.459162, mean_q: 0.692050\n",
      " 28554/50000: episode: 619, duration: 0.147s, episode steps: 37, steps per second: 251, episode reward: 0.867, mean reward: 0.023 [-0.006, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.118 [-0.569, 0.190], loss: 0.000034, mean_absolute_error: 0.451489, mean_q: 0.681159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28608/50000: episode: 620, duration: 0.238s, episode steps: 54, steps per second: 227, episode reward: 0.729, mean reward: 0.013 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.180 [-0.842, 0.230], loss: 0.000058, mean_absolute_error: 0.452772, mean_q: 0.682891\n",
      " 28694/50000: episode: 621, duration: 0.458s, episode steps: 86, steps per second: 188, episode reward: 0.706, mean reward: 0.008 [-0.008, 1.000], mean action: 1.105 [0.000, 2.000], mean observation: 0.040 [-0.320, 0.847], loss: 0.000049, mean_absolute_error: 0.458233, mean_q: 0.691100\n",
      " 28718/50000: episode: 622, duration: 0.162s, episode steps: 24, steps per second: 148, episode reward: 0.962, mean reward: 0.040 [-0.002, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.053 [-0.233, 0.110], loss: 0.000047, mean_absolute_error: 0.456522, mean_q: 0.687664\n",
      " 28758/50000: episode: 623, duration: 0.315s, episode steps: 40, steps per second: 127, episode reward: 0.919, mean reward: 0.023 [-0.003, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.076 [-0.100, 0.309], loss: 0.000044, mean_absolute_error: 0.459093, mean_q: 0.692082\n",
      " 28759/50000: episode: 624, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [-0.010, 0.088], loss: 0.000011, mean_absolute_error: 0.464622, mean_q: 0.702342\n",
      " 28805/50000: episode: 625, duration: 0.258s, episode steps: 46, steps per second: 178, episode reward: 0.821, mean reward: 0.018 [-0.007, 1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.133 [-0.669, 0.210], loss: 0.000035, mean_absolute_error: 0.448962, mean_q: 0.676413\n",
      " 28844/50000: episode: 626, duration: 0.256s, episode steps: 39, steps per second: 153, episode reward: 0.869, mean reward: 0.022 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.110 [-0.549, 0.200], loss: 0.000045, mean_absolute_error: 0.455630, mean_q: 0.685765\n",
      " 28871/50000: episode: 627, duration: 0.210s, episode steps: 27, steps per second: 129, episode reward: 0.939, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.068 [-0.336, 0.160], loss: 0.000041, mean_absolute_error: 0.462847, mean_q: 0.698806\n",
      " 28873/50000: episode: 628, duration: 0.062s, episode steps: 2, steps per second: 32, episode reward: 0.999, mean reward: 0.499 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [-0.020, 0.102], loss: 0.000075, mean_absolute_error: 0.465023, mean_q: 0.702524\n",
      " 28918/50000: episode: 629, duration: 0.438s, episode steps: 45, steps per second: 103, episode reward: 0.906, mean reward: 0.020 [-0.003, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.082 [-0.070, 0.309], loss: 0.000060, mean_absolute_error: 0.460687, mean_q: 0.694239\n",
      " 28978/50000: episode: 630, duration: 0.364s, episode steps: 60, steps per second: 165, episode reward: 0.771, mean reward: 0.013 [-0.007, 1.000], mean action: 0.933 [0.000, 2.000], mean observation: 0.138 [-0.200, 0.735], loss: 0.000034, mean_absolute_error: 0.458227, mean_q: 0.691496\n",
      " 28996/50000: episode: 631, duration: 0.127s, episode steps: 18, steps per second: 142, episode reward: 0.974, mean reward: 0.054 [-0.002, 1.000], mean action: 1.444 [0.000, 2.000], mean observation: -0.046 [-0.195, 0.090], loss: 0.000041, mean_absolute_error: 0.451471, mean_q: 0.680965\n",
      " 29017/50000: episode: 632, duration: 0.087s, episode steps: 21, steps per second: 241, episode reward: 0.969, mean reward: 0.046 [-0.002, 1.000], mean action: 0.762 [0.000, 2.000], mean observation: 0.052 [-0.070, 0.200], loss: 0.000049, mean_absolute_error: 0.456353, mean_q: 0.689137\n",
      " 29061/50000: episode: 633, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 0.814, mean reward: 0.018 [-0.007, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.145 [-0.690, 0.220], loss: 0.000036, mean_absolute_error: 0.459940, mean_q: 0.694736\n",
      " 29092/50000: episode: 634, duration: 0.308s, episode steps: 31, steps per second: 101, episode reward: 0.919, mean reward: 0.030 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.086 [-0.382, 0.130], loss: 0.000046, mean_absolute_error: 0.456001, mean_q: 0.686629\n",
      " 29093/50000: episode: 635, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.021 [-0.052, 0.010], loss: 0.000123, mean_absolute_error: 0.470289, mean_q: 0.706081\n",
      " 29175/50000: episode: 636, duration: 0.489s, episode steps: 82, steps per second: 168, episode reward: 0.706, mean reward: 0.009 [-0.009, 1.000], mean action: 1.110 [0.000, 2.000], mean observation: 0.057 [-0.300, 0.879], loss: 0.000034, mean_absolute_error: 0.454950, mean_q: 0.686866\n",
      " 29176/50000: episode: 637, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [-0.010, 0.022], loss: 0.000024, mean_absolute_error: 0.424199, mean_q: 0.632410\n",
      " 29223/50000: episode: 638, duration: 0.313s, episode steps: 47, steps per second: 150, episode reward: 0.868, mean reward: 0.018 [-0.005, 1.000], mean action: 0.915 [0.000, 2.000], mean observation: 0.099 [-0.170, 0.497], loss: 0.000053, mean_absolute_error: 0.458957, mean_q: 0.693291\n",
      " 29236/50000: episode: 639, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 0.983, mean reward: 0.076 [-0.002, 1.000], mean action: 1.615 [1.000, 2.000], mean observation: -0.042 [-0.163, 0.080], loss: 0.000051, mean_absolute_error: 0.467938, mean_q: 0.704923\n",
      " 29285/50000: episode: 640, duration: 0.281s, episode steps: 49, steps per second: 174, episode reward: 0.746, mean reward: 0.015 [-0.009, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.180 [-0.260, 0.871], loss: 0.000031, mean_absolute_error: 0.450551, mean_q: 0.680752\n",
      " 29328/50000: episode: 641, duration: 0.279s, episode steps: 43, steps per second: 154, episode reward: 0.850, mean reward: 0.020 [-0.006, 1.000], mean action: 1.186 [0.000, 2.000], mean observation: -0.122 [-0.561, 0.180], loss: 0.000042, mean_absolute_error: 0.460862, mean_q: 0.696273\n",
      " 29337/50000: episode: 642, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 0.991, mean reward: 0.110 [-0.001, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.044 [-0.030, 0.116], loss: 0.000044, mean_absolute_error: 0.440497, mean_q: 0.663764\n",
      " 29392/50000: episode: 643, duration: 0.298s, episode steps: 55, steps per second: 184, episode reward: 0.727, mean reward: 0.013 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.183 [-0.817, 0.220], loss: 0.000054, mean_absolute_error: 0.460903, mean_q: 0.695245\n",
      " 29427/50000: episode: 644, duration: 0.260s, episode steps: 35, steps per second: 135, episode reward: 0.905, mean reward: 0.026 [-0.004, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.088 [-0.433, 0.160], loss: 0.000048, mean_absolute_error: 0.455684, mean_q: 0.687462\n",
      " 29461/50000: episode: 645, duration: 0.170s, episode steps: 34, steps per second: 199, episode reward: 0.913, mean reward: 0.027 [-0.004, 1.000], mean action: 1.235 [0.000, 2.000], mean observation: -0.087 [-0.383, 0.140], loss: 0.000038, mean_absolute_error: 0.458056, mean_q: 0.691392\n",
      " 29513/50000: episode: 646, duration: 0.288s, episode steps: 52, steps per second: 181, episode reward: 0.767, mean reward: 0.015 [-0.008, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.143 [-0.260, 0.831], loss: 0.000028, mean_absolute_error: 0.452730, mean_q: 0.683937\n",
      " 29556/50000: episode: 647, duration: 0.254s, episode steps: 43, steps per second: 170, episode reward: 0.850, mean reward: 0.020 [-0.006, 1.000], mean action: 1.163 [0.000, 2.000], mean observation: -0.119 [-0.581, 0.200], loss: 0.000047, mean_absolute_error: 0.455360, mean_q: 0.688161\n",
      " 29569/50000: episode: 648, duration: 0.100s, episode steps: 13, steps per second: 130, episode reward: 0.985, mean reward: 0.076 [-0.002, 1.000], mean action: 0.692 [0.000, 2.000], mean observation: 0.043 [-0.070, 0.152], loss: 0.000015, mean_absolute_error: 0.452444, mean_q: 0.683900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29622/50000: episode: 649, duration: 0.385s, episode steps: 53, steps per second: 138, episode reward: 0.713, mean reward: 0.013 [-0.009, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.196 [-0.260, 0.899], loss: 0.000038, mean_absolute_error: 0.461609, mean_q: 0.696939\n",
      " 29623/50000: episode: 650, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [-0.010, 0.029], loss: 0.000018, mean_absolute_error: 0.414149, mean_q: 0.624766\n",
      " 29650/50000: episode: 651, duration: 0.210s, episode steps: 27, steps per second: 129, episode reward: 0.944, mean reward: 0.035 [-0.003, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.066 [-0.315, 0.130], loss: 0.000033, mean_absolute_error: 0.463840, mean_q: 0.700178\n",
      " 29725/50000: episode: 652, duration: 0.412s, episode steps: 75, steps per second: 182, episode reward: 0.637, mean reward: 0.008 [-0.010, 1.000], mean action: 0.920 [0.000, 2.000], mean observation: 0.183 [-0.270, 0.990], loss: 0.000047, mean_absolute_error: 0.457521, mean_q: 0.690202\n",
      " 29764/50000: episode: 653, duration: 0.150s, episode steps: 39, steps per second: 260, episode reward: 0.870, mean reward: 0.022 [-0.005, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.110 [-0.544, 0.190], loss: 0.000052, mean_absolute_error: 0.451874, mean_q: 0.681046\n",
      " 29824/50000: episode: 654, duration: 0.198s, episode steps: 60, steps per second: 303, episode reward: 0.768, mean reward: 0.013 [-0.007, 1.000], mean action: 0.933 [0.000, 2.000], mean observation: 0.142 [-0.200, 0.724], loss: 0.000027, mean_absolute_error: 0.452653, mean_q: 0.683577\n",
      " 29862/50000: episode: 655, duration: 0.160s, episode steps: 38, steps per second: 237, episode reward: 0.919, mean reward: 0.024 [-0.003, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.077 [-0.080, 0.326], loss: 0.000030, mean_absolute_error: 0.464203, mean_q: 0.701190\n",
      " 29926/50000: episode: 656, duration: 0.273s, episode steps: 64, steps per second: 234, episode reward: 0.771, mean reward: 0.012 [-0.007, 1.000], mean action: 0.922 [0.000, 2.000], mean observation: 0.132 [-0.200, 0.706], loss: 0.000026, mean_absolute_error: 0.461351, mean_q: 0.696592\n",
      " 29993/50000: episode: 657, duration: 0.281s, episode steps: 67, steps per second: 239, episode reward: 0.677, mean reward: 0.010 [-0.009, 1.000], mean action: 0.925 [0.000, 2.000], mean observation: 0.180 [-0.250, 0.930], loss: 0.000035, mean_absolute_error: 0.453720, mean_q: 0.684683\n",
      " 30042/50000: episode: 658, duration: 0.266s, episode steps: 49, steps per second: 184, episode reward: 0.792, mean reward: 0.016 [-0.007, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.151 [-0.706, 0.210], loss: 0.000032, mean_absolute_error: 0.459255, mean_q: 0.693181\n",
      " 30072/50000: episode: 659, duration: 0.113s, episode steps: 30, steps per second: 265, episode reward: 0.926, mean reward: 0.031 [-0.004, 1.000], mean action: 1.267 [0.000, 2.000], mean observation: -0.078 [-0.374, 0.150], loss: 0.000033, mean_absolute_error: 0.460255, mean_q: 0.695065\n",
      " 30111/50000: episode: 660, duration: 0.153s, episode steps: 39, steps per second: 254, episode reward: 0.914, mean reward: 0.023 [-0.004, 1.000], mean action: 0.897 [0.000, 2.000], mean observation: 0.079 [-0.090, 0.353], loss: 0.000029, mean_absolute_error: 0.462602, mean_q: 0.698654\n",
      " 30112/50000: episode: 661, duration: 0.008s, episode steps: 1, steps per second: 129, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.028 [0.000, 0.056], loss: 0.000045, mean_absolute_error: 0.419179, mean_q: 0.634954\n",
      " 30155/50000: episode: 662, duration: 0.163s, episode steps: 43, steps per second: 263, episode reward: 0.834, mean reward: 0.019 [-0.006, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.130 [-0.633, 0.210], loss: 0.000039, mean_absolute_error: 0.458898, mean_q: 0.693807\n",
      " 30215/50000: episode: 663, duration: 0.292s, episode steps: 60, steps per second: 205, episode reward: 0.803, mean reward: 0.013 [-0.006, 1.000], mean action: 0.950 [0.000, 2.000], mean observation: 0.121 [-0.170, 0.629], loss: 0.000028, mean_absolute_error: 0.462638, mean_q: 0.698098\n",
      " 30276/50000: episode: 664, duration: 0.212s, episode steps: 61, steps per second: 288, episode reward: 0.729, mean reward: 0.012 [-0.009, 1.000], mean action: 0.918 [0.000, 2.000], mean observation: 0.161 [-0.240, 0.858], loss: 0.000032, mean_absolute_error: 0.456611, mean_q: 0.689662\n",
      " 30300/50000: episode: 665, duration: 0.154s, episode steps: 24, steps per second: 156, episode reward: 0.949, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.063 [-0.309, 0.140], loss: 0.000052, mean_absolute_error: 0.459639, mean_q: 0.693640\n",
      " 30356/50000: episode: 666, duration: 0.293s, episode steps: 56, steps per second: 191, episode reward: 0.829, mean reward: 0.015 [-0.005, 1.000], mean action: 0.893 [0.000, 2.000], mean observation: 0.114 [-0.150, 0.545], loss: 0.000031, mean_absolute_error: 0.466485, mean_q: 0.704619\n",
      " 30408/50000: episode: 667, duration: 0.194s, episode steps: 52, steps per second: 268, episode reward: 0.740, mean reward: 0.014 [-0.009, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.177 [-0.863, 0.250], loss: 0.000047, mean_absolute_error: 0.464069, mean_q: 0.700687\n",
      " 30448/50000: episode: 668, duration: 0.254s, episode steps: 40, steps per second: 158, episode reward: 0.861, mean reward: 0.022 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.116 [-0.572, 0.190], loss: 0.000033, mean_absolute_error: 0.460816, mean_q: 0.695428\n",
      " 30478/50000: episode: 669, duration: 0.217s, episode steps: 30, steps per second: 138, episode reward: 0.930, mean reward: 0.031 [-0.004, 1.000], mean action: 1.267 [0.000, 2.000], mean observation: -0.073 [-0.360, 0.130], loss: 0.000035, mean_absolute_error: 0.449512, mean_q: 0.678858\n",
      " 30502/50000: episode: 670, duration: 0.291s, episode steps: 24, steps per second: 83, episode reward: 0.953, mean reward: 0.040 [-0.003, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.062 [-0.279, 0.130], loss: 0.000047, mean_absolute_error: 0.453885, mean_q: 0.686077\n",
      " 30559/50000: episode: 671, duration: 0.529s, episode steps: 57, steps per second: 108, episode reward: 0.700, mean reward: 0.012 [-0.009, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.193 [-0.897, 0.260], loss: 0.000042, mean_absolute_error: 0.459878, mean_q: 0.694516\n",
      " 30613/50000: episode: 672, duration: 0.454s, episode steps: 54, steps per second: 119, episode reward: 0.736, mean reward: 0.014 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.174 [-0.868, 0.250], loss: 0.000069, mean_absolute_error: 0.463802, mean_q: 0.700131\n",
      " 30666/50000: episode: 673, duration: 0.366s, episode steps: 53, steps per second: 145, episode reward: 0.743, mean reward: 0.014 [-0.008, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.174 [-0.824, 0.250], loss: 0.000031, mean_absolute_error: 0.467926, mean_q: 0.705626\n",
      " 30695/50000: episode: 674, duration: 0.185s, episode steps: 29, steps per second: 157, episode reward: 0.947, mean reward: 0.033 [-0.003, 1.000], mean action: 0.759 [0.000, 2.000], mean observation: 0.063 [-0.080, 0.269], loss: 0.000043, mean_absolute_error: 0.477003, mean_q: 0.720364\n",
      " 30732/50000: episode: 675, duration: 0.320s, episode steps: 37, steps per second: 115, episode reward: 0.881, mean reward: 0.024 [-0.005, 1.000], mean action: 1.189 [0.000, 2.000], mean observation: -0.105 [-0.520, 0.190], loss: 0.000031, mean_absolute_error: 0.465789, mean_q: 0.703725\n",
      " 30783/50000: episode: 676, duration: 0.331s, episode steps: 51, steps per second: 154, episode reward: 0.782, mean reward: 0.015 [-0.007, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.158 [-0.679, 0.220], loss: 0.000031, mean_absolute_error: 0.453530, mean_q: 0.684443\n",
      " 30841/50000: episode: 677, duration: 0.454s, episode steps: 58, steps per second: 128, episode reward: 0.759, mean reward: 0.013 [-0.008, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.150 [-0.200, 0.773], loss: 0.000044, mean_absolute_error: 0.460696, mean_q: 0.694973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30885/50000: episode: 678, duration: 0.237s, episode steps: 44, steps per second: 185, episode reward: 0.824, mean reward: 0.019 [-0.007, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.136 [-0.662, 0.210], loss: 0.000034, mean_absolute_error: 0.465890, mean_q: 0.703449\n",
      " 30931/50000: episode: 679, duration: 0.224s, episode steps: 46, steps per second: 205, episode reward: 0.810, mean reward: 0.018 [-0.007, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.142 [-0.707, 0.230], loss: 0.000061, mean_absolute_error: 0.471648, mean_q: 0.712622\n",
      " 30985/50000: episode: 680, duration: 0.288s, episode steps: 54, steps per second: 187, episode reward: 0.697, mean reward: 0.013 [-0.010, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.200 [-0.966, 0.270], loss: 0.000059, mean_absolute_error: 0.460383, mean_q: 0.694721\n",
      " 31041/50000: episode: 681, duration: 0.244s, episode steps: 56, steps per second: 230, episode reward: 0.719, mean reward: 0.013 [-0.009, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.183 [-0.865, 0.260], loss: 0.000041, mean_absolute_error: 0.464044, mean_q: 0.700709\n",
      " 31111/50000: episode: 682, duration: 0.292s, episode steps: 70, steps per second: 240, episode reward: 0.628, mean reward: 0.009 [-0.010, 1.000], mean action: 0.886 [0.000, 2.000], mean observation: 0.202 [-0.270, 0.996], loss: 0.000039, mean_absolute_error: 0.468967, mean_q: 0.706994\n",
      " 31132/50000: episode: 683, duration: 0.083s, episode steps: 21, steps per second: 252, episode reward: 0.966, mean reward: 0.046 [-0.002, 1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.056 [-0.212, 0.100], loss: 0.000026, mean_absolute_error: 0.465384, mean_q: 0.702876\n",
      " 31172/50000: episode: 684, duration: 0.162s, episode steps: 40, steps per second: 247, episode reward: 0.908, mean reward: 0.023 [-0.004, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.082 [-0.090, 0.373], loss: 0.000059, mean_absolute_error: 0.466466, mean_q: 0.703753\n",
      " 31180/50000: episode: 685, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 0.991, mean reward: 0.124 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.037 [-0.134, 0.080], loss: 0.000050, mean_absolute_error: 0.463057, mean_q: 0.697259\n",
      " 31220/50000: episode: 686, duration: 0.126s, episode steps: 40, steps per second: 318, episode reward: 0.903, mean reward: 0.023 [-0.004, 1.000], mean action: 0.825 [0.000, 2.000], mean observation: 0.087 [-0.090, 0.385], loss: 0.000038, mean_absolute_error: 0.476470, mean_q: 0.719324\n",
      " 31269/50000: episode: 687, duration: 0.151s, episode steps: 49, steps per second: 324, episode reward: 0.780, mean reward: 0.016 [-0.008, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.158 [-0.756, 0.240], loss: 0.000030, mean_absolute_error: 0.461495, mean_q: 0.696363\n",
      " 31270/50000: episode: 688, duration: 0.006s, episode steps: 1, steps per second: 174, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.024 [-0.049, 0.000], loss: 0.000011, mean_absolute_error: 0.465900, mean_q: 0.707360\n",
      " 31287/50000: episode: 689, duration: 0.051s, episode steps: 17, steps per second: 335, episode reward: 0.978, mean reward: 0.058 [-0.002, 1.000], mean action: 1.294 [0.000, 2.000], mean observation: -0.049 [-0.166, 0.070], loss: 0.000016, mean_absolute_error: 0.466468, mean_q: 0.703329\n",
      " 31335/50000: episode: 690, duration: 0.182s, episode steps: 48, steps per second: 264, episode reward: 0.783, mean reward: 0.016 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.160 [-0.736, 0.230], loss: 0.000041, mean_absolute_error: 0.467566, mean_q: 0.705511\n",
      " 31394/50000: episode: 691, duration: 0.215s, episode steps: 59, steps per second: 274, episode reward: 0.748, mean reward: 0.013 [-0.008, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.157 [-0.180, 0.776], loss: 0.000040, mean_absolute_error: 0.470050, mean_q: 0.708544\n",
      " 31429/50000: episode: 692, duration: 0.113s, episode steps: 35, steps per second: 310, episode reward: 0.898, mean reward: 0.026 [-0.004, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.097 [-0.448, 0.170], loss: 0.000033, mean_absolute_error: 0.462028, mean_q: 0.697819\n",
      " 31482/50000: episode: 693, duration: 0.168s, episode steps: 53, steps per second: 316, episode reward: 0.826, mean reward: 0.016 [-0.006, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.120 [-0.140, 0.563], loss: 0.000029, mean_absolute_error: 0.464756, mean_q: 0.701329\n",
      " 31525/50000: episode: 694, duration: 0.152s, episode steps: 43, steps per second: 284, episode reward: 0.850, mean reward: 0.020 [-0.006, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.122 [-0.556, 0.190], loss: 0.000040, mean_absolute_error: 0.467427, mean_q: 0.705315\n",
      " 31526/50000: episode: 695, duration: 0.007s, episode steps: 1, steps per second: 138, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.033 [-0.066, 0.000], loss: 0.000013, mean_absolute_error: 0.466615, mean_q: 0.703987\n",
      " 31571/50000: episode: 696, duration: 0.193s, episode steps: 45, steps per second: 233, episode reward: 0.879, mean reward: 0.020 [-0.004, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.101 [-0.120, 0.405], loss: 0.000023, mean_absolute_error: 0.463407, mean_q: 0.699163\n",
      " 31605/50000: episode: 697, duration: 0.162s, episode steps: 34, steps per second: 210, episode reward: 0.923, mean reward: 0.027 [-0.003, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.079 [-0.100, 0.332], loss: 0.000034, mean_absolute_error: 0.474267, mean_q: 0.714977\n",
      " 31625/50000: episode: 698, duration: 0.076s, episode steps: 20, steps per second: 263, episode reward: 0.964, mean reward: 0.048 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.055 [-0.242, 0.120], loss: 0.000032, mean_absolute_error: 0.462958, mean_q: 0.698066\n",
      " 31626/50000: episode: 699, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [-0.010, 0.040], loss: 0.000009, mean_absolute_error: 0.419048, mean_q: 0.636221\n",
      " 31627/50000: episode: 700, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.003 [0.000, 0.006], loss: 0.000024, mean_absolute_error: 0.476830, mean_q: 0.721071\n",
      " 31628/50000: episode: 701, duration: 0.008s, episode steps: 1, steps per second: 129, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.003 [-0.006, 0.000], loss: 0.000010, mean_absolute_error: 0.483067, mean_q: 0.727733\n",
      " 31657/50000: episode: 702, duration: 0.090s, episode steps: 29, steps per second: 321, episode reward: 0.933, mean reward: 0.032 [-0.003, 1.000], mean action: 1.241 [0.000, 2.000], mean observation: -0.074 [-0.348, 0.150], loss: 0.000054, mean_absolute_error: 0.462317, mean_q: 0.696953\n",
      " 31658/50000: episode: 703, duration: 0.006s, episode steps: 1, steps per second: 159, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.001 [0.000, 0.002], loss: 0.000024, mean_absolute_error: 0.431819, mean_q: 0.652270\n",
      " 31697/50000: episode: 704, duration: 0.113s, episode steps: 39, steps per second: 346, episode reward: 0.873, mean reward: 0.022 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.108 [-0.522, 0.190], loss: 0.000028, mean_absolute_error: 0.472902, mean_q: 0.712943\n",
      " 31730/50000: episode: 705, duration: 0.096s, episode steps: 33, steps per second: 344, episode reward: 0.917, mean reward: 0.028 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.075 [-0.407, 0.160], loss: 0.000030, mean_absolute_error: 0.460553, mean_q: 0.693691\n",
      " 31752/50000: episode: 706, duration: 0.064s, episode steps: 22, steps per second: 343, episode reward: 0.959, mean reward: 0.044 [-0.003, 1.000], mean action: 1.409 [0.000, 2.000], mean observation: -0.056 [-0.259, 0.120], loss: 0.000022, mean_absolute_error: 0.468297, mean_q: 0.706908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31808/50000: episode: 707, duration: 0.169s, episode steps: 56, steps per second: 332, episode reward: 0.676, mean reward: 0.012 [-0.010, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.213 [-0.971, 0.280], loss: 0.000026, mean_absolute_error: 0.469702, mean_q: 0.708771\n",
      " 31863/50000: episode: 708, duration: 0.186s, episode steps: 55, steps per second: 295, episode reward: 0.694, mean reward: 0.013 [-0.010, 1.000], mean action: 1.109 [0.000, 2.000], mean observation: -0.200 [-0.964, 0.270], loss: 0.000030, mean_absolute_error: 0.470820, mean_q: 0.709762\n",
      " 31919/50000: episode: 709, duration: 0.165s, episode steps: 56, steps per second: 339, episode reward: 0.700, mean reward: 0.012 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.193 [-0.948, 0.260], loss: 0.000036, mean_absolute_error: 0.463240, mean_q: 0.698540\n",
      " 31981/50000: episode: 710, duration: 0.229s, episode steps: 62, steps per second: 271, episode reward: 0.677, mean reward: 0.011 [-0.010, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.191 [-0.270, 0.971], loss: 0.000049, mean_absolute_error: 0.468191, mean_q: 0.705960\n",
      " 32036/50000: episode: 711, duration: 0.159s, episode steps: 55, steps per second: 345, episode reward: 0.703, mean reward: 0.013 [-0.009, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.195 [-0.931, 0.240], loss: 0.000034, mean_absolute_error: 0.461930, mean_q: 0.695653\n",
      " 32037/50000: episode: 712, duration: 0.006s, episode steps: 1, steps per second: 173, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.086, 0.010], loss: 0.000031, mean_absolute_error: 0.487645, mean_q: 0.734367\n",
      " 32070/50000: episode: 713, duration: 0.096s, episode steps: 33, steps per second: 342, episode reward: 0.903, mean reward: 0.027 [-0.005, 1.000], mean action: 1.242 [0.000, 2.000], mean observation: -0.095 [-0.452, 0.160], loss: 0.000046, mean_absolute_error: 0.468683, mean_q: 0.705935\n",
      " 32071/50000: episode: 714, duration: 0.006s, episode steps: 1, steps per second: 173, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [-0.010, 0.037], loss: 0.000037, mean_absolute_error: 0.463114, mean_q: 0.695574\n",
      " 32117/50000: episode: 715, duration: 0.134s, episode steps: 46, steps per second: 344, episode reward: 0.867, mean reward: 0.019 [-0.005, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.102 [-0.140, 0.500], loss: 0.000056, mean_absolute_error: 0.468154, mean_q: 0.704998\n",
      " 32177/50000: episode: 716, duration: 0.182s, episode steps: 60, steps per second: 330, episode reward: 0.760, mean reward: 0.013 [-0.008, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.143 [-0.200, 0.754], loss: 0.000026, mean_absolute_error: 0.470366, mean_q: 0.708523\n",
      " 32221/50000: episode: 717, duration: 0.137s, episode steps: 44, steps per second: 321, episode reward: 0.876, mean reward: 0.020 [-0.005, 1.000], mean action: 0.841 [0.000, 2.000], mean observation: 0.100 [-0.130, 0.466], loss: 0.000027, mean_absolute_error: 0.459654, mean_q: 0.691929\n",
      " 32275/50000: episode: 718, duration: 0.187s, episode steps: 54, steps per second: 288, episode reward: 0.723, mean reward: 0.013 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.184 [-0.250, 0.894], loss: 0.000031, mean_absolute_error: 0.461536, mean_q: 0.694232\n",
      " 32315/50000: episode: 719, duration: 0.154s, episode steps: 40, steps per second: 260, episode reward: 0.895, mean reward: 0.022 [-0.004, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.091 [-0.120, 0.412], loss: 0.000035, mean_absolute_error: 0.465757, mean_q: 0.701066\n",
      " 32362/50000: episode: 720, duration: 0.193s, episode steps: 47, steps per second: 243, episode reward: 0.800, mean reward: 0.017 [-0.007, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.147 [-0.720, 0.200], loss: 0.000045, mean_absolute_error: 0.460230, mean_q: 0.692651\n",
      " 32378/50000: episode: 721, duration: 0.059s, episode steps: 16, steps per second: 269, episode reward: 0.980, mean reward: 0.061 [-0.002, 1.000], mean action: 0.438 [0.000, 2.000], mean observation: 0.032 [-0.100, 0.175], loss: 0.000037, mean_absolute_error: 0.458381, mean_q: 0.688585\n",
      " 32419/50000: episode: 722, duration: 0.118s, episode steps: 41, steps per second: 349, episode reward: 0.885, mean reward: 0.022 [-0.004, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.097 [-0.130, 0.442], loss: 0.000026, mean_absolute_error: 0.469886, mean_q: 0.707715\n",
      " 32428/50000: episode: 723, duration: 0.029s, episode steps: 9, steps per second: 313, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 1.778 [0.000, 2.000], mean observation: -0.035 [-0.135, 0.080], loss: 0.000032, mean_absolute_error: 0.466268, mean_q: 0.701345\n",
      " 32429/50000: episode: 724, duration: 0.007s, episode steps: 1, steps per second: 146, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.007 [0.004, 0.010], loss: 0.000020, mean_absolute_error: 0.432217, mean_q: 0.649867\n",
      " 32462/50000: episode: 725, duration: 0.096s, episode steps: 33, steps per second: 343, episode reward: 0.909, mean reward: 0.028 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.086 [-0.435, 0.160], loss: 0.000027, mean_absolute_error: 0.466853, mean_q: 0.703847\n",
      " 32476/50000: episode: 726, duration: 0.043s, episode steps: 14, steps per second: 324, episode reward: 0.981, mean reward: 0.070 [-0.002, 1.000], mean action: 1.643 [0.000, 2.000], mean observation: -0.044 [-0.170, 0.090], loss: 0.000025, mean_absolute_error: 0.474750, mean_q: 0.713426\n",
      " 32509/50000: episode: 727, duration: 0.102s, episode steps: 33, steps per second: 323, episode reward: 0.906, mean reward: 0.027 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.089 [-0.444, 0.150], loss: 0.000037, mean_absolute_error: 0.457992, mean_q: 0.686835\n",
      " 32524/50000: episode: 728, duration: 0.053s, episode steps: 15, steps per second: 286, episode reward: 0.977, mean reward: 0.065 [-0.002, 1.000], mean action: 1.533 [0.000, 2.000], mean observation: -0.044 [-0.200, 0.110], loss: 0.000044, mean_absolute_error: 0.458892, mean_q: 0.690346\n",
      " 32577/50000: episode: 729, duration: 0.184s, episode steps: 53, steps per second: 288, episode reward: 0.707, mean reward: 0.013 [-0.009, 1.000], mean action: 0.849 [0.000, 2.000], mean observation: 0.198 [-0.270, 0.941], loss: 0.000043, mean_absolute_error: 0.468869, mean_q: 0.704518\n",
      " 32592/50000: episode: 730, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 0.400 [0.000, 2.000], mean observation: 0.048 [-0.090, 0.171], loss: 0.000032, mean_absolute_error: 0.478621, mean_q: 0.721078\n",
      " 32603/50000: episode: 731, duration: 0.082s, episode steps: 11, steps per second: 134, episode reward: 0.987, mean reward: 0.090 [-0.001, 1.000], mean action: 1.818 [0.000, 2.000], mean observation: -0.039 [-0.145, 0.090], loss: 0.000056, mean_absolute_error: 0.483137, mean_q: 0.727147\n",
      " 32627/50000: episode: 732, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 0.951, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.066 [-0.284, 0.130], loss: 0.000055, mean_absolute_error: 0.465876, mean_q: 0.699463\n",
      " 32651/50000: episode: 733, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 0.950, mean reward: 0.040 [-0.003, 1.000], mean action: 1.292 [0.000, 2.000], mean observation: -0.064 [-0.295, 0.140], loss: 0.000066, mean_absolute_error: 0.462260, mean_q: 0.696452\n",
      " 32706/50000: episode: 734, duration: 0.189s, episode steps: 55, steps per second: 291, episode reward: 0.708, mean reward: 0.013 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.190 [-0.930, 0.250], loss: 0.000025, mean_absolute_error: 0.467593, mean_q: 0.703643\n",
      " 32765/50000: episode: 735, duration: 0.169s, episode steps: 59, steps per second: 349, episode reward: 0.674, mean reward: 0.011 [-0.010, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.202 [-0.976, 0.240], loss: 0.000027, mean_absolute_error: 0.465608, mean_q: 0.698573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32815/50000: episode: 736, duration: 0.146s, episode steps: 50, steps per second: 343, episode reward: 0.824, mean reward: 0.016 [-0.006, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.127 [-0.140, 0.595], loss: 0.000023, mean_absolute_error: 0.472303, mean_q: 0.710819\n",
      " 32841/50000: episode: 737, duration: 0.089s, episode steps: 26, steps per second: 293, episode reward: 0.942, mean reward: 0.036 [-0.003, 1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.069 [-0.331, 0.140], loss: 0.000040, mean_absolute_error: 0.473586, mean_q: 0.711831\n",
      " 32883/50000: episode: 738, duration: 0.128s, episode steps: 42, steps per second: 329, episode reward: 0.862, mean reward: 0.021 [-0.005, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.117 [-0.507, 0.170], loss: 0.000046, mean_absolute_error: 0.471389, mean_q: 0.709296\n",
      " 32898/50000: episode: 739, duration: 0.051s, episode steps: 15, steps per second: 292, episode reward: 0.978, mean reward: 0.065 [-0.002, 1.000], mean action: 0.400 [0.000, 2.000], mean observation: 0.045 [-0.100, 0.197], loss: 0.000034, mean_absolute_error: 0.474099, mean_q: 0.711998\n",
      " 32941/50000: episode: 740, duration: 0.132s, episode steps: 43, steps per second: 326, episode reward: 0.914, mean reward: 0.021 [-0.003, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.062 [-0.110, 0.342], loss: 0.000038, mean_absolute_error: 0.463183, mean_q: 0.695223\n",
      " 32974/50000: episode: 741, duration: 0.099s, episode steps: 33, steps per second: 333, episode reward: 0.905, mean reward: 0.027 [-0.004, 1.000], mean action: 1.242 [0.000, 2.000], mean observation: -0.091 [-0.450, 0.170], loss: 0.000046, mean_absolute_error: 0.467288, mean_q: 0.701465\n",
      " 33029/50000: episode: 742, duration: 0.186s, episode steps: 55, steps per second: 296, episode reward: 0.774, mean reward: 0.014 [-0.007, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.147 [-0.210, 0.732], loss: 0.000026, mean_absolute_error: 0.461297, mean_q: 0.692609\n",
      " 33074/50000: episode: 743, duration: 0.154s, episode steps: 45, steps per second: 293, episode reward: 0.867, mean reward: 0.019 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.104 [-0.140, 0.503], loss: 0.000057, mean_absolute_error: 0.463830, mean_q: 0.695696\n",
      " 33134/50000: episode: 744, duration: 0.175s, episode steps: 60, steps per second: 344, episode reward: 0.710, mean reward: 0.012 [-0.009, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.176 [-0.260, 0.898], loss: 0.000037, mean_absolute_error: 0.474573, mean_q: 0.712843\n",
      " 33191/50000: episode: 745, duration: 0.178s, episode steps: 57, steps per second: 321, episode reward: 0.728, mean reward: 0.013 [-0.009, 1.000], mean action: 0.895 [0.000, 2.000], mean observation: 0.172 [-0.250, 0.866], loss: 0.000052, mean_absolute_error: 0.465471, mean_q: 0.698120\n",
      " 33192/50000: episode: 746, duration: 0.006s, episode steps: 1, steps per second: 158, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [-0.010, 0.045], loss: 0.000021, mean_absolute_error: 0.417502, mean_q: 0.601821\n",
      " 33215/50000: episode: 747, duration: 0.080s, episode steps: 23, steps per second: 286, episode reward: 0.959, mean reward: 0.042 [-0.002, 1.000], mean action: 1.391 [0.000, 2.000], mean observation: -0.059 [-0.233, 0.120], loss: 0.000071, mean_absolute_error: 0.474674, mean_q: 0.711046\n",
      " 33225/50000: episode: 748, duration: 0.037s, episode steps: 10, steps per second: 273, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.040 [-0.141, 0.070], loss: 0.000052, mean_absolute_error: 0.478920, mean_q: 0.718895\n",
      " 33274/50000: episode: 749, duration: 0.199s, episode steps: 49, steps per second: 246, episode reward: 0.787, mean reward: 0.016 [-0.007, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.152 [-0.240, 0.745], loss: 0.000028, mean_absolute_error: 0.465211, mean_q: 0.697956\n",
      " 33300/50000: episode: 750, duration: 0.077s, episode steps: 26, steps per second: 339, episode reward: 0.952, mean reward: 0.037 [-0.003, 1.000], mean action: 0.692 [0.000, 2.000], mean observation: 0.060 [-0.110, 0.270], loss: 0.000032, mean_absolute_error: 0.465308, mean_q: 0.698103\n",
      " 33339/50000: episode: 751, duration: 0.121s, episode steps: 39, steps per second: 323, episode reward: 0.873, mean reward: 0.022 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.112 [-0.506, 0.170], loss: 0.000036, mean_absolute_error: 0.466535, mean_q: 0.700739\n",
      " 33391/50000: episode: 752, duration: 0.196s, episode steps: 52, steps per second: 266, episode reward: 0.732, mean reward: 0.014 [-0.009, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.182 [-0.260, 0.867], loss: 0.000031, mean_absolute_error: 0.476250, mean_q: 0.715366\n",
      " 33451/50000: episode: 753, duration: 0.287s, episode steps: 60, steps per second: 209, episode reward: 0.665, mean reward: 0.011 [-0.010, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.206 [-0.974, 0.230], loss: 0.000050, mean_absolute_error: 0.465579, mean_q: 0.697136\n",
      " 33499/50000: episode: 754, duration: 0.188s, episode steps: 48, steps per second: 255, episode reward: 0.799, mean reward: 0.017 [-0.007, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.150 [-0.682, 0.200], loss: 0.000024, mean_absolute_error: 0.465615, mean_q: 0.699687\n",
      " 33537/50000: episode: 755, duration: 0.116s, episode steps: 38, steps per second: 328, episode reward: 0.867, mean reward: 0.023 [-0.006, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.115 [-0.562, 0.200], loss: 0.000046, mean_absolute_error: 0.475754, mean_q: 0.713519\n",
      " 33594/50000: episode: 756, duration: 0.172s, episode steps: 57, steps per second: 332, episode reward: 0.685, mean reward: 0.012 [-0.010, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.200 [-0.966, 0.250], loss: 0.000026, mean_absolute_error: 0.471047, mean_q: 0.706314\n",
      " 33635/50000: episode: 757, duration: 0.124s, episode steps: 41, steps per second: 330, episode reward: 0.867, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.108 [-0.533, 0.170], loss: 0.000031, mean_absolute_error: 0.470722, mean_q: 0.706123\n",
      " 33680/50000: episode: 758, duration: 0.221s, episode steps: 45, steps per second: 203, episode reward: 0.811, mean reward: 0.018 [-0.007, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.145 [-0.696, 0.200], loss: 0.000036, mean_absolute_error: 0.472118, mean_q: 0.706344\n",
      " 33704/50000: episode: 759, duration: 0.092s, episode steps: 24, steps per second: 260, episode reward: 0.955, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.058 [-0.263, 0.130], loss: 0.000021, mean_absolute_error: 0.468891, mean_q: 0.703604\n",
      " 33741/50000: episode: 760, duration: 0.110s, episode steps: 37, steps per second: 337, episode reward: 0.886, mean reward: 0.024 [-0.005, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.102 [-0.480, 0.150], loss: 0.000020, mean_absolute_error: 0.459909, mean_q: 0.686870\n",
      " 33785/50000: episode: 761, duration: 0.139s, episode steps: 44, steps per second: 317, episode reward: 0.845, mean reward: 0.019 [-0.006, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.121 [-0.584, 0.160], loss: 0.000027, mean_absolute_error: 0.463382, mean_q: 0.695298\n",
      " 33837/50000: episode: 762, duration: 0.163s, episode steps: 52, steps per second: 319, episode reward: 0.741, mean reward: 0.014 [-0.009, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.175 [-0.270, 0.870], loss: 0.000030, mean_absolute_error: 0.462128, mean_q: 0.690535\n",
      " 33893/50000: episode: 763, duration: 0.173s, episode steps: 56, steps per second: 324, episode reward: 0.684, mean reward: 0.012 [-0.010, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.203 [-0.270, 0.990], loss: 0.000035, mean_absolute_error: 0.464060, mean_q: 0.694677\n",
      " 33943/50000: episode: 764, duration: 0.148s, episode steps: 50, steps per second: 338, episode reward: 0.762, mean reward: 0.015 [-0.008, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.170 [-0.783, 0.220], loss: 0.000019, mean_absolute_error: 0.468607, mean_q: 0.701399\n",
      " 34003/50000: episode: 765, duration: 0.194s, episode steps: 60, steps per second: 309, episode reward: 0.678, mean reward: 0.011 [-0.010, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.196 [-0.280, 0.964], loss: 0.000021, mean_absolute_error: 0.468012, mean_q: 0.700269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34004/50000: episode: 766, duration: 0.008s, episode steps: 1, steps per second: 119, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.020 [-0.029, -0.010], loss: 0.000091, mean_absolute_error: 0.486853, mean_q: 0.730320\n",
      " 34057/50000: episode: 767, duration: 0.201s, episode steps: 53, steps per second: 263, episode reward: 0.734, mean reward: 0.014 [-0.009, 1.000], mean action: 1.151 [0.000, 2.000], mean observation: -0.179 [-0.862, 0.220], loss: 0.000042, mean_absolute_error: 0.466578, mean_q: 0.698871\n",
      " 34058/50000: episode: 768, duration: 0.007s, episode steps: 1, steps per second: 138, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [-0.010, 0.089], loss: 0.000026, mean_absolute_error: 0.496890, mean_q: 0.741557\n",
      " 34086/50000: episode: 769, duration: 0.085s, episode steps: 28, steps per second: 328, episode reward: 0.934, mean reward: 0.033 [-0.004, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.072 [-0.355, 0.150], loss: 0.000029, mean_absolute_error: 0.463047, mean_q: 0.692274\n",
      " 34144/50000: episode: 770, duration: 0.292s, episode steps: 58, steps per second: 199, episode reward: 0.766, mean reward: 0.013 [-0.007, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.145 [-0.220, 0.741], loss: 0.000033, mean_absolute_error: 0.474669, mean_q: 0.711111\n",
      " 34210/50000: episode: 771, duration: 0.293s, episode steps: 66, steps per second: 226, episode reward: 0.681, mean reward: 0.010 [-0.009, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.178 [-0.240, 0.896], loss: 0.000045, mean_absolute_error: 0.469509, mean_q: 0.701671\n",
      " 34261/50000: episode: 772, duration: 0.281s, episode steps: 51, steps per second: 182, episode reward: 0.817, mean reward: 0.016 [-0.006, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.130 [-0.170, 0.608], loss: 0.000035, mean_absolute_error: 0.468009, mean_q: 0.700361\n",
      " 34306/50000: episode: 773, duration: 0.224s, episode steps: 45, steps per second: 201, episode reward: 0.830, mean reward: 0.018 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.132 [-0.610, 0.180], loss: 0.000025, mean_absolute_error: 0.461730, mean_q: 0.690857\n",
      " 34349/50000: episode: 774, duration: 0.235s, episode steps: 43, steps per second: 183, episode reward: 0.830, mean reward: 0.019 [-0.006, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.138 [-0.617, 0.200], loss: 0.000032, mean_absolute_error: 0.470681, mean_q: 0.704031\n",
      " 34362/50000: episode: 775, duration: 0.054s, episode steps: 13, steps per second: 243, episode reward: 0.984, mean reward: 0.076 [-0.002, 1.000], mean action: 1.538 [0.000, 2.000], mean observation: -0.042 [-0.156, 0.090], loss: 0.000041, mean_absolute_error: 0.471081, mean_q: 0.703645\n",
      " 34414/50000: episode: 776, duration: 0.242s, episode steps: 52, steps per second: 215, episode reward: 0.764, mean reward: 0.015 [-0.008, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.161 [-0.787, 0.200], loss: 0.000038, mean_absolute_error: 0.472550, mean_q: 0.707030\n",
      " 34436/50000: episode: 777, duration: 0.097s, episode steps: 22, steps per second: 227, episode reward: 0.956, mean reward: 0.043 [-0.003, 1.000], mean action: 1.364 [0.000, 2.000], mean observation: -0.060 [-0.285, 0.140], loss: 0.000047, mean_absolute_error: 0.468102, mean_q: 0.701554\n",
      " 34464/50000: episode: 778, duration: 0.102s, episode steps: 28, steps per second: 275, episode reward: 0.933, mean reward: 0.033 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.078 [-0.343, 0.150], loss: 0.000031, mean_absolute_error: 0.467644, mean_q: 0.700047\n",
      " 34476/50000: episode: 779, duration: 0.039s, episode steps: 12, steps per second: 307, episode reward: 0.986, mean reward: 0.082 [-0.001, 1.000], mean action: 1.583 [0.000, 2.000], mean observation: -0.044 [-0.140, 0.080], loss: 0.000056, mean_absolute_error: 0.469692, mean_q: 0.704083\n",
      " 34527/50000: episode: 780, duration: 0.166s, episode steps: 51, steps per second: 307, episode reward: 0.765, mean reward: 0.015 [-0.008, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.164 [-0.777, 0.210], loss: 0.000039, mean_absolute_error: 0.467790, mean_q: 0.697580\n",
      " 34578/50000: episode: 781, duration: 0.175s, episode steps: 51, steps per second: 291, episode reward: 0.848, mean reward: 0.017 [-0.005, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.102 [-0.170, 0.541], loss: 0.000071, mean_absolute_error: 0.464791, mean_q: 0.692669\n",
      " 34634/50000: episode: 782, duration: 0.177s, episode steps: 56, steps per second: 316, episode reward: 0.867, mean reward: 0.015 [-0.005, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.070 [-0.140, 0.463], loss: 0.000034, mean_absolute_error: 0.467986, mean_q: 0.699513\n",
      " 34635/50000: episode: 783, duration: 0.006s, episode steps: 1, steps per second: 164, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.042 [-0.010, 0.094], loss: 0.000101, mean_absolute_error: 0.490758, mean_q: 0.735876\n",
      " 34685/50000: episode: 784, duration: 0.156s, episode steps: 50, steps per second: 321, episode reward: 0.818, mean reward: 0.016 [-0.006, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.129 [-0.170, 0.625], loss: 0.000028, mean_absolute_error: 0.470738, mean_q: 0.704321\n",
      " 34737/50000: episode: 785, duration: 0.177s, episode steps: 52, steps per second: 294, episode reward: 0.770, mean reward: 0.015 [-0.007, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.162 [-0.716, 0.200], loss: 0.000036, mean_absolute_error: 0.470570, mean_q: 0.703969\n",
      " 34747/50000: episode: 786, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.037 [-0.090, 0.144], loss: 0.000025, mean_absolute_error: 0.470926, mean_q: 0.706791\n",
      " 34783/50000: episode: 787, duration: 0.169s, episode steps: 36, steps per second: 213, episode reward: 0.912, mean reward: 0.025 [-0.004, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.084 [-0.120, 0.380], loss: 0.000023, mean_absolute_error: 0.473528, mean_q: 0.708149\n",
      " 34809/50000: episode: 788, duration: 0.098s, episode steps: 26, steps per second: 264, episode reward: 0.943, mean reward: 0.036 [-0.003, 1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.066 [-0.330, 0.150], loss: 0.000052, mean_absolute_error: 0.470611, mean_q: 0.706296\n",
      " 34851/50000: episode: 789, duration: 0.139s, episode steps: 42, steps per second: 301, episode reward: 0.889, mean reward: 0.021 [-0.004, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.089 [-0.130, 0.429], loss: 0.000025, mean_absolute_error: 0.462391, mean_q: 0.691055\n",
      " 34909/50000: episode: 790, duration: 0.217s, episode steps: 58, steps per second: 268, episode reward: 0.691, mean reward: 0.012 [-0.009, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.195 [-0.932, 0.220], loss: 0.000031, mean_absolute_error: 0.465560, mean_q: 0.696014\n",
      " 34965/50000: episode: 791, duration: 0.189s, episode steps: 56, steps per second: 296, episode reward: 0.699, mean reward: 0.012 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.195 [-0.931, 0.220], loss: 0.000043, mean_absolute_error: 0.469497, mean_q: 0.701174\n",
      " 35003/50000: episode: 792, duration: 0.131s, episode steps: 38, steps per second: 291, episode reward: 0.860, mean reward: 0.023 [-0.006, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.119 [-0.586, 0.230], loss: 0.000027, mean_absolute_error: 0.476717, mean_q: 0.713310\n",
      " 35051/50000: episode: 793, duration: 0.148s, episode steps: 48, steps per second: 323, episode reward: 0.835, mean reward: 0.017 [-0.006, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.122 [-0.170, 0.571], loss: 0.000031, mean_absolute_error: 0.474482, mean_q: 0.708519\n",
      " 35073/50000: episode: 794, duration: 0.080s, episode steps: 22, steps per second: 276, episode reward: 0.955, mean reward: 0.043 [-0.003, 1.000], mean action: 1.409 [0.000, 2.000], mean observation: -0.059 [-0.289, 0.130], loss: 0.000036, mean_absolute_error: 0.470173, mean_q: 0.703900\n",
      " 35118/50000: episode: 795, duration: 0.173s, episode steps: 45, steps per second: 260, episode reward: 0.849, mean reward: 0.019 [-0.006, 1.000], mean action: 0.822 [0.000, 2.000], mean observation: 0.116 [-0.150, 0.566], loss: 0.000026, mean_absolute_error: 0.460192, mean_q: 0.686402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35169/50000: episode: 796, duration: 0.192s, episode steps: 51, steps per second: 266, episode reward: 0.852, mean reward: 0.017 [-0.006, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.092 [-0.170, 0.555], loss: 0.000032, mean_absolute_error: 0.466850, mean_q: 0.696939\n",
      " 35170/50000: episode: 797, duration: 0.007s, episode steps: 1, steps per second: 152, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.002 [-0.010, 0.014], loss: 0.000021, mean_absolute_error: 0.467095, mean_q: 0.699245\n",
      " 35220/50000: episode: 798, duration: 0.333s, episode steps: 50, steps per second: 150, episode reward: 0.845, mean reward: 0.017 [-0.005, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.112 [-0.160, 0.531], loss: 0.000046, mean_absolute_error: 0.456985, mean_q: 0.679548\n",
      " 35221/50000: episode: 799, duration: 0.007s, episode steps: 1, steps per second: 135, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.035 [-0.080, 0.010], loss: 0.000016, mean_absolute_error: 0.455727, mean_q: 0.681418\n",
      " 35260/50000: episode: 800, duration: 0.172s, episode steps: 39, steps per second: 226, episode reward: 0.902, mean reward: 0.023 [-0.004, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.088 [-0.120, 0.394], loss: 0.000043, mean_absolute_error: 0.470805, mean_q: 0.702093\n",
      " 35318/50000: episode: 801, duration: 0.337s, episode steps: 58, steps per second: 172, episode reward: 0.794, mean reward: 0.014 [-0.006, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.132 [-0.160, 0.617], loss: 0.000027, mean_absolute_error: 0.470744, mean_q: 0.701207\n",
      " 35354/50000: episode: 802, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 0.879, mean reward: 0.024 [-0.005, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.109 [-0.532, 0.200], loss: 0.000045, mean_absolute_error: 0.467144, mean_q: 0.694732\n",
      " 35362/50000: episode: 803, duration: 0.036s, episode steps: 8, steps per second: 223, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 0.250 [0.000, 2.000], mean observation: 0.038 [-0.070, 0.131], loss: 0.000018, mean_absolute_error: 0.471288, mean_q: 0.701686\n",
      " 35407/50000: episode: 804, duration: 0.198s, episode steps: 45, steps per second: 227, episode reward: 0.804, mean reward: 0.018 [-0.007, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.148 [-0.720, 0.230], loss: 0.000018, mean_absolute_error: 0.471953, mean_q: 0.703466\n",
      " 35461/50000: episode: 805, duration: 0.277s, episode steps: 54, steps per second: 195, episode reward: 0.695, mean reward: 0.013 [-0.010, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.204 [-0.961, 0.260], loss: 0.000033, mean_absolute_error: 0.471065, mean_q: 0.700965\n",
      " 35500/50000: episode: 806, duration: 0.162s, episode steps: 39, steps per second: 240, episode reward: 0.866, mean reward: 0.022 [-0.006, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.113 [-0.557, 0.200], loss: 0.000031, mean_absolute_error: 0.465447, mean_q: 0.694222\n",
      " 35547/50000: episode: 807, duration: 0.167s, episode steps: 47, steps per second: 281, episode reward: 0.788, mean reward: 0.017 [-0.007, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.157 [-0.742, 0.220], loss: 0.000028, mean_absolute_error: 0.463263, mean_q: 0.690011\n",
      " 35579/50000: episode: 808, duration: 0.108s, episode steps: 32, steps per second: 296, episode reward: 0.911, mean reward: 0.028 [-0.004, 1.000], mean action: 1.281 [0.000, 2.000], mean observation: -0.084 [-0.439, 0.170], loss: 0.000034, mean_absolute_error: 0.469483, mean_q: 0.698640\n",
      " 35648/50000: episode: 809, duration: 0.286s, episode steps: 69, steps per second: 241, episode reward: 0.783, mean reward: 0.011 [-0.006, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.111 [-0.150, 0.607], loss: 0.000040, mean_absolute_error: 0.468090, mean_q: 0.696345\n",
      " 35683/50000: episode: 810, duration: 0.147s, episode steps: 35, steps per second: 238, episode reward: 0.896, mean reward: 0.026 [-0.005, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.099 [-0.450, 0.180], loss: 0.000033, mean_absolute_error: 0.470838, mean_q: 0.702971\n",
      " 35720/50000: episode: 811, duration: 0.109s, episode steps: 37, steps per second: 341, episode reward: 0.882, mean reward: 0.024 [-0.005, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.107 [-0.499, 0.180], loss: 0.000042, mean_absolute_error: 0.461912, mean_q: 0.689085\n",
      " 35773/50000: episode: 812, duration: 0.170s, episode steps: 53, steps per second: 312, episode reward: 0.691, mean reward: 0.013 [-0.010, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.207 [-0.992, 0.280], loss: 0.000035, mean_absolute_error: 0.470652, mean_q: 0.702911\n",
      " 35804/50000: episode: 813, duration: 0.099s, episode steps: 31, steps per second: 314, episode reward: 0.925, mean reward: 0.030 [-0.004, 1.000], mean action: 0.710 [0.000, 2.000], mean observation: 0.080 [-0.120, 0.368], loss: 0.000048, mean_absolute_error: 0.466899, mean_q: 0.692601\n",
      " 35840/50000: episode: 814, duration: 0.115s, episode steps: 36, steps per second: 314, episode reward: 0.890, mean reward: 0.025 [-0.005, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.099 [-0.488, 0.170], loss: 0.000025, mean_absolute_error: 0.476275, mean_q: 0.710064\n",
      " 35867/50000: episode: 815, duration: 0.094s, episode steps: 27, steps per second: 287, episode reward: 0.951, mean reward: 0.035 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.052 [-0.120, 0.270], loss: 0.000025, mean_absolute_error: 0.476885, mean_q: 0.711101\n",
      " 35876/50000: episode: 816, duration: 0.028s, episode steps: 9, steps per second: 327, episode reward: 0.991, mean reward: 0.110 [-0.001, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.038 [-0.050, 0.131], loss: 0.000024, mean_absolute_error: 0.466071, mean_q: 0.693868\n",
      " 35928/50000: episode: 817, duration: 0.150s, episode steps: 52, steps per second: 347, episode reward: 0.752, mean reward: 0.014 [-0.008, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.167 [-0.799, 0.250], loss: 0.000022, mean_absolute_error: 0.474711, mean_q: 0.706401\n",
      " 35987/50000: episode: 818, duration: 0.217s, episode steps: 59, steps per second: 272, episode reward: 0.817, mean reward: 0.014 [-0.006, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.101 [-0.190, 0.613], loss: 0.000037, mean_absolute_error: 0.468031, mean_q: 0.697330\n",
      " 36046/50000: episode: 819, duration: 0.293s, episode steps: 59, steps per second: 201, episode reward: 0.847, mean reward: 0.014 [-0.005, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.080 [-0.150, 0.525], loss: 0.000032, mean_absolute_error: 0.468027, mean_q: 0.697709\n",
      " 36069/50000: episode: 820, duration: 0.070s, episode steps: 23, steps per second: 328, episode reward: 0.956, mean reward: 0.042 [-0.003, 1.000], mean action: 0.652 [0.000, 2.000], mean observation: 0.059 [-0.120, 0.273], loss: 0.000025, mean_absolute_error: 0.463627, mean_q: 0.687669\n",
      " 36121/50000: episode: 821, duration: 0.150s, episode steps: 52, steps per second: 347, episode reward: 0.731, mean reward: 0.014 [-0.009, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.185 [-0.879, 0.250], loss: 0.000025, mean_absolute_error: 0.468647, mean_q: 0.698960\n",
      " 36122/50000: episode: 822, duration: 0.006s, episode steps: 1, steps per second: 172, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.001 [-0.003, 0.000], loss: 0.000364, mean_absolute_error: 0.477097, mean_q: 0.706902\n",
      " 36123/50000: episode: 823, duration: 0.006s, episode steps: 1, steps per second: 169, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [-0.010, 0.042], loss: 0.000041, mean_absolute_error: 0.437138, mean_q: 0.637659\n",
      " 36172/50000: episode: 824, duration: 0.153s, episode steps: 49, steps per second: 319, episode reward: 0.766, mean reward: 0.016 [-0.008, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.166 [-0.818, 0.250], loss: 0.000032, mean_absolute_error: 0.474415, mean_q: 0.706560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36220/50000: episode: 825, duration: 0.143s, episode steps: 48, steps per second: 335, episode reward: 0.797, mean reward: 0.017 [-0.007, 1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.149 [-0.702, 0.230], loss: 0.000029, mean_absolute_error: 0.468684, mean_q: 0.697370\n",
      " 36258/50000: episode: 826, duration: 0.111s, episode steps: 38, steps per second: 341, episode reward: 0.909, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.079 [-0.130, 0.380], loss: 0.000037, mean_absolute_error: 0.465095, mean_q: 0.694086\n",
      " 36259/50000: episode: 827, duration: 0.006s, episode steps: 1, steps per second: 175, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [-0.010, 0.080], loss: 0.000034, mean_absolute_error: 0.439014, mean_q: 0.646546\n",
      " 36317/50000: episode: 828, duration: 0.202s, episode steps: 58, steps per second: 288, episode reward: 0.767, mean reward: 0.013 [-0.007, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.146 [-0.170, 0.722], loss: 0.000038, mean_absolute_error: 0.464446, mean_q: 0.692030\n",
      " 36336/50000: episode: 829, duration: 0.065s, episode steps: 19, steps per second: 294, episode reward: 0.971, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.049 [-0.100, 0.201], loss: 0.000024, mean_absolute_error: 0.474878, mean_q: 0.707467\n",
      " 36381/50000: episode: 830, duration: 0.188s, episode steps: 45, steps per second: 240, episode reward: 0.869, mean reward: 0.019 [-0.005, 1.000], mean action: 0.822 [0.000, 2.000], mean observation: 0.102 [-0.150, 0.494], loss: 0.000042, mean_absolute_error: 0.471327, mean_q: 0.702067\n",
      " 36418/50000: episode: 831, duration: 0.117s, episode steps: 37, steps per second: 315, episode reward: 0.906, mean reward: 0.024 [-0.004, 1.000], mean action: 0.838 [0.000, 2.000], mean observation: 0.088 [-0.150, 0.399], loss: 0.000039, mean_absolute_error: 0.474029, mean_q: 0.708445\n",
      " 36460/50000: episode: 832, duration: 0.122s, episode steps: 42, steps per second: 344, episode reward: 0.841, mean reward: 0.020 [-0.006, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.121 [-0.642, 0.230], loss: 0.000034, mean_absolute_error: 0.465418, mean_q: 0.692928\n",
      " 36512/50000: episode: 833, duration: 0.160s, episode steps: 52, steps per second: 324, episode reward: 0.714, mean reward: 0.014 [-0.009, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.193 [-0.940, 0.260], loss: 0.000034, mean_absolute_error: 0.468947, mean_q: 0.699483\n",
      " 36558/50000: episode: 834, duration: 0.160s, episode steps: 46, steps per second: 288, episode reward: 0.816, mean reward: 0.018 [-0.007, 1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.137 [-0.685, 0.230], loss: 0.000033, mean_absolute_error: 0.462528, mean_q: 0.688971\n",
      " 36585/50000: episode: 835, duration: 0.087s, episode steps: 27, steps per second: 309, episode reward: 0.938, mean reward: 0.035 [-0.003, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.070 [-0.347, 0.160], loss: 0.000044, mean_absolute_error: 0.456940, mean_q: 0.679108\n",
      " 36586/50000: episode: 836, duration: 0.007s, episode steps: 1, steps per second: 133, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.013 [-0.015, -0.010], loss: 0.000036, mean_absolute_error: 0.482518, mean_q: 0.718722\n",
      " 36632/50000: episode: 837, duration: 0.155s, episode steps: 46, steps per second: 297, episode reward: 0.904, mean reward: 0.020 [-0.004, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.055 [-0.150, 0.389], loss: 0.000048, mean_absolute_error: 0.466860, mean_q: 0.695321\n",
      " 36681/50000: episode: 838, duration: 0.140s, episode steps: 49, steps per second: 350, episode reward: 0.793, mean reward: 0.016 [-0.007, 1.000], mean action: 1.122 [0.000, 2.000], mean observation: -0.146 [-0.739, 0.220], loss: 0.000038, mean_absolute_error: 0.466224, mean_q: 0.694854\n",
      " 36682/50000: episode: 839, duration: 0.006s, episode steps: 1, steps per second: 166, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [-0.010, 0.051], loss: 0.000027, mean_absolute_error: 0.485270, mean_q: 0.719963\n",
      " 36710/50000: episode: 840, duration: 0.095s, episode steps: 28, steps per second: 295, episode reward: 0.941, mean reward: 0.034 [-0.003, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.068 [-0.318, 0.130], loss: 0.000021, mean_absolute_error: 0.470368, mean_q: 0.702762\n",
      " 36723/50000: episode: 841, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 0.983, mean reward: 0.076 [-0.002, 1.000], mean action: 0.308 [0.000, 2.000], mean observation: 0.047 [-0.100, 0.163], loss: 0.000033, mean_absolute_error: 0.472237, mean_q: 0.703633\n",
      " 36800/50000: episode: 842, duration: 0.280s, episode steps: 77, steps per second: 275, episode reward: 0.694, mean reward: 0.009 [-0.009, 1.000], mean action: 0.883 [0.000, 2.000], mean observation: 0.141 [-0.230, 0.858], loss: 0.000037, mean_absolute_error: 0.472239, mean_q: 0.702311\n",
      " 36830/50000: episode: 843, duration: 0.106s, episode steps: 30, steps per second: 283, episode reward: 0.931, mean reward: 0.031 [-0.004, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.073 [-0.359, 0.150], loss: 0.000029, mean_absolute_error: 0.474997, mean_q: 0.706477\n",
      " 36831/50000: episode: 844, duration: 0.006s, episode steps: 1, steps per second: 161, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [-0.010, 0.090], loss: 0.000011, mean_absolute_error: 0.484334, mean_q: 0.733598\n",
      " 36832/50000: episode: 845, duration: 0.007s, episode steps: 1, steps per second: 151, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.087, 0.010], loss: 0.000017, mean_absolute_error: 0.465903, mean_q: 0.671089\n",
      " 36880/50000: episode: 846, duration: 0.146s, episode steps: 48, steps per second: 328, episode reward: 0.842, mean reward: 0.018 [-0.005, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.120 [-0.160, 0.538], loss: 0.000032, mean_absolute_error: 0.467996, mean_q: 0.696941\n",
      " 36936/50000: episode: 847, duration: 0.242s, episode steps: 56, steps per second: 232, episode reward: 0.869, mean reward: 0.016 [-0.004, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.074 [-0.120, 0.435], loss: 0.000044, mean_absolute_error: 0.474885, mean_q: 0.708662\n",
      " 36968/50000: episode: 848, duration: 0.138s, episode steps: 32, steps per second: 232, episode reward: 0.925, mean reward: 0.029 [-0.004, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.073 [-0.130, 0.369], loss: 0.000053, mean_absolute_error: 0.488432, mean_q: 0.727813\n",
      " 36998/50000: episode: 849, duration: 0.126s, episode steps: 30, steps per second: 239, episode reward: 0.942, mean reward: 0.031 [-0.003, 1.000], mean action: 1.033 [0.000, 2.000], mean observation: -0.068 [-0.281, 0.120], loss: 0.000062, mean_absolute_error: 0.469551, mean_q: 0.695831\n",
      " 37037/50000: episode: 850, duration: 0.172s, episode steps: 39, steps per second: 227, episode reward: 0.935, mean reward: 0.024 [-0.003, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.041 [-0.120, 0.315], loss: 0.000038, mean_absolute_error: 0.479070, mean_q: 0.715402\n",
      " 37107/50000: episode: 851, duration: 0.251s, episode steps: 70, steps per second: 279, episode reward: 0.765, mean reward: 0.011 [-0.007, 1.000], mean action: 0.871 [0.000, 2.000], mean observation: 0.112 [-0.190, 0.689], loss: 0.000031, mean_absolute_error: 0.473345, mean_q: 0.706291\n",
      " 37146/50000: episode: 852, duration: 0.143s, episode steps: 39, steps per second: 273, episode reward: 0.894, mean reward: 0.023 [-0.004, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.097 [-0.120, 0.413], loss: 0.000043, mean_absolute_error: 0.465503, mean_q: 0.693391\n",
      " 37191/50000: episode: 853, duration: 0.140s, episode steps: 45, steps per second: 322, episode reward: 0.841, mean reward: 0.019 [-0.006, 1.000], mean action: 1.089 [0.000, 2.000], mean observation: -0.121 [-0.609, 0.210], loss: 0.000054, mean_absolute_error: 0.468401, mean_q: 0.695992\n",
      " 37193/50000: episode: 854, duration: 0.009s, episode steps: 2, steps per second: 229, episode reward: 0.999, mean reward: 0.499 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.043 [-0.102, 0.020], loss: 0.000036, mean_absolute_error: 0.463180, mean_q: 0.694216\n",
      " 37228/50000: episode: 855, duration: 0.114s, episode steps: 35, steps per second: 307, episode reward: 0.954, mean reward: 0.027 [-0.003, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.018 [-0.120, 0.259], loss: 0.000034, mean_absolute_error: 0.473170, mean_q: 0.703053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37281/50000: episode: 856, duration: 0.156s, episode steps: 53, steps per second: 340, episode reward: 0.832, mean reward: 0.016 [-0.006, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.105 [-0.180, 0.595], loss: 0.000077, mean_absolute_error: 0.472438, mean_q: 0.703632\n",
      " 37308/50000: episode: 857, duration: 0.081s, episode steps: 27, steps per second: 334, episode reward: 0.973, mean reward: 0.036 [-0.002, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.008 [-0.110, 0.186], loss: 0.000045, mean_absolute_error: 0.460330, mean_q: 0.681747\n",
      " 37334/50000: episode: 858, duration: 0.077s, episode steps: 26, steps per second: 336, episode reward: 0.963, mean reward: 0.037 [-0.002, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: -0.052 [-0.214, 0.090], loss: 0.000025, mean_absolute_error: 0.466137, mean_q: 0.693085\n",
      " 37355/50000: episode: 859, duration: 0.069s, episode steps: 21, steps per second: 306, episode reward: 0.969, mean reward: 0.046 [-0.002, 1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.056 [-0.186, 0.080], loss: 0.000092, mean_absolute_error: 0.481679, mean_q: 0.720099\n",
      " 37418/50000: episode: 860, duration: 0.203s, episode steps: 63, steps per second: 311, episode reward: 0.682, mean reward: 0.011 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.186 [-0.250, 0.940], loss: 0.000046, mean_absolute_error: 0.467862, mean_q: 0.695571\n",
      " 37469/50000: episode: 861, duration: 0.166s, episode steps: 51, steps per second: 306, episode reward: 0.815, mean reward: 0.016 [-0.006, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.134 [-0.170, 0.593], loss: 0.000049, mean_absolute_error: 0.475127, mean_q: 0.707939\n",
      " 37523/50000: episode: 862, duration: 0.165s, episode steps: 54, steps per second: 326, episode reward: 0.826, mean reward: 0.015 [-0.006, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.111 [-0.160, 0.589], loss: 0.000063, mean_absolute_error: 0.472913, mean_q: 0.700860\n",
      " 37524/50000: episode: 863, duration: 0.006s, episode steps: 1, steps per second: 175, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [-0.010, 0.076], loss: 0.000009, mean_absolute_error: 0.494331, mean_q: 0.724584\n",
      " 37561/50000: episode: 864, duration: 0.110s, episode steps: 37, steps per second: 337, episode reward: 0.930, mean reward: 0.025 [-0.003, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.050 [-0.110, 0.333], loss: 0.000030, mean_absolute_error: 0.463594, mean_q: 0.689705\n",
      " 37624/50000: episode: 865, duration: 0.193s, episode steps: 63, steps per second: 326, episode reward: 0.675, mean reward: 0.011 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.192 [-0.230, 0.926], loss: 0.000058, mean_absolute_error: 0.476079, mean_q: 0.709286\n",
      " 37682/50000: episode: 866, duration: 0.168s, episode steps: 58, steps per second: 345, episode reward: 0.807, mean reward: 0.014 [-0.006, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.111 [-0.170, 0.637], loss: 0.000029, mean_absolute_error: 0.473377, mean_q: 0.706632\n",
      " 37710/50000: episode: 867, duration: 0.096s, episode steps: 28, steps per second: 293, episode reward: 0.950, mean reward: 0.034 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.052 [-0.130, 0.266], loss: 0.000037, mean_absolute_error: 0.481170, mean_q: 0.716918\n",
      " 37765/50000: episode: 868, duration: 0.175s, episode steps: 55, steps per second: 314, episode reward: 0.714, mean reward: 0.013 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.189 [-0.884, 0.200], loss: 0.000047, mean_absolute_error: 0.470171, mean_q: 0.699667\n",
      " 37780/50000: episode: 869, duration: 0.052s, episode steps: 15, steps per second: 287, episode reward: 0.982, mean reward: 0.065 [-0.002, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.031 [-0.161, 0.110], loss: 0.000063, mean_absolute_error: 0.468681, mean_q: 0.696285\n",
      " 37796/50000: episode: 870, duration: 0.065s, episode steps: 16, steps per second: 246, episode reward: 0.980, mean reward: 0.061 [-0.002, 1.000], mean action: 0.438 [0.000, 2.000], mean observation: 0.029 [-0.120, 0.172], loss: 0.000042, mean_absolute_error: 0.473707, mean_q: 0.704392\n",
      " 37843/50000: episode: 871, duration: 0.157s, episode steps: 47, steps per second: 300, episode reward: 0.853, mean reward: 0.018 [-0.005, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.106 [-0.545, 0.180], loss: 0.000069, mean_absolute_error: 0.464938, mean_q: 0.689061\n",
      " 37898/50000: episode: 872, duration: 0.162s, episode steps: 55, steps per second: 340, episode reward: 0.735, mean reward: 0.013 [-0.008, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.175 [-0.831, 0.190], loss: 0.000058, mean_absolute_error: 0.472953, mean_q: 0.703875\n",
      " 37958/50000: episode: 873, duration: 0.182s, episode steps: 60, steps per second: 329, episode reward: 0.695, mean reward: 0.012 [-0.009, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.187 [-0.200, 0.911], loss: 0.000040, mean_absolute_error: 0.476416, mean_q: 0.711181\n",
      " 37976/50000: episode: 874, duration: 0.053s, episode steps: 18, steps per second: 337, episode reward: 0.974, mean reward: 0.054 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.037 [-0.110, 0.204], loss: 0.000060, mean_absolute_error: 0.480285, mean_q: 0.716373\n",
      " 38027/50000: episode: 875, duration: 0.147s, episode steps: 51, steps per second: 347, episode reward: 0.789, mean reward: 0.015 [-0.007, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.147 [-0.707, 0.170], loss: 0.000036, mean_absolute_error: 0.474918, mean_q: 0.708483\n",
      " 38086/50000: episode: 876, duration: 0.186s, episode steps: 59, steps per second: 317, episode reward: 0.795, mean reward: 0.013 [-0.006, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.124 [-0.170, 0.632], loss: 0.000066, mean_absolute_error: 0.467640, mean_q: 0.696786\n",
      " 38140/50000: episode: 877, duration: 0.168s, episode steps: 54, steps per second: 321, episode reward: 0.766, mean reward: 0.014 [-0.008, 1.000], mean action: 1.093 [0.000, 2.000], mean observation: -0.156 [-0.761, 0.190], loss: 0.000062, mean_absolute_error: 0.480528, mean_q: 0.716995\n",
      " 38154/50000: episode: 878, duration: 0.054s, episode steps: 14, steps per second: 258, episode reward: 0.980, mean reward: 0.070 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.043 [-0.185, 0.100], loss: 0.000030, mean_absolute_error: 0.468030, mean_q: 0.697884\n",
      " 38185/50000: episode: 879, duration: 0.095s, episode steps: 31, steps per second: 326, episode reward: 0.931, mean reward: 0.030 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.057 [-0.370, 0.190], loss: 0.000038, mean_absolute_error: 0.469720, mean_q: 0.699421\n",
      " 38246/50000: episode: 880, duration: 0.181s, episode steps: 61, steps per second: 337, episode reward: 0.744, mean reward: 0.012 [-0.008, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.153 [-0.200, 0.770], loss: 0.000051, mean_absolute_error: 0.472456, mean_q: 0.704204\n",
      " 38264/50000: episode: 881, duration: 0.058s, episode steps: 18, steps per second: 312, episode reward: 0.972, mean reward: 0.054 [-0.002, 1.000], mean action: 0.556 [0.000, 2.000], mean observation: 0.047 [-0.090, 0.214], loss: 0.000058, mean_absolute_error: 0.477692, mean_q: 0.714065\n",
      " 38296/50000: episode: 882, duration: 0.099s, episode steps: 32, steps per second: 324, episode reward: 0.929, mean reward: 0.029 [-0.003, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.069 [-0.120, 0.339], loss: 0.000037, mean_absolute_error: 0.476958, mean_q: 0.711355\n",
      " 38366/50000: episode: 883, duration: 0.204s, episode steps: 70, steps per second: 342, episode reward: 0.667, mean reward: 0.010 [-0.009, 1.000], mean action: 0.871 [0.000, 2.000], mean observation: 0.172 [-0.240, 0.928], loss: 0.000054, mean_absolute_error: 0.477633, mean_q: 0.713336\n",
      " 38428/50000: episode: 884, duration: 0.194s, episode steps: 62, steps per second: 320, episode reward: 0.682, mean reward: 0.011 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.190 [-0.877, 0.190], loss: 0.000038, mean_absolute_error: 0.468762, mean_q: 0.698780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38444/50000: episode: 885, duration: 0.056s, episode steps: 16, steps per second: 285, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 1.562 [0.000, 2.000], mean observation: -0.046 [-0.177, 0.100], loss: 0.000039, mean_absolute_error: 0.473110, mean_q: 0.708783\n",
      " 38484/50000: episode: 886, duration: 0.128s, episode steps: 40, steps per second: 312, episode reward: 0.898, mean reward: 0.022 [-0.004, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.091 [-0.100, 0.391], loss: 0.000041, mean_absolute_error: 0.472498, mean_q: 0.706840\n",
      " 38538/50000: episode: 887, duration: 0.168s, episode steps: 54, steps per second: 322, episode reward: 0.802, mean reward: 0.015 [-0.006, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.135 [-0.170, 0.629], loss: 0.000043, mean_absolute_error: 0.476175, mean_q: 0.710602\n",
      " 38576/50000: episode: 888, duration: 0.117s, episode steps: 38, steps per second: 324, episode reward: 0.896, mean reward: 0.024 [-0.004, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.093 [-0.130, 0.437], loss: 0.000041, mean_absolute_error: 0.473224, mean_q: 0.705572\n",
      " 38637/50000: episode: 889, duration: 0.218s, episode steps: 61, steps per second: 280, episode reward: 0.723, mean reward: 0.012 [-0.008, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.171 [-0.190, 0.790], loss: 0.000034, mean_absolute_error: 0.474317, mean_q: 0.709670\n",
      " 38688/50000: episode: 890, duration: 0.280s, episode steps: 51, steps per second: 182, episode reward: 0.797, mean reward: 0.016 [-0.007, 1.000], mean action: 0.843 [0.000, 2.000], mean observation: 0.144 [-0.160, 0.666], loss: 0.000037, mean_absolute_error: 0.470189, mean_q: 0.703640\n",
      " 38689/50000: episode: 891, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.010 [-0.030, 0.010], loss: 0.000024, mean_absolute_error: 0.463324, mean_q: 0.700478\n",
      " 38717/50000: episode: 892, duration: 0.121s, episode steps: 28, steps per second: 231, episode reward: 0.945, mean reward: 0.034 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.052 [-0.311, 0.140], loss: 0.000045, mean_absolute_error: 0.472241, mean_q: 0.705227\n",
      " 38778/50000: episode: 893, duration: 0.266s, episode steps: 61, steps per second: 229, episode reward: 0.715, mean reward: 0.012 [-0.009, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.172 [-0.230, 0.860], loss: 0.000043, mean_absolute_error: 0.472810, mean_q: 0.706660\n",
      " 38809/50000: episode: 894, duration: 0.134s, episode steps: 31, steps per second: 232, episode reward: 0.926, mean reward: 0.030 [-0.004, 1.000], mean action: 0.710 [0.000, 2.000], mean observation: 0.079 [-0.120, 0.357], loss: 0.000053, mean_absolute_error: 0.473131, mean_q: 0.707964\n",
      " 38864/50000: episode: 895, duration: 0.186s, episode steps: 55, steps per second: 295, episode reward: 0.795, mean reward: 0.014 [-0.007, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.130 [-0.190, 0.684], loss: 0.000062, mean_absolute_error: 0.475517, mean_q: 0.710191\n",
      " 38903/50000: episode: 896, duration: 0.128s, episode steps: 39, steps per second: 304, episode reward: 0.896, mean reward: 0.023 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.078 [-0.452, 0.160], loss: 0.000043, mean_absolute_error: 0.474843, mean_q: 0.713067\n",
      " 38904/50000: episode: 897, duration: 0.007s, episode steps: 1, steps per second: 154, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [-0.010, 0.071], loss: 0.000069, mean_absolute_error: 0.436197, mean_q: 0.661132\n",
      " 38958/50000: episode: 898, duration: 0.197s, episode steps: 54, steps per second: 274, episode reward: 0.802, mean reward: 0.015 [-0.007, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.129 [-0.170, 0.658], loss: 0.000037, mean_absolute_error: 0.471135, mean_q: 0.705138\n",
      " 39004/50000: episode: 899, duration: 0.136s, episode steps: 46, steps per second: 338, episode reward: 0.829, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.125 [-0.627, 0.190], loss: 0.000035, mean_absolute_error: 0.474029, mean_q: 0.707277\n",
      " 39023/50000: episode: 900, duration: 0.060s, episode steps: 19, steps per second: 317, episode reward: 0.972, mean reward: 0.051 [-0.002, 1.000], mean action: 0.579 [0.000, 2.000], mean observation: 0.050 [-0.090, 0.194], loss: 0.000065, mean_absolute_error: 0.469444, mean_q: 0.700073\n",
      " 39063/50000: episode: 901, duration: 0.126s, episode steps: 40, steps per second: 317, episode reward: 0.866, mean reward: 0.022 [-0.005, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.111 [-0.543, 0.160], loss: 0.000038, mean_absolute_error: 0.475389, mean_q: 0.712202\n",
      " 39074/50000: episode: 902, duration: 0.034s, episode steps: 11, steps per second: 324, episode reward: 0.988, mean reward: 0.090 [-0.001, 1.000], mean action: 1.818 [0.000, 2.000], mean observation: -0.031 [-0.145, 0.100], loss: 0.000039, mean_absolute_error: 0.478956, mean_q: 0.719348\n",
      " 39121/50000: episode: 903, duration: 0.142s, episode steps: 47, steps per second: 330, episode reward: 0.835, mean reward: 0.018 [-0.006, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.115 [-0.170, 0.611], loss: 0.000037, mean_absolute_error: 0.471249, mean_q: 0.705304\n",
      " 39168/50000: episode: 904, duration: 0.147s, episode steps: 47, steps per second: 319, episode reward: 0.861, mean reward: 0.018 [-0.005, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.096 [-0.160, 0.527], loss: 0.000047, mean_absolute_error: 0.472826, mean_q: 0.706870\n",
      " 39222/50000: episode: 905, duration: 0.156s, episode steps: 54, steps per second: 346, episode reward: 0.760, mean reward: 0.014 [-0.008, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.157 [-0.230, 0.784], loss: 0.000041, mean_absolute_error: 0.477594, mean_q: 0.717416\n",
      " 39271/50000: episode: 906, duration: 0.141s, episode steps: 49, steps per second: 347, episode reward: 0.777, mean reward: 0.016 [-0.007, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.161 [-0.736, 0.230], loss: 0.000054, mean_absolute_error: 0.473862, mean_q: 0.708138\n",
      " 39328/50000: episode: 907, duration: 0.187s, episode steps: 57, steps per second: 304, episode reward: 0.702, mean reward: 0.012 [-0.009, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.190 [-0.892, 0.230], loss: 0.000050, mean_absolute_error: 0.470410, mean_q: 0.704651\n",
      " 39362/50000: episode: 908, duration: 0.118s, episode steps: 34, steps per second: 288, episode reward: 0.897, mean reward: 0.026 [-0.005, 1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.092 [-0.484, 0.200], loss: 0.000058, mean_absolute_error: 0.473315, mean_q: 0.709185\n",
      " 39422/50000: episode: 909, duration: 0.203s, episode steps: 60, steps per second: 295, episode reward: 0.723, mean reward: 0.012 [-0.008, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.170 [-0.832, 0.180], loss: 0.000041, mean_absolute_error: 0.475610, mean_q: 0.711198\n",
      " 39497/50000: episode: 910, duration: 0.227s, episode steps: 75, steps per second: 330, episode reward: 0.646, mean reward: 0.009 [-0.009, 1.000], mean action: 0.880 [0.000, 2.000], mean observation: 0.179 [-0.230, 0.947], loss: 0.000079, mean_absolute_error: 0.467426, mean_q: 0.700006\n",
      " 39537/50000: episode: 911, duration: 0.129s, episode steps: 40, steps per second: 311, episode reward: 0.874, mean reward: 0.022 [-0.005, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.099 [-0.527, 0.160], loss: 0.000033, mean_absolute_error: 0.468986, mean_q: 0.704370\n",
      " 39592/50000: episode: 912, duration: 0.160s, episode steps: 55, steps per second: 345, episode reward: 0.731, mean reward: 0.013 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.177 [-0.821, 0.220], loss: 0.000046, mean_absolute_error: 0.471084, mean_q: 0.705931\n",
      " 39593/50000: episode: 913, duration: 0.006s, episode steps: 1, steps per second: 173, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.001 [-0.009, 0.010], loss: 0.000061, mean_absolute_error: 0.475831, mean_q: 0.705165\n",
      " 39634/50000: episode: 914, duration: 0.121s, episode steps: 41, steps per second: 339, episode reward: 0.864, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.105 [-0.160, 0.558], loss: 0.000033, mean_absolute_error: 0.470540, mean_q: 0.704536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39681/50000: episode: 915, duration: 0.141s, episode steps: 47, steps per second: 333, episode reward: 0.797, mean reward: 0.017 [-0.007, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.151 [-0.705, 0.200], loss: 0.000031, mean_absolute_error: 0.471191, mean_q: 0.706017\n",
      " 39735/50000: episode: 916, duration: 0.191s, episode steps: 54, steps per second: 283, episode reward: 0.750, mean reward: 0.014 [-0.008, 1.000], mean action: 0.926 [0.000, 2.000], mean observation: 0.166 [-0.210, 0.814], loss: 0.000044, mean_absolute_error: 0.467334, mean_q: 0.701160\n",
      " 39783/50000: episode: 917, duration: 0.215s, episode steps: 48, steps per second: 223, episode reward: 0.819, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.125 [-0.657, 0.200], loss: 0.000037, mean_absolute_error: 0.472426, mean_q: 0.709183\n",
      " 39843/50000: episode: 918, duration: 0.291s, episode steps: 60, steps per second: 206, episode reward: 0.759, mean reward: 0.013 [-0.008, 1.000], mean action: 1.050 [0.000, 2.000], mean observation: -0.146 [-0.764, 0.230], loss: 0.000073, mean_absolute_error: 0.469687, mean_q: 0.705642\n",
      " 39891/50000: episode: 919, duration: 0.210s, episode steps: 48, steps per second: 229, episode reward: 0.807, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.143 [-0.654, 0.170], loss: 0.000077, mean_absolute_error: 0.473436, mean_q: 0.708160\n",
      " 39941/50000: episode: 920, duration: 0.155s, episode steps: 50, steps per second: 324, episode reward: 0.771, mean reward: 0.015 [-0.008, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.164 [-0.759, 0.200], loss: 0.000039, mean_absolute_error: 0.476723, mean_q: 0.715561\n",
      " 40001/50000: episode: 921, duration: 0.172s, episode steps: 60, steps per second: 348, episode reward: 0.752, mean reward: 0.013 [-0.008, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.145 [-0.210, 0.783], loss: 0.000040, mean_absolute_error: 0.467438, mean_q: 0.700909\n",
      " 40069/50000: episode: 922, duration: 0.194s, episode steps: 68, steps per second: 350, episode reward: 0.678, mean reward: 0.010 [-0.009, 1.000], mean action: 0.912 [0.000, 2.000], mean observation: 0.178 [-0.240, 0.911], loss: 0.000051, mean_absolute_error: 0.473949, mean_q: 0.711480\n",
      " 40126/50000: episode: 923, duration: 0.177s, episode steps: 57, steps per second: 322, episode reward: 0.711, mean reward: 0.012 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.182 [-0.240, 0.911], loss: 0.000053, mean_absolute_error: 0.473247, mean_q: 0.710649\n",
      " 40166/50000: episode: 924, duration: 0.127s, episode steps: 40, steps per second: 314, episode reward: 0.927, mean reward: 0.023 [-0.003, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.049 [-0.130, 0.328], loss: 0.000092, mean_absolute_error: 0.477137, mean_q: 0.715710\n",
      " 40220/50000: episode: 925, duration: 0.170s, episode steps: 54, steps per second: 318, episode reward: 0.783, mean reward: 0.015 [-0.007, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.145 [-0.170, 0.702], loss: 0.000037, mean_absolute_error: 0.471956, mean_q: 0.708119\n",
      " 40232/50000: episode: 926, duration: 0.038s, episode steps: 12, steps per second: 315, episode reward: 0.984, mean reward: 0.082 [-0.002, 1.000], mean action: 1.667 [0.000, 2.000], mean observation: -0.040 [-0.169, 0.100], loss: 0.000031, mean_absolute_error: 0.477980, mean_q: 0.718704\n",
      " 40293/50000: episode: 927, duration: 0.263s, episode steps: 61, steps per second: 232, episode reward: 0.656, mean reward: 0.011 [-0.010, 1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.210 [-0.984, 0.200], loss: 0.000043, mean_absolute_error: 0.477540, mean_q: 0.716360\n",
      " 40343/50000: episode: 928, duration: 0.182s, episode steps: 50, steps per second: 274, episode reward: 0.795, mean reward: 0.016 [-0.007, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.146 [-0.180, 0.688], loss: 0.000042, mean_absolute_error: 0.470361, mean_q: 0.707120\n",
      " 40381/50000: episode: 929, duration: 0.112s, episode steps: 38, steps per second: 340, episode reward: 0.871, mean reward: 0.023 [-0.005, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.111 [-0.546, 0.190], loss: 0.000034, mean_absolute_error: 0.471172, mean_q: 0.707492\n",
      " 40443/50000: episode: 930, duration: 0.180s, episode steps: 62, steps per second: 345, episode reward: 0.672, mean reward: 0.011 [-0.010, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.196 [-0.270, 0.958], loss: 0.000036, mean_absolute_error: 0.469714, mean_q: 0.706320\n",
      " 40472/50000: episode: 931, duration: 0.088s, episode steps: 29, steps per second: 330, episode reward: 0.932, mean reward: 0.032 [-0.004, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.070 [-0.351, 0.160], loss: 0.000045, mean_absolute_error: 0.477043, mean_q: 0.718075\n",
      " 40473/50000: episode: 932, duration: 0.008s, episode steps: 1, steps per second: 133, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [-0.010, 0.042], loss: 0.000037, mean_absolute_error: 0.496289, mean_q: 0.750390\n",
      " 40521/50000: episode: 933, duration: 0.206s, episode steps: 48, steps per second: 233, episode reward: 0.840, mean reward: 0.017 [-0.006, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.116 [-0.170, 0.590], loss: 0.000031, mean_absolute_error: 0.471821, mean_q: 0.710176\n",
      " 40522/50000: episode: 934, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.014 [-0.019, -0.010], loss: 0.000238, mean_absolute_error: 0.480266, mean_q: 0.719463\n",
      " 40562/50000: episode: 935, duration: 0.144s, episode steps: 40, steps per second: 277, episode reward: 0.852, mean reward: 0.021 [-0.006, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.123 [-0.601, 0.180], loss: 0.000079, mean_absolute_error: 0.471825, mean_q: 0.709657\n",
      " 40563/50000: episode: 936, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.023 [-0.055, 0.010], loss: 0.000070, mean_absolute_error: 0.469803, mean_q: 0.708928\n",
      " 40592/50000: episode: 937, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 0.946, mean reward: 0.033 [-0.003, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.051 [-0.286, 0.140], loss: 0.000053, mean_absolute_error: 0.461742, mean_q: 0.694357\n",
      " 40626/50000: episode: 938, duration: 0.153s, episode steps: 34, steps per second: 222, episode reward: 0.906, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.090 [-0.140, 0.428], loss: 0.000029, mean_absolute_error: 0.464353, mean_q: 0.697382\n",
      " 40672/50000: episode: 939, duration: 0.254s, episode steps: 46, steps per second: 181, episode reward: 0.823, mean reward: 0.018 [-0.007, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.130 [-0.653, 0.190], loss: 0.000039, mean_absolute_error: 0.476783, mean_q: 0.717717\n",
      " 40733/50000: episode: 940, duration: 0.274s, episode steps: 61, steps per second: 223, episode reward: 0.715, mean reward: 0.012 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.170 [-0.230, 0.881], loss: 0.000047, mean_absolute_error: 0.475313, mean_q: 0.715509\n",
      " 40790/50000: episode: 941, duration: 0.243s, episode steps: 57, steps per second: 234, episode reward: 0.695, mean reward: 0.012 [-0.009, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.197 [-0.910, 0.220], loss: 0.000042, mean_absolute_error: 0.479425, mean_q: 0.722201\n",
      " 40826/50000: episode: 942, duration: 0.150s, episode steps: 36, steps per second: 240, episode reward: 0.887, mean reward: 0.025 [-0.005, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.103 [-0.492, 0.180], loss: 0.000033, mean_absolute_error: 0.479083, mean_q: 0.721661\n",
      " 40884/50000: episode: 943, duration: 0.242s, episode steps: 58, steps per second: 239, episode reward: 0.741, mean reward: 0.013 [-0.008, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.155 [-0.250, 0.840], loss: 0.000034, mean_absolute_error: 0.471296, mean_q: 0.709846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40949/50000: episode: 944, duration: 0.254s, episode steps: 65, steps per second: 256, episode reward: 0.650, mean reward: 0.010 [-0.009, 1.000], mean action: 1.092 [0.000, 2.000], mean observation: -0.205 [-0.936, 0.200], loss: 0.000048, mean_absolute_error: 0.478396, mean_q: 0.719927\n",
      " 41010/50000: episode: 945, duration: 0.189s, episode steps: 61, steps per second: 322, episode reward: 0.714, mean reward: 0.012 [-0.008, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.174 [-0.210, 0.839], loss: 0.000031, mean_absolute_error: 0.472024, mean_q: 0.711797\n",
      " 41043/50000: episode: 946, duration: 0.106s, episode steps: 33, steps per second: 310, episode reward: 0.906, mean reward: 0.027 [-0.004, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.092 [-0.140, 0.440], loss: 0.000088, mean_absolute_error: 0.461444, mean_q: 0.695043\n",
      " 41092/50000: episode: 947, duration: 0.143s, episode steps: 49, steps per second: 343, episode reward: 0.784, mean reward: 0.016 [-0.008, 1.000], mean action: 1.163 [0.000, 2.000], mean observation: -0.155 [-0.752, 0.210], loss: 0.000030, mean_absolute_error: 0.479825, mean_q: 0.723430\n",
      " 41152/50000: episode: 948, duration: 0.184s, episode steps: 60, steps per second: 325, episode reward: 0.702, mean reward: 0.012 [-0.008, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.188 [-0.821, 0.200], loss: 0.000057, mean_absolute_error: 0.465389, mean_q: 0.700116\n",
      " 41213/50000: episode: 949, duration: 0.184s, episode steps: 61, steps per second: 331, episode reward: 0.692, mean reward: 0.011 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.189 [-0.869, 0.210], loss: 0.000048, mean_absolute_error: 0.469794, mean_q: 0.707858\n",
      " 41281/50000: episode: 950, duration: 0.213s, episode steps: 68, steps per second: 319, episode reward: 0.648, mean reward: 0.010 [-0.010, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.191 [-0.230, 0.991], loss: 0.000035, mean_absolute_error: 0.471027, mean_q: 0.711098\n",
      " 41336/50000: episode: 951, duration: 0.223s, episode steps: 55, steps per second: 247, episode reward: 0.744, mean reward: 0.014 [-0.008, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.167 [-0.240, 0.820], loss: 0.000051, mean_absolute_error: 0.482868, mean_q: 0.727995\n",
      " 41379/50000: episode: 952, duration: 0.128s, episode steps: 43, steps per second: 335, episode reward: 0.846, mean reward: 0.020 [-0.006, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.122 [-0.600, 0.190], loss: 0.000034, mean_absolute_error: 0.470080, mean_q: 0.709351\n",
      " 41437/50000: episode: 953, duration: 0.169s, episode steps: 58, steps per second: 344, episode reward: 0.706, mean reward: 0.012 [-0.009, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.183 [-0.250, 0.906], loss: 0.000040, mean_absolute_error: 0.475820, mean_q: 0.717461\n",
      " 41463/50000: episode: 954, duration: 0.082s, episode steps: 26, steps per second: 317, episode reward: 0.944, mean reward: 0.036 [-0.003, 1.000], mean action: 0.692 [0.000, 2.000], mean observation: 0.068 [-0.140, 0.311], loss: 0.000045, mean_absolute_error: 0.473271, mean_q: 0.714041\n",
      " 41504/50000: episode: 955, duration: 0.122s, episode steps: 41, steps per second: 335, episode reward: 0.897, mean reward: 0.022 [-0.004, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.090 [-0.110, 0.390], loss: 0.000034, mean_absolute_error: 0.471184, mean_q: 0.710589\n",
      " 41555/50000: episode: 956, duration: 0.188s, episode steps: 51, steps per second: 271, episode reward: 0.810, mean reward: 0.016 [-0.007, 1.000], mean action: 1.118 [0.000, 2.000], mean observation: -0.132 [-0.653, 0.190], loss: 0.000094, mean_absolute_error: 0.476191, mean_q: 0.718279\n",
      " 41598/50000: episode: 957, duration: 0.129s, episode steps: 43, steps per second: 333, episode reward: 0.880, mean reward: 0.020 [-0.005, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.098 [-0.120, 0.461], loss: 0.000065, mean_absolute_error: 0.476763, mean_q: 0.717833\n",
      " 41624/50000: episode: 958, duration: 0.117s, episode steps: 26, steps per second: 222, episode reward: 0.948, mean reward: 0.036 [-0.003, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.063 [-0.302, 0.120], loss: 0.000062, mean_absolute_error: 0.468442, mean_q: 0.704732\n",
      " 41642/50000: episode: 959, duration: 0.064s, episode steps: 18, steps per second: 280, episode reward: 0.976, mean reward: 0.054 [-0.002, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.048 [-0.060, 0.177], loss: 0.000040, mean_absolute_error: 0.472883, mean_q: 0.713544\n",
      " 41688/50000: episode: 960, duration: 0.171s, episode steps: 46, steps per second: 269, episode reward: 0.821, mean reward: 0.018 [-0.006, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.136 [-0.200, 0.641], loss: 0.000026, mean_absolute_error: 0.480535, mean_q: 0.725919\n",
      " 41748/50000: episode: 961, duration: 0.302s, episode steps: 60, steps per second: 199, episode reward: 0.702, mean reward: 0.012 [-0.009, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.182 [-0.894, 0.210], loss: 0.000031, mean_absolute_error: 0.472012, mean_q: 0.712960\n",
      " 41808/50000: episode: 962, duration: 0.250s, episode steps: 60, steps per second: 240, episode reward: 0.676, mean reward: 0.011 [-0.010, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.199 [-0.959, 0.210], loss: 0.000029, mean_absolute_error: 0.473860, mean_q: 0.715158\n",
      " 41827/50000: episode: 963, duration: 0.086s, episode steps: 19, steps per second: 222, episode reward: 0.967, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.051 [-0.110, 0.233], loss: 0.000027, mean_absolute_error: 0.471778, mean_q: 0.713016\n",
      " 41838/50000: episode: 964, duration: 0.040s, episode steps: 11, steps per second: 276, episode reward: 0.987, mean reward: 0.090 [-0.001, 1.000], mean action: 0.273 [0.000, 2.000], mean observation: 0.038 [-0.080, 0.145], loss: 0.000024, mean_absolute_error: 0.473675, mean_q: 0.716138\n",
      " 41888/50000: episode: 965, duration: 0.170s, episode steps: 50, steps per second: 294, episode reward: 0.741, mean reward: 0.015 [-0.009, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.181 [-0.270, 0.878], loss: 0.000032, mean_absolute_error: 0.471992, mean_q: 0.712209\n",
      " 41911/50000: episode: 966, duration: 0.074s, episode steps: 23, steps per second: 312, episode reward: 0.960, mean reward: 0.042 [-0.002, 1.000], mean action: 0.652 [0.000, 2.000], mean observation: 0.058 [-0.100, 0.239], loss: 0.000031, mean_absolute_error: 0.481994, mean_q: 0.727507\n",
      " 41969/50000: episode: 967, duration: 0.178s, episode steps: 58, steps per second: 325, episode reward: 0.759, mean reward: 0.013 [-0.008, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.142 [-0.230, 0.786], loss: 0.000045, mean_absolute_error: 0.476220, mean_q: 0.718761\n",
      " 41997/50000: episode: 968, duration: 0.101s, episode steps: 28, steps per second: 278, episode reward: 0.936, mean reward: 0.033 [-0.003, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: -0.072 [-0.346, 0.130], loss: 0.000038, mean_absolute_error: 0.478744, mean_q: 0.722189\n",
      " 42044/50000: episode: 969, duration: 0.153s, episode steps: 47, steps per second: 307, episode reward: 0.844, mean reward: 0.018 [-0.006, 1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.118 [-0.554, 0.160], loss: 0.000035, mean_absolute_error: 0.476038, mean_q: 0.719217\n",
      " 42100/50000: episode: 970, duration: 0.192s, episode steps: 56, steps per second: 292, episode reward: 0.742, mean reward: 0.013 [-0.008, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.169 [-0.230, 0.796], loss: 0.000042, mean_absolute_error: 0.477788, mean_q: 0.721473\n",
      " 42152/50000: episode: 971, duration: 0.167s, episode steps: 52, steps per second: 312, episode reward: 0.821, mean reward: 0.016 [-0.006, 1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.128 [-0.560, 0.150], loss: 0.000041, mean_absolute_error: 0.477590, mean_q: 0.721072\n",
      " 42186/50000: episode: 972, duration: 0.099s, episode steps: 34, steps per second: 342, episode reward: 0.915, mean reward: 0.027 [-0.004, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.082 [-0.404, 0.160], loss: 0.000052, mean_absolute_error: 0.478124, mean_q: 0.722125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42240/50000: episode: 973, duration: 0.212s, episode steps: 54, steps per second: 255, episode reward: 0.759, mean reward: 0.014 [-0.008, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.160 [-0.788, 0.200], loss: 0.000033, mean_absolute_error: 0.470152, mean_q: 0.710461\n",
      " 42313/50000: episode: 974, duration: 0.255s, episode steps: 73, steps per second: 286, episode reward: 0.698, mean reward: 0.010 [-0.009, 1.000], mean action: 0.986 [0.000, 2.000], mean observation: 0.153 [-0.250, 0.887], loss: 0.000029, mean_absolute_error: 0.478226, mean_q: 0.722172\n",
      " 42314/50000: episode: 975, duration: 0.006s, episode steps: 1, steps per second: 158, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.033 [0.010, 0.057], loss: 0.000053, mean_absolute_error: 0.453774, mean_q: 0.678668\n",
      " 42332/50000: episode: 976, duration: 0.056s, episode steps: 18, steps per second: 321, episode reward: 0.972, mean reward: 0.054 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.049 [-0.090, 0.211], loss: 0.000040, mean_absolute_error: 0.476021, mean_q: 0.718387\n",
      " 42349/50000: episode: 977, duration: 0.054s, episode steps: 17, steps per second: 314, episode reward: 0.979, mean reward: 0.058 [-0.002, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.045 [-0.050, 0.160], loss: 0.000044, mean_absolute_error: 0.472436, mean_q: 0.712258\n",
      " 42395/50000: episode: 978, duration: 0.133s, episode steps: 46, steps per second: 345, episode reward: 0.817, mean reward: 0.018 [-0.007, 1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.139 [-0.665, 0.180], loss: 0.000051, mean_absolute_error: 0.479831, mean_q: 0.723484\n",
      " 42447/50000: episode: 979, duration: 0.151s, episode steps: 52, steps per second: 343, episode reward: 0.800, mean reward: 0.015 [-0.007, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.130 [-0.190, 0.658], loss: 0.000027, mean_absolute_error: 0.477887, mean_q: 0.721441\n",
      " 42448/50000: episode: 980, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.009 [0.008, 0.010], loss: 0.000017, mean_absolute_error: 0.437621, mean_q: 0.660133\n",
      " 42509/50000: episode: 981, duration: 0.218s, episode steps: 61, steps per second: 280, episode reward: 0.712, mean reward: 0.012 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.163 [-0.250, 0.869], loss: 0.000039, mean_absolute_error: 0.475395, mean_q: 0.717986\n",
      " 42552/50000: episode: 982, duration: 0.179s, episode steps: 43, steps per second: 240, episode reward: 0.851, mean reward: 0.020 [-0.006, 1.000], mean action: 0.907 [0.000, 2.000], mean observation: 0.118 [-0.160, 0.582], loss: 0.000039, mean_absolute_error: 0.474177, mean_q: 0.715722\n",
      " 42605/50000: episode: 983, duration: 0.212s, episode steps: 53, steps per second: 250, episode reward: 0.793, mean reward: 0.015 [-0.007, 1.000], mean action: 1.075 [0.000, 2.000], mean observation: -0.141 [-0.688, 0.170], loss: 0.000042, mean_absolute_error: 0.481206, mean_q: 0.726411\n",
      " 42631/50000: episode: 984, duration: 0.093s, episode steps: 26, steps per second: 278, episode reward: 0.960, mean reward: 0.037 [-0.002, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.053 [-0.090, 0.227], loss: 0.000110, mean_absolute_error: 0.483412, mean_q: 0.729841\n",
      " 42692/50000: episode: 985, duration: 0.195s, episode steps: 61, steps per second: 313, episode reward: 0.661, mean reward: 0.011 [-0.009, 1.000], mean action: 0.869 [0.000, 2.000], mean observation: 0.212 [-0.230, 0.910], loss: 0.000039, mean_absolute_error: 0.477235, mean_q: 0.720310\n",
      " 42741/50000: episode: 986, duration: 0.141s, episode steps: 49, steps per second: 348, episode reward: 0.822, mean reward: 0.017 [-0.007, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.117 [-0.200, 0.665], loss: 0.000036, mean_absolute_error: 0.476954, mean_q: 0.719759\n",
      " 42777/50000: episode: 987, duration: 0.113s, episode steps: 36, steps per second: 320, episode reward: 0.912, mean reward: 0.025 [-0.004, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.085 [-0.376, 0.130], loss: 0.000043, mean_absolute_error: 0.479129, mean_q: 0.723610\n",
      " 42810/50000: episode: 988, duration: 0.126s, episode steps: 33, steps per second: 262, episode reward: 0.915, mean reward: 0.028 [-0.004, 1.000], mean action: 1.212 [0.000, 2.000], mean observation: -0.085 [-0.395, 0.140], loss: 0.000037, mean_absolute_error: 0.472027, mean_q: 0.712725\n",
      " 42841/50000: episode: 989, duration: 0.100s, episode steps: 31, steps per second: 311, episode reward: 0.926, mean reward: 0.030 [-0.004, 1.000], mean action: 1.194 [0.000, 2.000], mean observation: -0.076 [-0.375, 0.150], loss: 0.000068, mean_absolute_error: 0.480370, mean_q: 0.724430\n",
      " 42904/50000: episode: 990, duration: 0.196s, episode steps: 63, steps per second: 322, episode reward: 0.693, mean reward: 0.011 [-0.009, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.180 [-0.896, 0.200], loss: 0.000039, mean_absolute_error: 0.477039, mean_q: 0.720076\n",
      " 42950/50000: episode: 991, duration: 0.197s, episode steps: 46, steps per second: 234, episode reward: 0.824, mean reward: 0.018 [-0.006, 1.000], mean action: 1.174 [0.000, 2.000], mean observation: -0.133 [-0.642, 0.170], loss: 0.000052, mean_absolute_error: 0.479425, mean_q: 0.723103\n",
      " 42978/50000: episode: 992, duration: 0.103s, episode steps: 28, steps per second: 271, episode reward: 0.949, mean reward: 0.034 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.045 [-0.170, 0.281], loss: 0.000057, mean_absolute_error: 0.480878, mean_q: 0.725780\n",
      " 43029/50000: episode: 993, duration: 0.146s, episode steps: 51, steps per second: 348, episode reward: 0.752, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.167 [-0.240, 0.844], loss: 0.000035, mean_absolute_error: 0.481108, mean_q: 0.726132\n",
      " 43068/50000: episode: 994, duration: 0.112s, episode steps: 39, steps per second: 348, episode reward: 0.860, mean reward: 0.022 [-0.006, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.118 [-0.210, 0.570], loss: 0.000044, mean_absolute_error: 0.475285, mean_q: 0.717590\n",
      " 43116/50000: episode: 995, duration: 0.142s, episode steps: 48, steps per second: 338, episode reward: 0.826, mean reward: 0.017 [-0.006, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.127 [-0.635, 0.200], loss: 0.000079, mean_absolute_error: 0.481047, mean_q: 0.725953\n",
      " 43161/50000: episode: 996, duration: 0.158s, episode steps: 45, steps per second: 285, episode reward: 0.823, mean reward: 0.018 [-0.007, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.130 [-0.210, 0.659], loss: 0.000066, mean_absolute_error: 0.483178, mean_q: 0.729073\n",
      " 43191/50000: episode: 997, duration: 0.093s, episode steps: 30, steps per second: 324, episode reward: 0.927, mean reward: 0.031 [-0.004, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.078 [-0.367, 0.150], loss: 0.000034, mean_absolute_error: 0.474418, mean_q: 0.715917\n",
      " 43197/50000: episode: 998, duration: 0.021s, episode steps: 6, steps per second: 285, episode reward: 0.995, mean reward: 0.166 [-0.001, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.041 [-0.030, 0.110], loss: 0.000035, mean_absolute_error: 0.470885, mean_q: 0.712254\n",
      " 43210/50000: episode: 999, duration: 0.039s, episode steps: 13, steps per second: 335, episode reward: 0.984, mean reward: 0.076 [-0.002, 1.000], mean action: 0.308 [0.000, 2.000], mean observation: 0.033 [-0.100, 0.164], loss: 0.000040, mean_absolute_error: 0.475551, mean_q: 0.718650\n",
      " 43221/50000: episode: 1000, duration: 0.034s, episode steps: 11, steps per second: 328, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 1.818 [0.000, 2.000], mean observation: -0.037 [-0.154, 0.090], loss: 0.000024, mean_absolute_error: 0.482354, mean_q: 0.727741\n",
      " 43276/50000: episode: 1001, duration: 0.159s, episode steps: 55, steps per second: 347, episode reward: 0.742, mean reward: 0.013 [-0.008, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.166 [-0.210, 0.816], loss: 0.000036, mean_absolute_error: 0.475205, mean_q: 0.717224\n",
      " 43326/50000: episode: 1002, duration: 0.191s, episode steps: 50, steps per second: 262, episode reward: 0.792, mean reward: 0.016 [-0.007, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.148 [-0.707, 0.170], loss: 0.000045, mean_absolute_error: 0.476468, mean_q: 0.719816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 43369/50000: episode: 1003, duration: 0.155s, episode steps: 43, steps per second: 278, episode reward: 0.873, mean reward: 0.020 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.086 [-0.200, 0.515], loss: 0.000031, mean_absolute_error: 0.482113, mean_q: 0.728290\n",
      " 43399/50000: episode: 1004, duration: 0.125s, episode steps: 30, steps per second: 240, episode reward: 0.947, mean reward: 0.032 [-0.003, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.031 [-0.190, 0.321], loss: 0.000038, mean_absolute_error: 0.480973, mean_q: 0.726074\n",
      " 43466/50000: episode: 1005, duration: 0.237s, episode steps: 67, steps per second: 282, episode reward: 0.652, mean reward: 0.010 [-0.010, 1.000], mean action: 1.134 [0.000, 2.000], mean observation: -0.189 [-0.969, 0.210], loss: 0.000082, mean_absolute_error: 0.476588, mean_q: 0.718520\n",
      " 43511/50000: episode: 1006, duration: 0.130s, episode steps: 45, steps per second: 346, episode reward: 0.843, mean reward: 0.019 [-0.006, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.120 [-0.596, 0.180], loss: 0.000079, mean_absolute_error: 0.480817, mean_q: 0.724819\n",
      " 43530/50000: episode: 1007, duration: 0.058s, episode steps: 19, steps per second: 325, episode reward: 0.972, mean reward: 0.051 [-0.002, 1.000], mean action: 1.368 [0.000, 2.000], mean observation: -0.055 [-0.177, 0.080], loss: 0.000034, mean_absolute_error: 0.486602, mean_q: 0.734084\n",
      " 43572/50000: episode: 1008, duration: 0.125s, episode steps: 42, steps per second: 335, episode reward: 0.869, mean reward: 0.021 [-0.005, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.109 [-0.505, 0.170], loss: 0.000056, mean_absolute_error: 0.476997, mean_q: 0.720467\n",
      " 43591/50000: episode: 1009, duration: 0.066s, episode steps: 19, steps per second: 289, episode reward: 0.967, mean reward: 0.051 [-0.002, 1.000], mean action: 0.632 [0.000, 2.000], mean observation: 0.050 [-0.130, 0.241], loss: 0.000061, mean_absolute_error: 0.477973, mean_q: 0.720140\n",
      " 43600/50000: episode: 1010, duration: 0.033s, episode steps: 9, steps per second: 271, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 1.556 [0.000, 2.000], mean observation: -0.039 [-0.137, 0.070], loss: 0.000056, mean_absolute_error: 0.489738, mean_q: 0.739970\n",
      " 43624/50000: episode: 1011, duration: 0.104s, episode steps: 24, steps per second: 230, episode reward: 0.953, mean reward: 0.040 [-0.003, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.060 [-0.291, 0.140], loss: 0.000036, mean_absolute_error: 0.478777, mean_q: 0.723912\n",
      " 43684/50000: episode: 1012, duration: 0.267s, episode steps: 60, steps per second: 225, episode reward: 0.682, mean reward: 0.011 [-0.009, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.196 [-0.929, 0.210], loss: 0.000040, mean_absolute_error: 0.481188, mean_q: 0.726632\n",
      " 43717/50000: episode: 1013, duration: 0.113s, episode steps: 33, steps per second: 292, episode reward: 0.909, mean reward: 0.028 [-0.004, 1.000], mean action: 1.212 [0.000, 2.000], mean observation: -0.092 [-0.410, 0.140], loss: 0.000086, mean_absolute_error: 0.478728, mean_q: 0.722818\n",
      " 43758/50000: episode: 1014, duration: 0.158s, episode steps: 41, steps per second: 260, episode reward: 0.862, mean reward: 0.021 [-0.006, 1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.114 [-0.551, 0.170], loss: 0.000038, mean_absolute_error: 0.474755, mean_q: 0.717104\n",
      " 43768/50000: episode: 1015, duration: 0.036s, episode steps: 10, steps per second: 275, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: -0.040 [-0.138, 0.050], loss: 0.000031, mean_absolute_error: 0.476740, mean_q: 0.719240\n",
      " 43788/50000: episode: 1016, duration: 0.074s, episode steps: 20, steps per second: 269, episode reward: 0.974, mean reward: 0.049 [-0.002, 1.000], mean action: 0.900 [0.000, 2.000], mean observation: 0.051 [-0.080, 0.168], loss: 0.000047, mean_absolute_error: 0.483283, mean_q: 0.729137\n",
      " 43846/50000: episode: 1017, duration: 0.223s, episode steps: 58, steps per second: 261, episode reward: 0.747, mean reward: 0.013 [-0.008, 1.000], mean action: 1.086 [0.000, 2.000], mean observation: -0.159 [-0.782, 0.160], loss: 0.000036, mean_absolute_error: 0.476082, mean_q: 0.718760\n",
      " 43847/50000: episode: 1018, duration: 0.007s, episode steps: 1, steps per second: 141, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.029 [-0.068, 0.010], loss: 0.000026, mean_absolute_error: 0.470182, mean_q: 0.708711\n",
      " 43855/50000: episode: 1019, duration: 0.032s, episode steps: 8, steps per second: 250, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 1.750 [0.000, 2.000], mean observation: -0.042 [-0.123, 0.060], loss: 0.000017, mean_absolute_error: 0.476485, mean_q: 0.719213\n",
      " 43908/50000: episode: 1020, duration: 0.270s, episode steps: 53, steps per second: 196, episode reward: 0.762, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.154 [-0.200, 0.788], loss: 0.000044, mean_absolute_error: 0.479701, mean_q: 0.723668\n",
      " 43931/50000: episode: 1021, duration: 0.088s, episode steps: 23, steps per second: 260, episode reward: 0.954, mean reward: 0.041 [-0.003, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.054 [-0.150, 0.294], loss: 0.000032, mean_absolute_error: 0.472789, mean_q: 0.714633\n",
      " 43971/50000: episode: 1022, duration: 0.192s, episode steps: 40, steps per second: 209, episode reward: 0.849, mean reward: 0.021 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.125 [-0.210, 0.608], loss: 0.000066, mean_absolute_error: 0.480748, mean_q: 0.725063\n",
      " 44005/50000: episode: 1023, duration: 0.167s, episode steps: 34, steps per second: 204, episode reward: 0.904, mean reward: 0.027 [-0.005, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.082 [-0.190, 0.458], loss: 0.000038, mean_absolute_error: 0.479010, mean_q: 0.722744\n",
      " 44065/50000: episode: 1024, duration: 0.240s, episode steps: 60, steps per second: 250, episode reward: 0.680, mean reward: 0.011 [-0.009, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.192 [-0.240, 0.941], loss: 0.000063, mean_absolute_error: 0.478304, mean_q: 0.721555\n",
      " 44113/50000: episode: 1025, duration: 0.272s, episode steps: 48, steps per second: 176, episode reward: 0.796, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.149 [-0.715, 0.200], loss: 0.000048, mean_absolute_error: 0.481043, mean_q: 0.725948\n",
      " 44118/50000: episode: 1026, duration: 0.024s, episode steps: 5, steps per second: 207, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.039 [-0.113, 0.050], loss: 0.000039, mean_absolute_error: 0.473457, mean_q: 0.715391\n",
      " 44119/50000: episode: 1027, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.006 [0.002, 0.010], loss: 0.000011, mean_absolute_error: 0.457709, mean_q: 0.693818\n",
      " 44156/50000: episode: 1028, duration: 0.166s, episode steps: 37, steps per second: 222, episode reward: 0.887, mean reward: 0.024 [-0.005, 1.000], mean action: 1.189 [0.000, 2.000], mean observation: -0.101 [-0.491, 0.150], loss: 0.000047, mean_absolute_error: 0.479021, mean_q: 0.722925\n",
      " 44206/50000: episode: 1029, duration: 0.317s, episode steps: 50, steps per second: 158, episode reward: 0.754, mean reward: 0.015 [-0.008, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.173 [-0.250, 0.836], loss: 0.000056, mean_absolute_error: 0.474401, mean_q: 0.715659\n",
      " 44235/50000: episode: 1030, duration: 0.191s, episode steps: 29, steps per second: 152, episode reward: 0.943, mean reward: 0.033 [-0.003, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.049 [-0.170, 0.315], loss: 0.000039, mean_absolute_error: 0.476835, mean_q: 0.719573\n",
      " 44300/50000: episode: 1031, duration: 0.400s, episode steps: 65, steps per second: 163, episode reward: 0.645, mean reward: 0.010 [-0.009, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.209 [-0.927, 0.190], loss: 0.000044, mean_absolute_error: 0.478909, mean_q: 0.723415\n",
      " 44301/50000: episode: 1032, duration: 0.007s, episode steps: 1, steps per second: 143, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.013 [0.000, 0.026], loss: 0.000016, mean_absolute_error: 0.454694, mean_q: 0.685649\n",
      " 44302/50000: episode: 1033, duration: 0.007s, episode steps: 1, steps per second: 142, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.025 [0.000, 0.049], loss: 0.000025, mean_absolute_error: 0.486747, mean_q: 0.733777\n",
      " 44303/50000: episode: 1034, duration: 0.008s, episode steps: 1, steps per second: 133, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [-0.010, 0.038], loss: 0.000019, mean_absolute_error: 0.466039, mean_q: 0.702708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44353/50000: episode: 1035, duration: 0.196s, episode steps: 50, steps per second: 255, episode reward: 0.811, mean reward: 0.016 [-0.007, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.134 [-0.654, 0.200], loss: 0.000078, mean_absolute_error: 0.482861, mean_q: 0.727611\n",
      " 44385/50000: episode: 1036, duration: 0.120s, episode steps: 32, steps per second: 267, episode reward: 0.913, mean reward: 0.029 [-0.004, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.086 [-0.423, 0.150], loss: 0.000036, mean_absolute_error: 0.480567, mean_q: 0.724923\n",
      " 44394/50000: episode: 1037, duration: 0.037s, episode steps: 9, steps per second: 243, episode reward: 0.991, mean reward: 0.110 [-0.001, 1.000], mean action: 0.556 [0.000, 2.000], mean observation: 0.039 [-0.050, 0.128], loss: 0.000034, mean_absolute_error: 0.465384, mean_q: 0.703959\n",
      " 44452/50000: episode: 1038, duration: 0.242s, episode steps: 58, steps per second: 239, episode reward: 0.700, mean reward: 0.012 [-0.009, 1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.190 [-0.895, 0.200], loss: 0.000040, mean_absolute_error: 0.475396, mean_q: 0.717511\n",
      " 44506/50000: episode: 1039, duration: 0.360s, episode steps: 54, steps per second: 150, episode reward: 0.744, mean reward: 0.014 [-0.008, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.170 [-0.822, 0.200], loss: 0.000040, mean_absolute_error: 0.478665, mean_q: 0.722716\n",
      " 44558/50000: episode: 1040, duration: 0.315s, episode steps: 52, steps per second: 165, episode reward: 0.759, mean reward: 0.015 [-0.008, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.157 [-0.240, 0.813], loss: 0.000044, mean_absolute_error: 0.479915, mean_q: 0.724331\n",
      " 44618/50000: episode: 1041, duration: 0.324s, episode steps: 60, steps per second: 185, episode reward: 0.731, mean reward: 0.012 [-0.009, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.148 [-0.220, 0.879], loss: 0.000049, mean_absolute_error: 0.478021, mean_q: 0.721171\n",
      " 44671/50000: episode: 1042, duration: 0.257s, episode steps: 53, steps per second: 206, episode reward: 0.726, mean reward: 0.014 [-0.009, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.184 [-0.240, 0.869], loss: 0.000035, mean_absolute_error: 0.474965, mean_q: 0.717322\n",
      " 44717/50000: episode: 1043, duration: 0.223s, episode steps: 46, steps per second: 206, episode reward: 0.849, mean reward: 0.018 [-0.006, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.113 [-0.575, 0.170], loss: 0.000044, mean_absolute_error: 0.480356, mean_q: 0.724749\n",
      " 44741/50000: episode: 1044, duration: 0.100s, episode steps: 24, steps per second: 240, episode reward: 0.952, mean reward: 0.040 [-0.003, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.053 [-0.150, 0.299], loss: 0.000046, mean_absolute_error: 0.474860, mean_q: 0.715894\n",
      " 44777/50000: episode: 1045, duration: 0.151s, episode steps: 36, steps per second: 238, episode reward: 0.930, mean reward: 0.026 [-0.003, 1.000], mean action: 0.944 [0.000, 2.000], mean observation: 0.070 [-0.130, 0.308], loss: 0.000043, mean_absolute_error: 0.479463, mean_q: 0.724108\n",
      " 44804/50000: episode: 1046, duration: 0.124s, episode steps: 27, steps per second: 217, episode reward: 0.942, mean reward: 0.035 [-0.003, 1.000], mean action: 1.185 [0.000, 2.000], mean observation: -0.067 [-0.318, 0.140], loss: 0.000047, mean_absolute_error: 0.478070, mean_q: 0.722350\n",
      " 44809/50000: episode: 1047, duration: 0.023s, episode steps: 5, steps per second: 218, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.039 [-0.040, 0.109], loss: 0.000039, mean_absolute_error: 0.480496, mean_q: 0.724697\n",
      " 44870/50000: episode: 1048, duration: 0.267s, episode steps: 61, steps per second: 228, episode reward: 0.672, mean reward: 0.011 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.202 [-0.894, 0.200], loss: 0.000042, mean_absolute_error: 0.480305, mean_q: 0.725119\n",
      " 44927/50000: episode: 1049, duration: 0.295s, episode steps: 57, steps per second: 193, episode reward: 0.744, mean reward: 0.013 [-0.008, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.154 [-0.220, 0.802], loss: 0.000034, mean_absolute_error: 0.476720, mean_q: 0.719845\n",
      " 44947/50000: episode: 1050, duration: 0.085s, episode steps: 20, steps per second: 236, episode reward: 0.967, mean reward: 0.048 [-0.002, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.051 [-0.228, 0.100], loss: 0.000077, mean_absolute_error: 0.481097, mean_q: 0.724558\n",
      " 44985/50000: episode: 1051, duration: 0.127s, episode steps: 38, steps per second: 299, episode reward: 0.883, mean reward: 0.023 [-0.005, 1.000], mean action: 0.895 [0.000, 2.000], mean observation: 0.101 [-0.170, 0.503], loss: 0.000045, mean_absolute_error: 0.471028, mean_q: 0.711449\n",
      " 45037/50000: episode: 1052, duration: 0.171s, episode steps: 52, steps per second: 304, episode reward: 0.777, mean reward: 0.015 [-0.007, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.147 [-0.210, 0.740], loss: 0.000046, mean_absolute_error: 0.485382, mean_q: 0.732231\n",
      " 45053/50000: episode: 1053, duration: 0.050s, episode steps: 16, steps per second: 319, episode reward: 0.976, mean reward: 0.061 [-0.002, 1.000], mean action: 0.562 [0.000, 2.000], mean observation: 0.045 [-0.090, 0.196], loss: 0.000052, mean_absolute_error: 0.472318, mean_q: 0.711985\n",
      " 45123/50000: episode: 1054, duration: 0.227s, episode steps: 70, steps per second: 309, episode reward: 0.641, mean reward: 0.009 [-0.010, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.188 [-0.968, 0.190], loss: 0.000049, mean_absolute_error: 0.480852, mean_q: 0.725722\n",
      " 45155/50000: episode: 1055, duration: 0.099s, episode steps: 32, steps per second: 324, episode reward: 0.907, mean reward: 0.028 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.090 [-0.190, 0.452], loss: 0.000037, mean_absolute_error: 0.476188, mean_q: 0.718968\n",
      " 45181/50000: episode: 1056, duration: 0.089s, episode steps: 26, steps per second: 291, episode reward: 0.950, mean reward: 0.037 [-0.003, 1.000], mean action: 0.808 [0.000, 2.000], mean observation: 0.062 [-0.130, 0.288], loss: 0.000034, mean_absolute_error: 0.484298, mean_q: 0.731728\n",
      " 45223/50000: episode: 1057, duration: 0.200s, episode steps: 42, steps per second: 210, episode reward: 0.847, mean reward: 0.020 [-0.006, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.120 [-0.608, 0.180], loss: 0.000050, mean_absolute_error: 0.477916, mean_q: 0.721465\n",
      " 45284/50000: episode: 1058, duration: 0.277s, episode steps: 61, steps per second: 220, episode reward: 0.678, mean reward: 0.011 [-0.009, 1.000], mean action: 1.098 [0.000, 2.000], mean observation: -0.201 [-0.876, 0.210], loss: 0.000052, mean_absolute_error: 0.478261, mean_q: 0.722084\n",
      " 45326/50000: episode: 1059, duration: 0.173s, episode steps: 42, steps per second: 243, episode reward: 0.852, mean reward: 0.020 [-0.006, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.120 [-0.563, 0.160], loss: 0.000051, mean_absolute_error: 0.471219, mean_q: 0.711970\n",
      " 45364/50000: episode: 1060, duration: 0.131s, episode steps: 38, steps per second: 290, episode reward: 0.885, mean reward: 0.023 [-0.005, 1.000], mean action: 0.895 [0.000, 2.000], mean observation: 0.101 [-0.170, 0.493], loss: 0.000055, mean_absolute_error: 0.484280, mean_q: 0.730908\n",
      " 45400/50000: episode: 1061, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 0.899, mean reward: 0.025 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.096 [-0.417, 0.150], loss: 0.000041, mean_absolute_error: 0.471807, mean_q: 0.712878\n",
      " 45449/50000: episode: 1062, duration: 0.255s, episode steps: 49, steps per second: 192, episode reward: 0.775, mean reward: 0.016 [-0.008, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.162 [-0.761, 0.200], loss: 0.000044, mean_absolute_error: 0.476749, mean_q: 0.719905\n",
      " 45462/50000: episode: 1063, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 0.984, mean reward: 0.076 [-0.002, 1.000], mean action: 1.692 [0.000, 2.000], mean observation: -0.042 [-0.155, 0.090], loss: 0.000039, mean_absolute_error: 0.489919, mean_q: 0.739185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45493/50000: episode: 1064, duration: 0.200s, episode steps: 31, steps per second: 155, episode reward: 0.916, mean reward: 0.030 [-0.004, 1.000], mean action: 1.258 [0.000, 2.000], mean observation: -0.084 [-0.420, 0.170], loss: 0.000033, mean_absolute_error: 0.477458, mean_q: 0.721560\n",
      " 45537/50000: episode: 1065, duration: 0.339s, episode steps: 44, steps per second: 130, episode reward: 0.836, mean reward: 0.019 [-0.006, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.128 [-0.621, 0.180], loss: 0.000035, mean_absolute_error: 0.484269, mean_q: 0.730989\n",
      " 45572/50000: episode: 1066, duration: 0.227s, episode steps: 35, steps per second: 154, episode reward: 0.920, mean reward: 0.026 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.068 [-0.374, 0.130], loss: 0.000027, mean_absolute_error: 0.467443, mean_q: 0.706883\n",
      " 45576/50000: episode: 1067, duration: 0.044s, episode steps: 4, steps per second: 92, episode reward: 0.997, mean reward: 0.249 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.106, 0.040], loss: 0.000015, mean_absolute_error: 0.491692, mean_q: 0.744513\n",
      " 45577/50000: episode: 1068, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.082, 0.010], loss: 0.000109, mean_absolute_error: 0.522154, mean_q: 0.785646\n",
      " 45603/50000: episode: 1069, duration: 0.246s, episode steps: 26, steps per second: 106, episode reward: 0.943, mean reward: 0.036 [-0.003, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.068 [-0.140, 0.322], loss: 0.000056, mean_absolute_error: 0.482419, mean_q: 0.727894\n",
      " 45628/50000: episode: 1070, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 0.961, mean reward: 0.038 [-0.002, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.055 [-0.090, 0.228], loss: 0.000055, mean_absolute_error: 0.474681, mean_q: 0.717039\n",
      " 45669/50000: episode: 1071, duration: 0.319s, episode steps: 41, steps per second: 129, episode reward: 0.858, mean reward: 0.021 [-0.006, 1.000], mean action: 0.805 [0.000, 2.000], mean observation: 0.117 [-0.180, 0.566], loss: 0.000047, mean_absolute_error: 0.481908, mean_q: 0.727125\n",
      " 45670/50000: episode: 1072, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [-0.010, 0.099], loss: 0.000014, mean_absolute_error: 0.497518, mean_q: 0.749835\n",
      " 45714/50000: episode: 1073, duration: 0.275s, episode steps: 44, steps per second: 160, episode reward: 0.846, mean reward: 0.019 [-0.006, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.120 [-0.589, 0.190], loss: 0.000049, mean_absolute_error: 0.476058, mean_q: 0.719002\n",
      " 45759/50000: episode: 1074, duration: 0.196s, episode steps: 45, steps per second: 230, episode reward: 0.822, mean reward: 0.018 [-0.007, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.133 [-0.669, 0.200], loss: 0.000036, mean_absolute_error: 0.479799, mean_q: 0.724351\n",
      " 45775/50000: episode: 1075, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 0.982, mean reward: 0.061 [-0.001, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.044 [-0.060, 0.143], loss: 0.000054, mean_absolute_error: 0.487597, mean_q: 0.736435\n",
      " 45820/50000: episode: 1076, duration: 0.153s, episode steps: 45, steps per second: 293, episode reward: 0.825, mean reward: 0.018 [-0.006, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.136 [-0.180, 0.635], loss: 0.000049, mean_absolute_error: 0.479724, mean_q: 0.724235\n",
      " 45831/50000: episode: 1077, duration: 0.033s, episode steps: 11, steps per second: 331, episode reward: 0.987, mean reward: 0.090 [-0.001, 1.000], mean action: 1.818 [0.000, 2.000], mean observation: -0.037 [-0.147, 0.090], loss: 0.000053, mean_absolute_error: 0.463534, mean_q: 0.701534\n",
      " 45850/50000: episode: 1078, duration: 0.058s, episode steps: 19, steps per second: 326, episode reward: 0.970, mean reward: 0.051 [-0.002, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.046 [-0.211, 0.130], loss: 0.000024, mean_absolute_error: 0.480349, mean_q: 0.725559\n",
      " 45864/50000: episode: 1079, duration: 0.043s, episode steps: 14, steps per second: 327, episode reward: 0.981, mean reward: 0.070 [-0.002, 1.000], mean action: 1.571 [0.000, 2.000], mean observation: -0.040 [-0.179, 0.100], loss: 0.000133, mean_absolute_error: 0.495300, mean_q: 0.748189\n",
      " 45885/50000: episode: 1080, duration: 0.067s, episode steps: 21, steps per second: 312, episode reward: 0.969, mean reward: 0.046 [-0.002, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.051 [-0.191, 0.080], loss: 0.000084, mean_absolute_error: 0.484473, mean_q: 0.730138\n",
      " 45900/50000: episode: 1081, duration: 0.055s, episode steps: 15, steps per second: 275, episode reward: 0.977, mean reward: 0.065 [-0.002, 1.000], mean action: 1.533 [0.000, 2.000], mean observation: -0.046 [-0.203, 0.110], loss: 0.000033, mean_absolute_error: 0.495615, mean_q: 0.747950\n",
      " 45944/50000: episode: 1082, duration: 0.136s, episode steps: 44, steps per second: 324, episode reward: 0.840, mean reward: 0.019 [-0.006, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.125 [-0.170, 0.603], loss: 0.000039, mean_absolute_error: 0.473880, mean_q: 0.716356\n",
      " 45965/50000: episode: 1083, duration: 0.085s, episode steps: 21, steps per second: 246, episode reward: 0.963, mean reward: 0.046 [-0.002, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: -0.055 [-0.248, 0.110], loss: 0.000030, mean_absolute_error: 0.487888, mean_q: 0.736639\n",
      " 45983/50000: episode: 1084, duration: 0.081s, episode steps: 18, steps per second: 221, episode reward: 0.972, mean reward: 0.054 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.044 [-0.218, 0.100], loss: 0.000044, mean_absolute_error: 0.487541, mean_q: 0.735819\n",
      " 45984/50000: episode: 1085, duration: 0.008s, episode steps: 1, steps per second: 130, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.041 [0.010, 0.073], loss: 0.000015, mean_absolute_error: 0.513048, mean_q: 0.774904\n",
      " 45985/50000: episode: 1086, duration: 0.008s, episode steps: 1, steps per second: 121, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.028 [0.010, 0.046], loss: 0.000036, mean_absolute_error: 0.474334, mean_q: 0.717294\n",
      " 46034/50000: episode: 1087, duration: 0.154s, episode steps: 49, steps per second: 318, episode reward: 0.797, mean reward: 0.016 [-0.007, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.147 [-0.690, 0.190], loss: 0.000033, mean_absolute_error: 0.479211, mean_q: 0.724281\n",
      " 46096/50000: episode: 1088, duration: 0.184s, episode steps: 62, steps per second: 337, episode reward: 0.683, mean reward: 0.011 [-0.009, 1.000], mean action: 0.952 [0.000, 2.000], mean observation: 0.190 [-0.210, 0.922], loss: 0.000049, mean_absolute_error: 0.479250, mean_q: 0.723750\n",
      " 46159/50000: episode: 1089, duration: 0.180s, episode steps: 63, steps per second: 351, episode reward: 0.702, mean reward: 0.011 [-0.009, 1.000], mean action: 0.937 [0.000, 2.000], mean observation: 0.177 [-0.200, 0.859], loss: 0.000040, mean_absolute_error: 0.478822, mean_q: 0.723871\n",
      " 46191/50000: episode: 1090, duration: 0.093s, episode steps: 32, steps per second: 344, episode reward: 0.926, mean reward: 0.029 [-0.004, 1.000], mean action: 0.781 [0.000, 2.000], mean observation: 0.077 [-0.140, 0.352], loss: 0.000034, mean_absolute_error: 0.482669, mean_q: 0.728995\n",
      " 46250/50000: episode: 1091, duration: 0.185s, episode steps: 59, steps per second: 318, episode reward: 0.693, mean reward: 0.012 [-0.009, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.195 [-0.880, 0.200], loss: 0.000034, mean_absolute_error: 0.472612, mean_q: 0.714324\n",
      " 46306/50000: episode: 1092, duration: 0.162s, episode steps: 56, steps per second: 345, episode reward: 0.729, mean reward: 0.013 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.172 [-0.858, 0.220], loss: 0.000036, mean_absolute_error: 0.482550, mean_q: 0.729143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46370/50000: episode: 1093, duration: 0.201s, episode steps: 64, steps per second: 318, episode reward: 0.650, mean reward: 0.010 [-0.010, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.205 [-0.989, 0.210], loss: 0.000049, mean_absolute_error: 0.488315, mean_q: 0.737014\n",
      " 46433/50000: episode: 1094, duration: 0.267s, episode steps: 63, steps per second: 236, episode reward: 0.748, mean reward: 0.012 [-0.008, 1.000], mean action: 0.937 [0.000, 2.000], mean observation: 0.147 [-0.180, 0.768], loss: 0.000035, mean_absolute_error: 0.478225, mean_q: 0.722763\n",
      " 46470/50000: episode: 1095, duration: 0.111s, episode steps: 37, steps per second: 333, episode reward: 0.895, mean reward: 0.024 [-0.005, 1.000], mean action: 1.216 [0.000, 2.000], mean observation: -0.095 [-0.456, 0.160], loss: 0.000028, mean_absolute_error: 0.473056, mean_q: 0.715335\n",
      " 46493/50000: episode: 1096, duration: 0.066s, episode steps: 23, steps per second: 348, episode reward: 0.956, mean reward: 0.042 [-0.003, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.059 [-0.110, 0.271], loss: 0.000034, mean_absolute_error: 0.489735, mean_q: 0.739742\n",
      " 46544/50000: episode: 1097, duration: 0.148s, episode steps: 51, steps per second: 344, episode reward: 0.780, mean reward: 0.015 [-0.007, 1.000], mean action: 1.098 [0.000, 2.000], mean observation: -0.153 [-0.739, 0.190], loss: 0.000039, mean_absolute_error: 0.477408, mean_q: 0.721037\n",
      " 46587/50000: episode: 1098, duration: 0.129s, episode steps: 43, steps per second: 332, episode reward: 0.879, mean reward: 0.020 [-0.005, 1.000], mean action: 0.884 [0.000, 2.000], mean observation: 0.096 [-0.170, 0.489], loss: 0.000036, mean_absolute_error: 0.475390, mean_q: 0.719036\n",
      " 46601/50000: episode: 1099, duration: 0.045s, episode steps: 14, steps per second: 313, episode reward: 0.980, mean reward: 0.070 [-0.002, 1.000], mean action: 1.571 [0.000, 2.000], mean observation: -0.043 [-0.191, 0.110], loss: 0.000054, mean_absolute_error: 0.487245, mean_q: 0.736033\n",
      " 46649/50000: episode: 1100, duration: 0.153s, episode steps: 48, steps per second: 313, episode reward: 0.847, mean reward: 0.018 [-0.005, 1.000], mean action: 0.938 [0.000, 2.000], mean observation: 0.114 [-0.170, 0.550], loss: 0.000035, mean_absolute_error: 0.479192, mean_q: 0.723925\n",
      " 46697/50000: episode: 1101, duration: 0.139s, episode steps: 48, steps per second: 346, episode reward: 0.793, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.149 [-0.728, 0.210], loss: 0.000035, mean_absolute_error: 0.479610, mean_q: 0.724797\n",
      " 46746/50000: episode: 1102, duration: 0.205s, episode steps: 49, steps per second: 239, episode reward: 0.822, mean reward: 0.017 [-0.006, 1.000], mean action: 0.939 [0.000, 2.000], mean observation: 0.127 [-0.190, 0.640], loss: 0.000049, mean_absolute_error: 0.474784, mean_q: 0.717064\n",
      " 46747/50000: episode: 1103, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.013 [0.010, 0.016], loss: 0.000108, mean_absolute_error: 0.452414, mean_q: 0.682948\n",
      " 46799/50000: episode: 1104, duration: 0.175s, episode steps: 52, steps per second: 296, episode reward: 0.733, mean reward: 0.014 [-0.009, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.183 [-0.876, 0.230], loss: 0.000050, mean_absolute_error: 0.481092, mean_q: 0.725912\n",
      " 46827/50000: episode: 1105, duration: 0.094s, episode steps: 28, steps per second: 297, episode reward: 0.935, mean reward: 0.033 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.075 [-0.327, 0.140], loss: 0.000034, mean_absolute_error: 0.484877, mean_q: 0.732833\n",
      " 46863/50000: episode: 1106, duration: 0.105s, episode steps: 36, steps per second: 344, episode reward: 0.883, mean reward: 0.025 [-0.005, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.105 [-0.516, 0.170], loss: 0.000037, mean_absolute_error: 0.483046, mean_q: 0.729525\n",
      " 46871/50000: episode: 1107, duration: 0.025s, episode steps: 8, steps per second: 315, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.040 [-0.050, 0.127], loss: 0.000025, mean_absolute_error: 0.467987, mean_q: 0.709202\n",
      " 46889/50000: episode: 1108, duration: 0.052s, episode steps: 18, steps per second: 346, episode reward: 0.974, mean reward: 0.054 [-0.002, 1.000], mean action: 1.278 [0.000, 2.000], mean observation: -0.049 [-0.188, 0.070], loss: 0.000053, mean_absolute_error: 0.488384, mean_q: 0.737329\n",
      " 46938/50000: episode: 1109, duration: 0.144s, episode steps: 49, steps per second: 340, episode reward: 0.812, mean reward: 0.017 [-0.007, 1.000], mean action: 1.122 [0.000, 2.000], mean observation: -0.136 [-0.650, 0.200], loss: 0.000031, mean_absolute_error: 0.484545, mean_q: 0.732177\n",
      " 47005/50000: episode: 1110, duration: 0.278s, episode steps: 67, steps per second: 241, episode reward: 0.665, mean reward: 0.010 [-0.009, 1.000], mean action: 0.925 [0.000, 2.000], mean observation: 0.189 [-0.200, 0.927], loss: 0.000040, mean_absolute_error: 0.478610, mean_q: 0.723601\n",
      " 47051/50000: episode: 1111, duration: 0.222s, episode steps: 46, steps per second: 208, episode reward: 0.880, mean reward: 0.019 [-0.005, 1.000], mean action: 0.957 [0.000, 2.000], mean observation: 0.092 [-0.150, 0.461], loss: 0.000044, mean_absolute_error: 0.483281, mean_q: 0.730408\n",
      " 47102/50000: episode: 1112, duration: 0.244s, episode steps: 51, steps per second: 209, episode reward: 0.846, mean reward: 0.017 [-0.005, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.108 [-0.150, 0.543], loss: 0.000052, mean_absolute_error: 0.485274, mean_q: 0.733088\n",
      " 47161/50000: episode: 1113, duration: 0.243s, episode steps: 59, steps per second: 243, episode reward: 0.696, mean reward: 0.012 [-0.009, 1.000], mean action: 1.102 [0.000, 2.000], mean observation: -0.190 [-0.904, 0.230], loss: 0.000044, mean_absolute_error: 0.486545, mean_q: 0.734885\n",
      " 47212/50000: episode: 1114, duration: 0.164s, episode steps: 51, steps per second: 310, episode reward: 0.806, mean reward: 0.016 [-0.006, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.139 [-0.160, 0.625], loss: 0.000056, mean_absolute_error: 0.483784, mean_q: 0.730348\n",
      " 47255/50000: episode: 1115, duration: 0.130s, episode steps: 43, steps per second: 332, episode reward: 0.877, mean reward: 0.020 [-0.005, 1.000], mean action: 0.930 [0.000, 2.000], mean observation: 0.099 [-0.160, 0.488], loss: 0.000039, mean_absolute_error: 0.481523, mean_q: 0.727590\n",
      " 47297/50000: episode: 1116, duration: 0.179s, episode steps: 42, steps per second: 235, episode reward: 0.876, mean reward: 0.021 [-0.005, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.101 [-0.160, 0.498], loss: 0.000070, mean_absolute_error: 0.482219, mean_q: 0.729186\n",
      " 47358/50000: episode: 1117, duration: 0.214s, episode steps: 61, steps per second: 284, episode reward: 0.683, mean reward: 0.011 [-0.009, 1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.192 [-0.940, 0.250], loss: 0.000053, mean_absolute_error: 0.485980, mean_q: 0.734129\n",
      " 47406/50000: episode: 1118, duration: 0.221s, episode steps: 48, steps per second: 217, episode reward: 0.794, mean reward: 0.017 [-0.007, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.149 [-0.731, 0.220], loss: 0.000048, mean_absolute_error: 0.478492, mean_q: 0.723517\n",
      " 47407/50000: episode: 1119, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [-0.010, 0.087], loss: 0.000034, mean_absolute_error: 0.530014, mean_q: 0.799462\n",
      " 47433/50000: episode: 1120, duration: 0.124s, episode steps: 26, steps per second: 210, episode reward: 0.947, mean reward: 0.036 [-0.003, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.064 [-0.308, 0.120], loss: 0.000036, mean_absolute_error: 0.482483, mean_q: 0.729080\n",
      " 47434/50000: episode: 1121, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [-0.010, 0.066], loss: 0.000063, mean_absolute_error: 0.496805, mean_q: 0.749907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47472/50000: episode: 1122, duration: 0.168s, episode steps: 38, steps per second: 227, episode reward: 0.901, mean reward: 0.024 [-0.004, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.090 [-0.140, 0.413], loss: 0.000039, mean_absolute_error: 0.481655, mean_q: 0.727918\n",
      " 47501/50000: episode: 1123, duration: 0.098s, episode steps: 29, steps per second: 296, episode reward: 0.940, mean reward: 0.032 [-0.003, 1.000], mean action: 1.172 [0.000, 2.000], mean observation: -0.068 [-0.313, 0.110], loss: 0.000041, mean_absolute_error: 0.477695, mean_q: 0.721081\n",
      " 47567/50000: episode: 1124, duration: 0.212s, episode steps: 66, steps per second: 311, episode reward: 0.684, mean reward: 0.010 [-0.009, 1.000], mean action: 0.939 [0.000, 2.000], mean observation: 0.180 [-0.190, 0.890], loss: 0.000035, mean_absolute_error: 0.483008, mean_q: 0.730413\n",
      " 47606/50000: episode: 1125, duration: 0.115s, episode steps: 39, steps per second: 338, episode reward: 0.888, mean reward: 0.023 [-0.005, 1.000], mean action: 0.897 [0.000, 2.000], mean observation: 0.096 [-0.160, 0.483], loss: 0.000060, mean_absolute_error: 0.480970, mean_q: 0.725971\n",
      " 47647/50000: episode: 1126, duration: 0.120s, episode steps: 41, steps per second: 341, episode reward: 0.896, mean reward: 0.022 [-0.004, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.089 [-0.150, 0.418], loss: 0.000038, mean_absolute_error: 0.479120, mean_q: 0.723130\n",
      " 47678/50000: episode: 1127, duration: 0.091s, episode steps: 31, steps per second: 342, episode reward: 0.922, mean reward: 0.030 [-0.004, 1.000], mean action: 0.742 [0.000, 2.000], mean observation: 0.081 [-0.130, 0.388], loss: 0.000033, mean_absolute_error: 0.476734, mean_q: 0.720400\n",
      " 47703/50000: episode: 1128, duration: 0.074s, episode steps: 25, steps per second: 340, episode reward: 0.959, mean reward: 0.038 [-0.002, 1.000], mean action: 1.280 [0.000, 2.000], mean observation: -0.061 [-0.214, 0.080], loss: 0.000045, mean_absolute_error: 0.482430, mean_q: 0.728865\n",
      " 47741/50000: episode: 1129, duration: 0.136s, episode steps: 38, steps per second: 279, episode reward: 0.908, mean reward: 0.024 [-0.004, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.083 [-0.360, 0.120], loss: 0.000107, mean_absolute_error: 0.479184, mean_q: 0.722785\n",
      " 47812/50000: episode: 1130, duration: 0.292s, episode steps: 71, steps per second: 243, episode reward: 0.640, mean reward: 0.009 [-0.010, 1.000], mean action: 0.944 [0.000, 2.000], mean observation: 0.191 [-0.240, 0.997], loss: 0.000052, mean_absolute_error: 0.484685, mean_q: 0.731690\n",
      " 47869/50000: episode: 1131, duration: 0.208s, episode steps: 57, steps per second: 274, episode reward: 0.726, mean reward: 0.013 [-0.009, 1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.173 [-0.871, 0.240], loss: 0.000037, mean_absolute_error: 0.480355, mean_q: 0.725293\n",
      " 47916/50000: episode: 1132, duration: 0.137s, episode steps: 47, steps per second: 342, episode reward: 0.825, mean reward: 0.018 [-0.006, 1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.131 [-0.631, 0.180], loss: 0.000033, mean_absolute_error: 0.484477, mean_q: 0.731315\n",
      " 47945/50000: episode: 1133, duration: 0.087s, episode steps: 29, steps per second: 334, episode reward: 0.938, mean reward: 0.032 [-0.003, 1.000], mean action: 1.241 [0.000, 2.000], mean observation: -0.069 [-0.325, 0.130], loss: 0.000042, mean_absolute_error: 0.480573, mean_q: 0.725469\n",
      " 48015/50000: episode: 1134, duration: 0.205s, episode steps: 70, steps per second: 341, episode reward: 0.647, mean reward: 0.009 [-0.010, 1.000], mean action: 0.929 [0.000, 2.000], mean observation: 0.190 [-0.240, 0.976], loss: 0.000058, mean_absolute_error: 0.486068, mean_q: 0.732964\n",
      " 48043/50000: episode: 1135, duration: 0.109s, episode steps: 28, steps per second: 256, episode reward: 0.948, mean reward: 0.034 [-0.003, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.061 [-0.100, 0.284], loss: 0.000043, mean_absolute_error: 0.489811, mean_q: 0.738432\n",
      " 48044/50000: episode: 1136, duration: 0.008s, episode steps: 1, steps per second: 131, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [-0.010, 0.088], loss: 0.000169, mean_absolute_error: 0.526376, mean_q: 0.794102\n",
      " 48088/50000: episode: 1137, duration: 0.147s, episode steps: 44, steps per second: 299, episode reward: 0.863, mean reward: 0.020 [-0.005, 1.000], mean action: 1.091 [0.000, 2.000], mean observation: -0.108 [-0.531, 0.170], loss: 0.000040, mean_absolute_error: 0.482747, mean_q: 0.727904\n",
      " 48127/50000: episode: 1138, duration: 0.117s, episode steps: 39, steps per second: 333, episode reward: 0.892, mean reward: 0.023 [-0.004, 1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.097 [-0.429, 0.130], loss: 0.000029, mean_absolute_error: 0.480672, mean_q: 0.725222\n",
      " 48168/50000: episode: 1139, duration: 0.176s, episode steps: 41, steps per second: 232, episode reward: 0.881, mean reward: 0.021 [-0.005, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.100 [-0.130, 0.476], loss: 0.000036, mean_absolute_error: 0.480987, mean_q: 0.725047\n",
      " 48225/50000: episode: 1140, duration: 0.228s, episode steps: 57, steps per second: 250, episode reward: 0.726, mean reward: 0.013 [-0.009, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.174 [-0.230, 0.869], loss: 0.000050, mean_absolute_error: 0.484614, mean_q: 0.730451\n",
      " 48279/50000: episode: 1141, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 0.715, mean reward: 0.013 [-0.009, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.190 [-0.230, 0.914], loss: 0.000040, mean_absolute_error: 0.485346, mean_q: 0.731569\n",
      " 48339/50000: episode: 1142, duration: 0.260s, episode steps: 60, steps per second: 231, episode reward: 0.779, mean reward: 0.013 [-0.007, 1.000], mean action: 0.917 [0.000, 2.000], mean observation: 0.137 [-0.170, 0.669], loss: 0.000039, mean_absolute_error: 0.483832, mean_q: 0.728751\n",
      " 48378/50000: episode: 1143, duration: 0.228s, episode steps: 39, steps per second: 171, episode reward: 0.904, mean reward: 0.023 [-0.004, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.084 [-0.140, 0.404], loss: 0.000043, mean_absolute_error: 0.477359, mean_q: 0.717988\n",
      " 48379/50000: episode: 1144, duration: 0.007s, episode steps: 1, steps per second: 133, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.008 [-0.016, 0.000], loss: 0.000011, mean_absolute_error: 0.476378, mean_q: 0.717727\n",
      " 48380/50000: episode: 1145, duration: 0.008s, episode steps: 1, steps per second: 125, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.007 [-0.010, 0.025], loss: 0.000079, mean_absolute_error: 0.451454, mean_q: 0.671794\n",
      " 48441/50000: episode: 1146, duration: 0.198s, episode steps: 61, steps per second: 308, episode reward: 0.706, mean reward: 0.012 [-0.009, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.178 [-0.240, 0.869], loss: 0.000067, mean_absolute_error: 0.483733, mean_q: 0.727721\n",
      " 48483/50000: episode: 1147, duration: 0.136s, episode steps: 42, steps per second: 308, episode reward: 0.846, mean reward: 0.020 [-0.006, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.130 [-0.180, 0.557], loss: 0.000031, mean_absolute_error: 0.486959, mean_q: 0.732067\n",
      " 48484/50000: episode: 1148, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [-0.010, 0.061], loss: 0.000037, mean_absolute_error: 0.505131, mean_q: 0.763104\n",
      " 48534/50000: episode: 1149, duration: 0.167s, episode steps: 50, steps per second: 299, episode reward: 0.788, mean reward: 0.016 [-0.007, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.149 [-0.733, 0.210], loss: 0.000055, mean_absolute_error: 0.485550, mean_q: 0.730129\n",
      " 48576/50000: episode: 1150, duration: 0.140s, episode steps: 42, steps per second: 300, episode reward: 0.879, mean reward: 0.021 [-0.005, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.098 [-0.180, 0.493], loss: 0.000060, mean_absolute_error: 0.490628, mean_q: 0.737798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48623/50000: episode: 1151, duration: 0.184s, episode steps: 47, steps per second: 256, episode reward: 0.828, mean reward: 0.018 [-0.006, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.130 [-0.608, 0.190], loss: 0.000048, mean_absolute_error: 0.479131, mean_q: 0.720473\n",
      " 48681/50000: episode: 1152, duration: 0.226s, episode steps: 58, steps per second: 256, episode reward: 0.745, mean reward: 0.013 [-0.008, 1.000], mean action: 1.086 [0.000, 2.000], mean observation: -0.159 [-0.818, 0.220], loss: 0.000039, mean_absolute_error: 0.484180, mean_q: 0.728266\n",
      " 48750/50000: episode: 1153, duration: 0.237s, episode steps: 69, steps per second: 291, episode reward: 0.644, mean reward: 0.009 [-0.009, 1.000], mean action: 1.087 [0.000, 2.000], mean observation: -0.199 [-0.930, 0.230], loss: 0.000048, mean_absolute_error: 0.477645, mean_q: 0.717595\n",
      " 48804/50000: episode: 1154, duration: 0.157s, episode steps: 54, steps per second: 345, episode reward: 0.766, mean reward: 0.014 [-0.007, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.158 [-0.739, 0.200], loss: 0.000051, mean_absolute_error: 0.483354, mean_q: 0.727102\n",
      " 48847/50000: episode: 1155, duration: 0.136s, episode steps: 43, steps per second: 317, episode reward: 0.850, mean reward: 0.020 [-0.006, 1.000], mean action: 0.884 [0.000, 2.000], mean observation: 0.119 [-0.190, 0.580], loss: 0.000036, mean_absolute_error: 0.475334, mean_q: 0.714585\n",
      " 49147/50000: episode: 1156, duration: 1.042s, episode steps: 300, steps per second: 288, episode reward: -2.693, mean reward: -0.009 [-0.010, -0.000], mean action: 0.103 [0.000, 2.000], mean observation: -0.779 [-1.000, 0.815], loss: 0.000045, mean_absolute_error: 0.483453, mean_q: 0.726866\n",
      " 49205/50000: episode: 1157, duration: 0.170s, episode steps: 58, steps per second: 341, episode reward: 0.763, mean reward: 0.013 [-0.007, 1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.149 [-0.746, 0.200], loss: 0.000047, mean_absolute_error: 0.475179, mean_q: 0.713195\n",
      " 49264/50000: episode: 1158, duration: 0.214s, episode steps: 59, steps per second: 276, episode reward: 0.738, mean reward: 0.013 [-0.008, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.162 [-0.810, 0.200], loss: 0.000051, mean_absolute_error: 0.481801, mean_q: 0.723218\n",
      " 49319/50000: episode: 1159, duration: 0.174s, episode steps: 55, steps per second: 317, episode reward: 0.713, mean reward: 0.013 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.188 [-0.911, 0.220], loss: 0.000044, mean_absolute_error: 0.485157, mean_q: 0.728084\n",
      " 49341/50000: episode: 1160, duration: 0.083s, episode steps: 22, steps per second: 265, episode reward: 0.964, mean reward: 0.044 [-0.002, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.055 [-0.080, 0.227], loss: 0.000052, mean_absolute_error: 0.481613, mean_q: 0.721201\n",
      " 49374/50000: episode: 1161, duration: 0.121s, episode steps: 33, steps per second: 274, episode reward: 0.935, mean reward: 0.028 [-0.003, 1.000], mean action: 1.182 [0.000, 2.000], mean observation: -0.073 [-0.272, 0.090], loss: 0.000050, mean_absolute_error: 0.481620, mean_q: 0.723293\n",
      " 49413/50000: episode: 1162, duration: 0.131s, episode steps: 39, steps per second: 298, episode reward: 0.923, mean reward: 0.024 [-0.004, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.034 [-0.180, 0.385], loss: 0.000042, mean_absolute_error: 0.479461, mean_q: 0.718813\n",
      " 49414/50000: episode: 1163, duration: 0.006s, episode steps: 1, steps per second: 171, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.000 [-0.009, 0.010], loss: 0.000036, mean_absolute_error: 0.475067, mean_q: 0.718772\n",
      " 49714/50000: episode: 1164, duration: 0.960s, episode steps: 300, steps per second: 313, episode reward: -2.659, mean reward: -0.009 [-0.010, -0.000], mean action: 0.090 [0.000, 2.000], mean observation: -0.795 [-1.000, 0.652], loss: 0.000074, mean_absolute_error: 0.486024, mean_q: 0.730014\n",
      " 49884/50000: episode: 1165, duration: 0.506s, episode steps: 170, steps per second: 336, episode reward: -0.000, mean reward: -0.000 [-0.010, 1.000], mean action: 1.047 [0.000, 2.000], mean observation: -0.236 [-1.000, 0.568], loss: 0.000083, mean_absolute_error: 0.481013, mean_q: 0.721154\n",
      " 49913/50000: episode: 1166, duration: 0.083s, episode steps: 29, steps per second: 349, episode reward: 0.941, mean reward: 0.032 [-0.003, 1.000], mean action: 1.276 [0.000, 2.000], mean observation: -0.071 [-0.283, 0.110], loss: 0.000050, mean_absolute_error: 0.481954, mean_q: 0.723148\n",
      "done, took 201.875 seconds\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "env = PointOnLine()\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# DQNのネットワーク定義\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# experience replay用のmemory\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "# 行動方策はオーソドックスなepsilon-greedy。ほかに、各行動のQ値によって確率を決定するBoltzmannQPolicyが利用可能\n",
    "policy = EpsGreedyQPolicy(eps=0.1) \n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2, nb_max_episode_steps=300)\n",
    "#学習の様子を描画したいときは、Envに_render()を実装して、visualize=True にします,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 0.967, steps: 20\n",
      "Episode 2: reward: 0.870, steps: 42\n",
      "Episode 3: reward: 0.042, steps: 158\n",
      "Episode 4: reward: 0.988, steps: 10\n",
      "Episode 5: reward: -0.249, steps: 180\n",
      "Episode 6: reward: -0.030, steps: 164\n",
      "Episode 7: reward: 1.000, steps: 1\n",
      "Episode 8: reward: -0.273, steps: 182\n",
      "Episode 9: reward: 0.185, steps: 146\n",
      "Episode 10: reward: 1.000, steps: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x170a8793c18>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x170a6a60e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import rl.callbacks\n",
    "class EpisodeLogger(rl.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "\n",
    "cb_ep = EpisodeLogger()\n",
    "dqn.test(env, nb_episodes=10, visualize=False, callbacks=[cb_ep])\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for obs in cb_ep.observations.values():\n",
    "    plt.plot([o[0] for o in obs])\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cralybot",
   "language": "python",
   "name": "cralybot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
