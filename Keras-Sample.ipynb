{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：<a href=\"https://qiita.com/inoory/items/e63ade6f21766c7c2393\">[Python] Keras-RLで簡単に強化学習(DQN)を試す</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "\n",
    "# 直線上を動く点の速度を操作し、目標(原点)に移動させることを目標とする環境\n",
    "class PointOnLine(gym.core.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.Discrete(3) # 行動空間。速度を下げる、そのまま、上げるの3種\n",
    "\n",
    "        high = np.array([1.0, 1.0]) # 観測空間(state)の次元 (位置と速度の2次元) とそれらの最大値\n",
    "        self.observation_space = gym.spaces.Box(low=-high, high=high) # 最小値は、最大値のマイナスがけ\n",
    "\n",
    "    # 各stepごとに呼ばれる\n",
    "    # actionを受け取り、次のstateとreward、episodeが終了したかどうかを返すように実装\n",
    "    def _step(self, action):\n",
    "        # actionを受け取り、次のstateを決定\n",
    "        dt = 0.1\n",
    "        acc = (action - 1) * 0.1\n",
    "        self._vel += acc * dt\n",
    "        self._vel = max(-1.0,  min(self._vel, 1.0))\n",
    "        self._pos += self._vel * dt\n",
    "        self._pos = max(-1.0,  min(self._pos, 1.0))\n",
    "\n",
    "        # 位置と速度の絶対値が十分小さくなったらepisode終了\n",
    "        done = abs(self._pos) < 0.1 and abs(self._vel) < 0.1\n",
    "\n",
    "        if done:\n",
    "            # 終了したときに正の報酬\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            # 時間経過ごとに負の報酬\n",
    "            # ゴールに近づくように、距離が近くなるほど絶対値を減らしておくと、学習が早く進む\n",
    "            reward = -0.01 * abs(self._pos)\n",
    "\n",
    "        # 次のstate、reward、終了したかどうか、追加情報の順に返す\n",
    "        # 追加情報は特にないので空dict\n",
    "        return np.array([self._pos, self._vel]), reward, done, {}\n",
    "\n",
    "    # 各episodeの開始時に呼ばれ、初期stateを返すように実装\n",
    "    def _reset(self):\n",
    "        # 初期stateは、位置はランダム、速度ゼロ\n",
    "        self._pos = np.random.rand()*2 - 1\n",
    "        self._vel = 0.0\n",
    "        return np.array([self._pos, self._vel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 643\n",
      "Trainable params: 643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n",
      "   300/50000: episode: 1, duration: 2.039s, episode steps: 300, steps per second: 147, episode reward: -2.657, mean reward: -0.009 [-0.010, -0.000], mean action: 0.390 [0.000, 2.000], mean observation: -0.768 [-1.000, 0.652], loss: 0.000178, mean_absolute_error: 0.022596, mean_q: -0.003835\n",
      "   316/50000: episode: 2, duration: 0.075s, episode steps: 16, steps per second: 213, episode reward: 0.978, mean reward: 0.061 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.043 [-0.180, 0.080], loss: 0.000017, mean_absolute_error: 0.025804, mean_q: -0.024568\n",
      "   317/50000: episode: 3, duration: 0.008s, episode steps: 1, steps per second: 128, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.045 [-0.100, 0.010], loss: 0.000017, mean_absolute_error: 0.024916, mean_q: -0.022932\n",
      "   617/50000: episode: 4, duration: 1.091s, episode steps: 300, steps per second: 275, episode reward: -1.798, mean reward: -0.006 [-0.008, -0.000], mean action: 1.020 [0.000, 2.000], mean observation: 0.285 [-0.396, 0.815], loss: 0.002171, mean_absolute_error: 0.039612, mean_q: 0.009728\n",
      "   873/50000: episode: 5, duration: 0.809s, episode steps: 256, steps per second: 317, episode reward: -0.806, mean reward: -0.003 [-0.010, 1.000], mean action: 0.965 [0.000, 2.000], mean observation: 0.348 [-0.260, 1.000], loss: 0.000738, mean_absolute_error: 0.049893, mean_q: 0.034980\n",
      "  1173/50000: episode: 6, duration: 0.995s, episode steps: 300, steps per second: 302, episode reward: -0.496, mean reward: -0.002 [-0.004, -0.001], mean action: 1.007 [0.000, 2.000], mean observation: 0.078 [-0.130, 0.375], loss: 0.000498, mean_absolute_error: 0.064904, mean_q: 0.060813\n",
      "  1473/50000: episode: 7, duration: 0.868s, episode steps: 300, steps per second: 346, episode reward: -0.834, mean reward: -0.003 [-0.004, -0.000], mean action: 1.007 [0.000, 2.000], mean observation: 0.140 [-0.170, 0.365], loss: 0.000254, mean_absolute_error: 0.083005, mean_q: 0.091477\n",
      "  1492/50000: episode: 8, duration: 0.059s, episode steps: 19, steps per second: 321, episode reward: 0.973, mean reward: 0.051 [-0.002, 1.000], mean action: 0.579 [0.000, 2.000], mean observation: 0.056 [-0.090, 0.168], loss: 0.000225, mean_absolute_error: 0.087379, mean_q: 0.093807\n",
      "  1792/50000: episode: 9, duration: 1.164s, episode steps: 300, steps per second: 258, episode reward: -2.978, mean reward: -0.010 [-0.010, -0.008], mean action: 0.873 [0.000, 2.000], mean observation: -0.693 [-1.000, 0.010], loss: 0.000308, mean_absolute_error: 0.088263, mean_q: 0.080730\n",
      "  2092/50000: episode: 10, duration: 1.016s, episode steps: 300, steps per second: 295, episode reward: -2.396, mean reward: -0.008 [-0.010, -0.000], mean action: 0.897 [0.000, 2.000], mean observation: -0.384 [-1.000, 0.983], loss: 0.000271, mean_absolute_error: 0.096468, mean_q: 0.062568\n",
      "  2117/50000: episode: 11, duration: 0.077s, episode steps: 25, steps per second: 323, episode reward: 0.976, mean reward: 0.039 [-0.002, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: 0.005 [-0.185, 0.160], loss: 0.000438, mean_absolute_error: 0.106285, mean_q: 0.065224\n",
      "  2125/50000: episode: 12, duration: 0.032s, episode steps: 8, steps per second: 251, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.039 [-0.050, 0.122], loss: 0.000230, mean_absolute_error: 0.102942, mean_q: 0.058375\n",
      "  2126/50000: episode: 13, duration: 0.006s, episode steps: 1, steps per second: 158, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.010 [-0.021, 0.000], loss: 0.000063, mean_absolute_error: 0.115313, mean_q: 0.043583\n",
      "  2426/50000: episode: 14, duration: 0.887s, episode steps: 300, steps per second: 338, episode reward: -2.992, mean reward: -0.010 [-0.010, -0.010], mean action: 0.890 [0.000, 2.000], mean observation: -0.604 [-1.000, 0.010], loss: 0.000416, mean_absolute_error: 0.110406, mean_q: 0.047935\n",
      "  2549/50000: episode: 15, duration: 0.371s, episode steps: 123, steps per second: 331, episode reward: 0.504, mean reward: 0.004 [-0.008, 1.000], mean action: 1.073 [0.000, 2.000], mean observation: 0.084 [-0.270, 0.833], loss: 0.000591, mean_absolute_error: 0.122544, mean_q: 0.047496\n",
      "  2577/50000: episode: 16, duration: 0.084s, episode steps: 28, steps per second: 334, episode reward: 0.937, mean reward: 0.033 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.061 [-0.170, 0.348], loss: 0.000232, mean_absolute_error: 0.125632, mean_q: 0.052551\n",
      "  2625/50000: episode: 17, duration: 0.155s, episode steps: 48, steps per second: 310, episode reward: 0.875, mean reward: 0.018 [-0.005, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.069 [-0.200, 0.493], loss: 0.000355, mean_absolute_error: 0.130186, mean_q: 0.054997\n",
      "  2626/50000: episode: 18, duration: 0.008s, episode steps: 1, steps per second: 128, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.042 [-0.075, -0.010], loss: 0.000367, mean_absolute_error: 0.115307, mean_q: 0.027158\n",
      "  2926/50000: episode: 19, duration: 0.895s, episode steps: 300, steps per second: 335, episode reward: -2.995, mean reward: -0.010 [-0.010, -0.009], mean action: 0.870 [0.000, 2.000], mean observation: -0.665 [-1.000, -0.010], loss: 0.000368, mean_absolute_error: 0.140955, mean_q: 0.052141\n",
      "  2927/50000: episode: 20, duration: 0.007s, episode steps: 1, steps per second: 152, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [-0.010, 0.040], loss: 0.000053, mean_absolute_error: 0.138178, mean_q: 0.020503\n",
      "  3227/50000: episode: 21, duration: 0.886s, episode steps: 300, steps per second: 339, episode reward: -2.545, mean reward: -0.008 [-0.010, -0.000], mean action: 0.863 [0.000, 2.000], mean observation: -0.548 [-1.000, 0.522], loss: 0.000318, mean_absolute_error: 0.161572, mean_q: 0.047817\n",
      "  3442/50000: episode: 22, duration: 0.671s, episode steps: 215, steps per second: 321, episode reward: 0.404, mean reward: 0.002 [-0.007, 1.000], mean action: 1.005 [0.000, 2.000], mean observation: -0.071 [-0.364, 0.689], loss: 0.000174, mean_absolute_error: 0.178064, mean_q: 0.056035\n",
      "  3566/50000: episode: 23, duration: 0.412s, episode steps: 124, steps per second: 301, episode reward: 0.610, mean reward: 0.005 [-0.008, 1.000], mean action: 1.024 [0.000, 2.000], mean observation: -0.026 [-0.360, 0.799], loss: 0.000206, mean_absolute_error: 0.190504, mean_q: 0.080139\n",
      "  3583/50000: episode: 24, duration: 0.077s, episode steps: 17, steps per second: 222, episode reward: 0.977, mean reward: 0.057 [-0.002, 1.000], mean action: 0.471 [0.000, 2.000], mean observation: 0.043 [-0.100, 0.175], loss: 0.000052, mean_absolute_error: 0.209556, mean_q: 0.098938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3664/50000: episode: 25, duration: 0.335s, episode steps: 81, steps per second: 242, episode reward: 0.727, mean reward: 0.009 [-0.008, 1.000], mean action: 1.025 [0.000, 2.000], mean observation: 0.065 [-0.300, 0.826], loss: 0.000319, mean_absolute_error: 0.204649, mean_q: 0.093581\n",
      "  3749/50000: episode: 26, duration: 0.302s, episode steps: 85, steps per second: 282, episode reward: 0.702, mean reward: 0.008 [-0.009, 1.000], mean action: 1.082 [0.000, 2.000], mean observation: 0.058 [-0.300, 0.852], loss: 0.000193, mean_absolute_error: 0.214266, mean_q: 0.112634\n",
      "  3767/50000: episode: 27, duration: 0.075s, episode steps: 18, steps per second: 239, episode reward: 0.973, mean reward: 0.054 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.039 [-0.130, 0.210], loss: 0.000172, mean_absolute_error: 0.221602, mean_q: 0.127839\n",
      "  3774/50000: episode: 28, duration: 0.029s, episode steps: 7, steps per second: 242, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 1.714 [0.000, 2.000], mean observation: -0.039 [-0.122, 0.050], loss: 0.000159, mean_absolute_error: 0.188834, mean_q: 0.059667\n",
      "  3798/50000: episode: 29, duration: 0.086s, episode steps: 24, steps per second: 280, episode reward: 0.964, mean reward: 0.040 [-0.002, 1.000], mean action: 1.083 [0.000, 2.000], mean observation: -0.053 [-0.218, 0.090], loss: 0.000122, mean_absolute_error: 0.213956, mean_q: 0.109109\n",
      "  3835/50000: episode: 30, duration: 0.174s, episode steps: 37, steps per second: 213, episode reward: 0.872, mean reward: 0.024 [-0.005, 1.000], mean action: 1.216 [0.000, 2.000], mean observation: -0.113 [-0.550, 0.210], loss: 0.000102, mean_absolute_error: 0.228724, mean_q: 0.130622\n",
      "  3919/50000: episode: 31, duration: 0.335s, episode steps: 84, steps per second: 251, episode reward: 0.425, mean reward: 0.005 [-0.010, 1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.286 [-0.976, 0.300], loss: 0.000155, mean_absolute_error: 0.230497, mean_q: 0.137050\n",
      "  3959/50000: episode: 32, duration: 0.265s, episode steps: 40, steps per second: 151, episode reward: 0.875, mean reward: 0.022 [-0.006, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.088 [-0.230, 0.553], loss: 0.000162, mean_absolute_error: 0.224108, mean_q: 0.111358\n",
      "  4060/50000: episode: 33, duration: 0.380s, episode steps: 101, steps per second: 266, episode reward: 0.264, mean reward: 0.003 [-0.010, 1.000], mean action: 1.040 [0.000, 2.000], mean observation: -0.328 [-1.000, 0.290], loss: 0.000151, mean_absolute_error: 0.238971, mean_q: 0.141397\n",
      "  4161/50000: episode: 34, duration: 0.364s, episode steps: 101, steps per second: 278, episode reward: 0.634, mean reward: 0.006 [-0.010, 1.000], mean action: 1.030 [0.000, 2.000], mean observation: 0.043 [-0.330, 0.959], loss: 0.000317, mean_absolute_error: 0.240934, mean_q: 0.143001\n",
      "  4164/50000: episode: 35, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 0.998, mean reward: 0.333 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.105, 0.030], loss: 0.000073, mean_absolute_error: 0.228699, mean_q: 0.124858\n",
      "  4203/50000: episode: 36, duration: 0.133s, episode steps: 39, steps per second: 293, episode reward: 0.896, mean reward: 0.023 [-0.004, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.089 [-0.448, 0.170], loss: 0.000060, mean_absolute_error: 0.252744, mean_q: 0.154888\n",
      "  4212/50000: episode: 37, duration: 0.033s, episode steps: 9, steps per second: 275, episode reward: 0.991, mean reward: 0.110 [-0.001, 1.000], mean action: 1.444 [0.000, 2.000], mean observation: -0.040 [-0.125, 0.040], loss: 0.000158, mean_absolute_error: 0.245876, mean_q: 0.126744\n",
      "  4233/50000: episode: 38, duration: 0.085s, episode steps: 21, steps per second: 248, episode reward: 0.960, mean reward: 0.046 [-0.003, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.055 [-0.130, 0.266], loss: 0.000120, mean_absolute_error: 0.270098, mean_q: 0.223146\n",
      "  4245/50000: episode: 39, duration: 0.048s, episode steps: 12, steps per second: 250, episode reward: 0.985, mean reward: 0.082 [-0.002, 1.000], mean action: 1.583 [0.000, 2.000], mean observation: -0.038 [-0.156, 0.080], loss: 0.000054, mean_absolute_error: 0.255407, mean_q: 0.157095\n",
      "  4260/50000: episode: 40, duration: 0.049s, episode steps: 15, steps per second: 308, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 0.467 [0.000, 2.000], mean observation: 0.044 [-0.080, 0.184], loss: 0.000641, mean_absolute_error: 0.250268, mean_q: 0.149868\n",
      "  4292/50000: episode: 41, duration: 0.113s, episode steps: 32, steps per second: 284, episode reward: 0.920, mean reward: 0.029 [-0.004, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.077 [-0.160, 0.379], loss: 0.000397, mean_absolute_error: 0.259844, mean_q: 0.168472\n",
      "  4312/50000: episode: 42, duration: 0.069s, episode steps: 20, steps per second: 291, episode reward: 0.970, mean reward: 0.049 [-0.002, 1.000], mean action: 0.550 [0.000, 2.000], mean observation: 0.034 [-0.130, 0.219], loss: 0.000526, mean_absolute_error: 0.270245, mean_q: 0.194810\n",
      "  4343/50000: episode: 43, duration: 0.088s, episode steps: 31, steps per second: 354, episode reward: 0.926, mean reward: 0.030 [-0.004, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.079 [-0.357, 0.150], loss: 0.000118, mean_absolute_error: 0.264009, mean_q: 0.186865\n",
      "  4377/50000: episode: 44, duration: 0.101s, episode steps: 34, steps per second: 335, episode reward: 0.917, mean reward: 0.027 [-0.004, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.080 [-0.140, 0.385], loss: 0.000553, mean_absolute_error: 0.267094, mean_q: 0.172144\n",
      "  4422/50000: episode: 45, duration: 0.130s, episode steps: 45, steps per second: 347, episode reward: 0.830, mean reward: 0.018 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.129 [-0.630, 0.200], loss: 0.000257, mean_absolute_error: 0.278882, mean_q: 0.194379\n",
      "  4442/50000: episode: 46, duration: 0.066s, episode steps: 20, steps per second: 304, episode reward: 0.970, mean reward: 0.049 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.039 [-0.216, 0.100], loss: 0.000340, mean_absolute_error: 0.296422, mean_q: 0.253544\n",
      "  4492/50000: episode: 47, duration: 0.166s, episode steps: 50, steps per second: 300, episode reward: 0.795, mean reward: 0.016 [-0.006, 1.000], mean action: 0.880 [0.000, 2.000], mean observation: 0.151 [-0.220, 0.643], loss: 0.000182, mean_absolute_error: 0.288853, mean_q: 0.230368\n",
      "  4493/50000: episode: 48, duration: 0.007s, episode steps: 1, steps per second: 142, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.028 [-0.067, 0.010], loss: 0.000062, mean_absolute_error: 0.331651, mean_q: 0.317794\n",
      "  4524/50000: episode: 49, duration: 0.103s, episode steps: 31, steps per second: 301, episode reward: 0.931, mean reward: 0.030 [-0.003, 1.000], mean action: 1.194 [0.000, 2.000], mean observation: -0.073 [-0.342, 0.130], loss: 0.000161, mean_absolute_error: 0.277920, mean_q: 0.189882\n",
      "  4547/50000: episode: 50, duration: 0.069s, episode steps: 23, steps per second: 334, episode reward: 0.962, mean reward: 0.042 [-0.002, 1.000], mean action: 1.304 [0.000, 2.000], mean observation: -0.057 [-0.226, 0.090], loss: 0.000229, mean_absolute_error: 0.294977, mean_q: 0.240737\n",
      "  4589/50000: episode: 51, duration: 0.129s, episode steps: 42, steps per second: 326, episode reward: 0.880, mean reward: 0.021 [-0.005, 1.000], mean action: 1.095 [0.000, 2.000], mean observation: -0.097 [-0.495, 0.170], loss: 0.000558, mean_absolute_error: 0.289389, mean_q: 0.220504\n",
      "  4640/50000: episode: 52, duration: 0.171s, episode steps: 51, steps per second: 298, episode reward: 0.736, mean reward: 0.014 [-0.009, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.181 [-0.280, 0.868], loss: 0.000181, mean_absolute_error: 0.301346, mean_q: 0.244003\n",
      "  4672/50000: episode: 53, duration: 0.098s, episode steps: 32, steps per second: 327, episode reward: 0.931, mean reward: 0.029 [-0.003, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.071 [-0.340, 0.120], loss: 0.000495, mean_absolute_error: 0.282640, mean_q: 0.214590\n",
      "  4722/50000: episode: 54, duration: 0.152s, episode steps: 50, steps per second: 329, episode reward: 0.786, mean reward: 0.016 [-0.007, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.151 [-0.733, 0.220], loss: 0.000193, mean_absolute_error: 0.297142, mean_q: 0.243449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4776/50000: episode: 55, duration: 0.160s, episode steps: 54, steps per second: 337, episode reward: 0.697, mean reward: 0.013 [-0.010, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.195 [-0.270, 0.984], loss: 0.000230, mean_absolute_error: 0.303434, mean_q: 0.250355\n",
      "  4809/50000: episode: 56, duration: 0.123s, episode steps: 33, steps per second: 268, episode reward: 0.917, mean reward: 0.028 [-0.004, 1.000], mean action: 0.848 [0.000, 2.000], mean observation: 0.082 [-0.150, 0.393], loss: 0.000156, mean_absolute_error: 0.292306, mean_q: 0.225044\n",
      "  4855/50000: episode: 57, duration: 0.145s, episode steps: 46, steps per second: 317, episode reward: 0.830, mean reward: 0.018 [-0.006, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.127 [-0.210, 0.639], loss: 0.000141, mean_absolute_error: 0.308014, mean_q: 0.273040\n",
      "  4880/50000: episode: 58, duration: 0.101s, episode steps: 25, steps per second: 247, episode reward: 0.955, mean reward: 0.038 [-0.003, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.060 [-0.258, 0.100], loss: 0.000554, mean_absolute_error: 0.306259, mean_q: 0.236481\n",
      "  4895/50000: episode: 59, duration: 0.054s, episode steps: 15, steps per second: 278, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 1.467 [0.000, 2.000], mean observation: -0.044 [-0.186, 0.080], loss: 0.000158, mean_absolute_error: 0.317213, mean_q: 0.311773\n",
      "  4954/50000: episode: 60, duration: 0.241s, episode steps: 59, steps per second: 245, episode reward: 0.733, mean reward: 0.012 [-0.008, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.165 [-0.210, 0.770], loss: 0.000369, mean_absolute_error: 0.314343, mean_q: 0.277986\n",
      "  4994/50000: episode: 61, duration: 0.165s, episode steps: 40, steps per second: 243, episode reward: 0.858, mean reward: 0.021 [-0.005, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.122 [-0.180, 0.548], loss: 0.000403, mean_absolute_error: 0.321889, mean_q: 0.284406\n",
      "  5024/50000: episode: 62, duration: 0.101s, episode steps: 30, steps per second: 296, episode reward: 0.929, mean reward: 0.031 [-0.004, 1.000], mean action: 1.267 [0.000, 2.000], mean observation: -0.075 [-0.359, 0.130], loss: 0.000175, mean_absolute_error: 0.318745, mean_q: 0.270617\n",
      "  5066/50000: episode: 63, duration: 0.124s, episode steps: 42, steps per second: 338, episode reward: 0.878, mean reward: 0.021 [-0.005, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.103 [-0.150, 0.460], loss: 0.000179, mean_absolute_error: 0.324521, mean_q: 0.298808\n",
      "  5067/50000: episode: 64, duration: 0.006s, episode steps: 1, steps per second: 170, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.006 [0.002, 0.010], loss: 0.000043, mean_absolute_error: 0.317620, mean_q: 0.191237\n",
      "  5083/50000: episode: 65, duration: 0.057s, episode steps: 16, steps per second: 280, episode reward: 0.978, mean reward: 0.061 [-0.002, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.045 [-0.180, 0.070], loss: 0.000033, mean_absolute_error: 0.313477, mean_q: 0.263099\n",
      "  5118/50000: episode: 66, duration: 0.122s, episode steps: 35, steps per second: 287, episode reward: 0.907, mean reward: 0.026 [-0.004, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.087 [-0.140, 0.417], loss: 0.000433, mean_absolute_error: 0.336538, mean_q: 0.325176\n",
      "  5165/50000: episode: 67, duration: 0.152s, episode steps: 47, steps per second: 310, episode reward: 0.785, mean reward: 0.017 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.159 [-0.220, 0.761], loss: 0.000409, mean_absolute_error: 0.327863, mean_q: 0.302382\n",
      "  5201/50000: episode: 68, duration: 0.114s, episode steps: 36, steps per second: 317, episode reward: 0.884, mean reward: 0.025 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.100 [-0.220, 0.516], loss: 0.000797, mean_absolute_error: 0.339227, mean_q: 0.334424\n",
      "  5251/50000: episode: 69, duration: 0.214s, episode steps: 50, steps per second: 234, episode reward: 0.800, mean reward: 0.016 [-0.007, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.141 [-0.200, 0.703], loss: 0.000559, mean_absolute_error: 0.336522, mean_q: 0.313404\n",
      "  5308/50000: episode: 70, duration: 0.182s, episode steps: 57, steps per second: 314, episode reward: 0.724, mean reward: 0.013 [-0.009, 1.000], mean action: 0.895 [0.000, 2.000], mean observation: 0.175 [-0.250, 0.875], loss: 0.000264, mean_absolute_error: 0.341388, mean_q: 0.319502\n",
      "  5370/50000: episode: 71, duration: 0.189s, episode steps: 62, steps per second: 328, episode reward: 0.718, mean reward: 0.012 [-0.008, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.169 [-0.180, 0.820], loss: 0.000353, mean_absolute_error: 0.340525, mean_q: 0.322722\n",
      "  5401/50000: episode: 72, duration: 0.121s, episode steps: 31, steps per second: 257, episode reward: 0.920, mean reward: 0.030 [-0.004, 1.000], mean action: 1.258 [0.000, 2.000], mean observation: -0.082 [-0.399, 0.130], loss: 0.000072, mean_absolute_error: 0.348068, mean_q: 0.344666\n",
      "  5416/50000: episode: 73, duration: 0.057s, episode steps: 15, steps per second: 265, episode reward: 0.981, mean reward: 0.065 [-0.002, 1.000], mean action: 1.467 [0.000, 2.000], mean observation: -0.045 [-0.161, 0.070], loss: 0.000044, mean_absolute_error: 0.349903, mean_q: 0.343988\n",
      "  5475/50000: episode: 74, duration: 0.199s, episode steps: 59, steps per second: 296, episode reward: 0.751, mean reward: 0.013 [-0.007, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.158 [-0.180, 0.728], loss: 0.000272, mean_absolute_error: 0.355824, mean_q: 0.359230\n",
      "  5476/50000: episode: 75, duration: 0.006s, episode steps: 1, steps per second: 161, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [-0.010, 0.076], loss: 0.000094, mean_absolute_error: 0.306341, mean_q: 0.178060\n",
      "  5487/50000: episode: 76, duration: 0.035s, episode steps: 11, steps per second: 310, episode reward: 0.988, mean reward: 0.090 [-0.001, 1.000], mean action: 1.545 [0.000, 2.000], mean observation: -0.041 [-0.139, 0.070], loss: 0.000120, mean_absolute_error: 0.356765, mean_q: 0.339822\n",
      "  5523/50000: episode: 77, duration: 0.111s, episode steps: 36, steps per second: 325, episode reward: 0.904, mean reward: 0.025 [-0.004, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.090 [-0.412, 0.130], loss: 0.000137, mean_absolute_error: 0.349788, mean_q: 0.345505\n",
      "  5561/50000: episode: 78, duration: 0.156s, episode steps: 38, steps per second: 244, episode reward: 0.902, mean reward: 0.024 [-0.004, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.089 [-0.405, 0.130], loss: 0.000271, mean_absolute_error: 0.357993, mean_q: 0.355800\n",
      "  5594/50000: episode: 79, duration: 0.096s, episode steps: 33, steps per second: 344, episode reward: 0.930, mean reward: 0.028 [-0.003, 1.000], mean action: 1.182 [0.000, 2.000], mean observation: -0.075 [-0.314, 0.090], loss: 0.000147, mean_absolute_error: 0.355281, mean_q: 0.344749\n",
      "  5630/50000: episode: 80, duration: 0.115s, episode steps: 36, steps per second: 312, episode reward: 0.907, mean reward: 0.025 [-0.004, 1.000], mean action: 0.861 [0.000, 2.000], mean observation: 0.086 [-0.130, 0.416], loss: 0.000331, mean_absolute_error: 0.362737, mean_q: 0.368846\n",
      "  5684/50000: episode: 81, duration: 0.187s, episode steps: 54, steps per second: 289, episode reward: 0.740, mean reward: 0.014 [-0.009, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.171 [-0.866, 0.250], loss: 0.000315, mean_absolute_error: 0.368205, mean_q: 0.368714\n",
      "  5728/50000: episode: 82, duration: 0.126s, episode steps: 44, steps per second: 350, episode reward: 0.860, mean reward: 0.020 [-0.005, 1.000], mean action: 1.159 [0.000, 2.000], mean observation: -0.113 [-0.513, 0.130], loss: 0.000125, mean_absolute_error: 0.366380, mean_q: 0.374836\n",
      "  5782/50000: episode: 83, duration: 0.153s, episode steps: 54, steps per second: 352, episode reward: 0.760, mean reward: 0.014 [-0.008, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.161 [-0.200, 0.772], loss: 0.000288, mean_absolute_error: 0.371149, mean_q: 0.377549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5798/50000: episode: 84, duration: 0.054s, episode steps: 16, steps per second: 296, episode reward: 0.982, mean reward: 0.061 [-0.001, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: 0.048 [-0.040, 0.138], loss: 0.000176, mean_absolute_error: 0.378384, mean_q: 0.399492\n",
      "  5825/50000: episode: 85, duration: 0.087s, episode steps: 27, steps per second: 310, episode reward: 0.951, mean reward: 0.035 [-0.003, 1.000], mean action: 0.815 [0.000, 2.000], mean observation: 0.062 [-0.100, 0.266], loss: 0.000106, mean_absolute_error: 0.369217, mean_q: 0.378842\n",
      "  5879/50000: episode: 86, duration: 0.159s, episode steps: 54, steps per second: 339, episode reward: 0.792, mean reward: 0.015 [-0.007, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.140 [-0.190, 0.672], loss: 0.000189, mean_absolute_error: 0.375680, mean_q: 0.394843\n",
      "  5943/50000: episode: 87, duration: 0.210s, episode steps: 64, steps per second: 305, episode reward: 0.679, mean reward: 0.011 [-0.009, 1.000], mean action: 0.906 [0.000, 2.000], mean observation: 0.187 [-0.200, 0.927], loss: 0.000211, mean_absolute_error: 0.378847, mean_q: 0.406802\n",
      "  5972/50000: episode: 88, duration: 0.083s, episode steps: 29, steps per second: 349, episode reward: 0.937, mean reward: 0.032 [-0.003, 1.000], mean action: 0.793 [0.000, 2.000], mean observation: 0.070 [-0.110, 0.328], loss: 0.000113, mean_absolute_error: 0.381605, mean_q: 0.418712\n",
      "  5973/50000: episode: 89, duration: 0.006s, episode steps: 1, steps per second: 172, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.009 [-0.029, 0.010], loss: 0.000021, mean_absolute_error: 0.372326, mean_q: 0.383529\n",
      "  6019/50000: episode: 90, duration: 0.154s, episode steps: 46, steps per second: 299, episode reward: 0.832, mean reward: 0.018 [-0.006, 1.000], mean action: 0.891 [0.000, 2.000], mean observation: 0.125 [-0.220, 0.637], loss: 0.000223, mean_absolute_error: 0.397819, mean_q: 0.440773\n",
      "  6069/50000: episode: 91, duration: 0.195s, episode steps: 50, steps per second: 257, episode reward: 0.782, mean reward: 0.016 [-0.007, 1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.155 [-0.745, 0.190], loss: 0.000062, mean_absolute_error: 0.385696, mean_q: 0.406301\n",
      "  6070/50000: episode: 92, duration: 0.007s, episode steps: 1, steps per second: 145, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.004 [-0.002, 0.010], loss: 0.000034, mean_absolute_error: 0.437923, mean_q: 0.528417\n",
      "  6111/50000: episode: 93, duration: 0.142s, episode steps: 41, steps per second: 289, episode reward: 0.882, mean reward: 0.022 [-0.005, 1.000], mean action: 0.878 [0.000, 2.000], mean observation: 0.098 [-0.160, 0.481], loss: 0.000391, mean_absolute_error: 0.384516, mean_q: 0.413615\n",
      "  6128/50000: episode: 94, duration: 0.051s, episode steps: 17, steps per second: 336, episode reward: 0.978, mean reward: 0.058 [-0.002, 1.000], mean action: 1.412 [1.000, 2.000], mean observation: -0.046 [-0.163, 0.070], loss: 0.000039, mean_absolute_error: 0.389075, mean_q: 0.412310\n",
      "  6184/50000: episode: 95, duration: 0.189s, episode steps: 56, steps per second: 296, episode reward: 0.818, mean reward: 0.015 [-0.006, 1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.117 [-0.616, 0.150], loss: 0.000421, mean_absolute_error: 0.395098, mean_q: 0.433460\n",
      "  6244/50000: episode: 96, duration: 0.198s, episode steps: 60, steps per second: 302, episode reward: 0.682, mean reward: 0.011 [-0.009, 1.000], mean action: 0.900 [0.000, 2.000], mean observation: 0.196 [-0.210, 0.942], loss: 0.000362, mean_absolute_error: 0.393815, mean_q: 0.428144\n",
      "  6245/50000: episode: 97, duration: 0.007s, episode steps: 1, steps per second: 139, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.045 [-0.010, 0.100], loss: 0.000061, mean_absolute_error: 0.339782, mean_q: 0.283430\n",
      "  6307/50000: episode: 98, duration: 0.188s, episode steps: 62, steps per second: 330, episode reward: 0.683, mean reward: 0.011 [-0.009, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.190 [-0.220, 0.931], loss: 0.000065, mean_absolute_error: 0.394427, mean_q: 0.426384\n",
      "  6368/50000: episode: 99, duration: 0.183s, episode steps: 61, steps per second: 334, episode reward: 0.686, mean reward: 0.011 [-0.009, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.189 [-0.220, 0.940], loss: 0.000305, mean_absolute_error: 0.398137, mean_q: 0.444061\n",
      "  6424/50000: episode: 100, duration: 0.202s, episode steps: 56, steps per second: 277, episode reward: 0.770, mean reward: 0.014 [-0.007, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.150 [-0.180, 0.727], loss: 0.000065, mean_absolute_error: 0.402584, mean_q: 0.442026\n",
      "  6492/50000: episode: 101, duration: 0.237s, episode steps: 68, steps per second: 287, episode reward: 0.640, mean reward: 0.009 [-0.010, 1.000], mean action: 0.926 [0.000, 2.000], mean observation: 0.201 [-0.240, 0.975], loss: 0.000332, mean_absolute_error: 0.397051, mean_q: 0.443664\n",
      "  6532/50000: episode: 102, duration: 0.134s, episode steps: 40, steps per second: 298, episode reward: 0.889, mean reward: 0.022 [-0.005, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.095 [-0.459, 0.130], loss: 0.000090, mean_absolute_error: 0.411246, mean_q: 0.479201\n",
      "  6574/50000: episode: 103, duration: 0.125s, episode steps: 42, steps per second: 337, episode reward: 0.901, mean reward: 0.021 [-0.004, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.087 [-0.362, 0.080], loss: 0.000028, mean_absolute_error: 0.403102, mean_q: 0.443678\n",
      "  6614/50000: episode: 104, duration: 0.131s, episode steps: 40, steps per second: 306, episode reward: 0.886, mean reward: 0.022 [-0.005, 1.000], mean action: 0.825 [0.000, 2.000], mean observation: 0.098 [-0.160, 0.462], loss: 0.000469, mean_absolute_error: 0.405086, mean_q: 0.440243\n",
      "  6650/50000: episode: 105, duration: 0.112s, episode steps: 36, steps per second: 321, episode reward: 0.905, mean reward: 0.025 [-0.004, 1.000], mean action: 0.806 [0.000, 2.000], mean observation: 0.088 [-0.130, 0.418], loss: 0.000165, mean_absolute_error: 0.420161, mean_q: 0.489889\n",
      "  6703/50000: episode: 106, duration: 0.154s, episode steps: 53, steps per second: 344, episode reward: 0.790, mean reward: 0.015 [-0.007, 1.000], mean action: 1.094 [0.000, 2.000], mean observation: -0.140 [-0.721, 0.200], loss: 0.000088, mean_absolute_error: 0.407788, mean_q: 0.455860\n",
      "  6740/50000: episode: 107, duration: 0.128s, episode steps: 37, steps per second: 289, episode reward: 0.911, mean reward: 0.025 [-0.004, 1.000], mean action: 1.162 [0.000, 2.000], mean observation: -0.084 [-0.376, 0.090], loss: 0.000093, mean_absolute_error: 0.415549, mean_q: 0.470073\n",
      "  6805/50000: episode: 108, duration: 0.199s, episode steps: 65, steps per second: 327, episode reward: 0.642, mean reward: 0.010 [-0.009, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.211 [-0.270, 0.938], loss: 0.000415, mean_absolute_error: 0.412116, mean_q: 0.474291\n",
      "  6857/50000: episode: 109, duration: 0.167s, episode steps: 52, steps per second: 312, episode reward: 0.797, mean reward: 0.015 [-0.007, 1.000], mean action: 1.135 [0.000, 2.000], mean observation: -0.139 [-0.686, 0.180], loss: 0.000078, mean_absolute_error: 0.412183, mean_q: 0.474755\n",
      "  6887/50000: episode: 110, duration: 0.091s, episode steps: 30, steps per second: 329, episode reward: 0.945, mean reward: 0.032 [-0.002, 1.000], mean action: 1.267 [0.000, 2.000], mean observation: -0.070 [-0.238, 0.100], loss: 0.001194, mean_absolute_error: 0.420051, mean_q: 0.489299\n",
      "  6905/50000: episode: 111, duration: 0.063s, episode steps: 18, steps per second: 288, episode reward: 0.971, mean reward: 0.054 [-0.002, 1.000], mean action: 0.611 [0.000, 2.000], mean observation: 0.048 [-0.110, 0.219], loss: 0.000285, mean_absolute_error: 0.408830, mean_q: 0.445950\n",
      "  6954/50000: episode: 112, duration: 0.171s, episode steps: 49, steps per second: 287, episode reward: 0.822, mean reward: 0.017 [-0.006, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.133 [-0.190, 0.588], loss: 0.000110, mean_absolute_error: 0.416654, mean_q: 0.474048\n",
      "  6996/50000: episode: 113, duration: 0.184s, episode steps: 42, steps per second: 229, episode reward: 0.874, mean reward: 0.021 [-0.005, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.104 [-0.499, 0.130], loss: 0.000217, mean_absolute_error: 0.425776, mean_q: 0.490370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7018/50000: episode: 114, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 0.963, mean reward: 0.044 [-0.002, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.053 [-0.110, 0.241], loss: 0.000052, mean_absolute_error: 0.425383, mean_q: 0.487421\n",
      "  7068/50000: episode: 115, duration: 0.210s, episode steps: 50, steps per second: 239, episode reward: 0.861, mean reward: 0.017 [-0.004, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.105 [-0.448, 0.090], loss: 0.000082, mean_absolute_error: 0.428183, mean_q: 0.504818\n",
      "  7091/50000: episode: 116, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 0.960, mean reward: 0.042 [-0.002, 1.000], mean action: 0.783 [0.000, 2.000], mean observation: 0.056 [-0.090, 0.247], loss: 0.000029, mean_absolute_error: 0.431833, mean_q: 0.494861\n",
      "  7092/50000: episode: 117, duration: 0.008s, episode steps: 1, steps per second: 126, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.007 [-0.025, 0.010], loss: 0.000029, mean_absolute_error: 0.423204, mean_q: 0.498099\n",
      "  7096/50000: episode: 118, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 0.997, mean reward: 0.249 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [-0.040, 0.105], loss: 0.000019, mean_absolute_error: 0.422047, mean_q: 0.547178\n",
      "  7097/50000: episode: 119, duration: 0.007s, episode steps: 1, steps per second: 134, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.002 [-0.004, 0.000], loss: 0.000016, mean_absolute_error: 0.427004, mean_q: 0.558686\n",
      "  7098/50000: episode: 120, duration: 0.007s, episode steps: 1, steps per second: 151, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.007 [-0.023, 0.010], loss: 0.000013, mean_absolute_error: 0.390997, mean_q: 0.317195\n",
      "  7136/50000: episode: 121, duration: 0.168s, episode steps: 38, steps per second: 227, episode reward: 0.919, mean reward: 0.024 [-0.003, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.078 [-0.322, 0.080], loss: 0.000156, mean_absolute_error: 0.424542, mean_q: 0.493167\n",
      "  7164/50000: episode: 122, duration: 0.125s, episode steps: 28, steps per second: 223, episode reward: 0.941, mean reward: 0.034 [-0.003, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.068 [-0.110, 0.318], loss: 0.000274, mean_absolute_error: 0.433953, mean_q: 0.517240\n",
      "  7191/50000: episode: 123, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 0.959, mean reward: 0.036 [-0.002, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.059 [-0.195, 0.070], loss: 0.000222, mean_absolute_error: 0.421466, mean_q: 0.480633\n",
      "  7244/50000: episode: 124, duration: 0.259s, episode steps: 53, steps per second: 204, episode reward: 0.780, mean reward: 0.015 [-0.007, 1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.148 [-0.739, 0.200], loss: 0.000118, mean_absolute_error: 0.426639, mean_q: 0.496065\n",
      "  7300/50000: episode: 125, duration: 0.218s, episode steps: 56, steps per second: 256, episode reward: 0.719, mean reward: 0.013 [-0.009, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.175 [-0.250, 0.888], loss: 0.000323, mean_absolute_error: 0.431314, mean_q: 0.511466\n",
      "  7344/50000: episode: 126, duration: 0.128s, episode steps: 44, steps per second: 344, episode reward: 0.854, mean reward: 0.019 [-0.006, 1.000], mean action: 0.841 [0.000, 2.000], mean observation: 0.113 [-0.180, 0.562], loss: 0.000209, mean_absolute_error: 0.431971, mean_q: 0.510642\n",
      "  7383/50000: episode: 127, duration: 0.119s, episode steps: 39, steps per second: 329, episode reward: 0.876, mean reward: 0.022 [-0.005, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.106 [-0.519, 0.170], loss: 0.000339, mean_absolute_error: 0.434058, mean_q: 0.513079\n",
      "  7418/50000: episode: 128, duration: 0.103s, episode steps: 35, steps per second: 341, episode reward: 0.911, mean reward: 0.026 [-0.004, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.083 [-0.160, 0.416], loss: 0.000107, mean_absolute_error: 0.426589, mean_q: 0.501538\n",
      "  7466/50000: episode: 129, duration: 0.138s, episode steps: 48, steps per second: 347, episode reward: 0.817, mean reward: 0.017 [-0.007, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.131 [-0.220, 0.670], loss: 0.000025, mean_absolute_error: 0.440605, mean_q: 0.524444\n",
      "  7471/50000: episode: 130, duration: 0.017s, episode steps: 5, steps per second: 291, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.112, 0.050], loss: 0.000023, mean_absolute_error: 0.440889, mean_q: 0.510092\n",
      "  7526/50000: episode: 131, duration: 0.159s, episode steps: 55, steps per second: 346, episode reward: 0.752, mean reward: 0.014 [-0.008, 1.000], mean action: 0.873 [0.000, 2.000], mean observation: 0.163 [-0.230, 0.798], loss: 0.000072, mean_absolute_error: 0.433204, mean_q: 0.511992\n",
      "  7590/50000: episode: 132, duration: 0.188s, episode steps: 64, steps per second: 341, episode reward: 0.636, mean reward: 0.010 [-0.010, 1.000], mean action: 0.906 [0.000, 2.000], mean observation: 0.215 [-0.260, 0.993], loss: 0.000148, mean_absolute_error: 0.433901, mean_q: 0.515598\n",
      "  7627/50000: episode: 133, duration: 0.140s, episode steps: 37, steps per second: 265, episode reward: 0.890, mean reward: 0.024 [-0.005, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.088 [-0.190, 0.491], loss: 0.000500, mean_absolute_error: 0.431094, mean_q: 0.493774\n",
      "  7666/50000: episode: 134, duration: 0.140s, episode steps: 39, steps per second: 278, episode reward: 0.881, mean reward: 0.023 [-0.005, 1.000], mean action: 0.872 [0.000, 2.000], mean observation: 0.101 [-0.190, 0.503], loss: 0.000177, mean_absolute_error: 0.438197, mean_q: 0.529251\n",
      "  7723/50000: episode: 135, duration: 0.161s, episode steps: 57, steps per second: 353, episode reward: 0.715, mean reward: 0.013 [-0.008, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.187 [-0.220, 0.825], loss: 0.000119, mean_absolute_error: 0.432920, mean_q: 0.501653\n",
      "  7752/50000: episode: 136, duration: 0.088s, episode steps: 29, steps per second: 329, episode reward: 0.936, mean reward: 0.032 [-0.003, 1.000], mean action: 0.759 [0.000, 2.000], mean observation: 0.070 [-0.140, 0.339], loss: 0.000025, mean_absolute_error: 0.437265, mean_q: 0.545923\n",
      "  7803/50000: episode: 137, duration: 0.149s, episode steps: 51, steps per second: 343, episode reward: 0.777, mean reward: 0.015 [-0.007, 1.000], mean action: 0.863 [0.000, 2.000], mean observation: 0.156 [-0.210, 0.745], loss: 0.000362, mean_absolute_error: 0.438521, mean_q: 0.529498\n",
      "  7853/50000: episode: 138, duration: 0.142s, episode steps: 50, steps per second: 351, episode reward: 0.890, mean reward: 0.018 [-0.003, 1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.092 [-0.285, 0.090], loss: 0.000129, mean_absolute_error: 0.436811, mean_q: 0.516586\n",
      "  7854/50000: episode: 139, duration: 0.006s, episode steps: 1, steps per second: 175, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.002 [-0.010, 0.015], loss: 0.000016, mean_absolute_error: 0.436595, mean_q: 0.564372\n",
      "  7904/50000: episode: 140, duration: 0.143s, episode steps: 50, steps per second: 350, episode reward: 0.755, mean reward: 0.015 [-0.008, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.174 [-0.250, 0.813], loss: 0.000276, mean_absolute_error: 0.442165, mean_q: 0.534261\n",
      "  7957/50000: episode: 141, duration: 0.154s, episode steps: 53, steps per second: 344, episode reward: 0.792, mean reward: 0.015 [-0.006, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.147 [-0.190, 0.624], loss: 0.000081, mean_absolute_error: 0.443210, mean_q: 0.533103\n",
      "  7998/50000: episode: 142, duration: 0.122s, episode steps: 41, steps per second: 335, episode reward: 0.855, mean reward: 0.021 [-0.006, 1.000], mean action: 0.805 [0.000, 2.000], mean observation: 0.118 [-0.200, 0.585], loss: 0.000318, mean_absolute_error: 0.438735, mean_q: 0.515778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8060/50000: episode: 143, duration: 0.211s, episode steps: 62, steps per second: 293, episode reward: 0.693, mean reward: 0.011 [-0.008, 1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.188 [-0.840, 0.200], loss: 0.000127, mean_absolute_error: 0.448110, mean_q: 0.551474\n",
      "  8106/50000: episode: 144, duration: 0.140s, episode steps: 46, steps per second: 328, episode reward: 0.828, mean reward: 0.018 [-0.006, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.132 [-0.200, 0.608], loss: 0.000145, mean_absolute_error: 0.448695, mean_q: 0.553551\n",
      "  8141/50000: episode: 145, duration: 0.116s, episode steps: 35, steps per second: 302, episode reward: 0.894, mean reward: 0.026 [-0.005, 1.000], mean action: 0.771 [0.000, 2.000], mean observation: 0.100 [-0.170, 0.465], loss: 0.000179, mean_absolute_error: 0.448171, mean_q: 0.544157\n",
      "  8195/50000: episode: 146, duration: 0.167s, episode steps: 54, steps per second: 323, episode reward: 0.734, mean reward: 0.014 [-0.008, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.180 [-0.240, 0.829], loss: 0.000081, mean_absolute_error: 0.439791, mean_q: 0.540419\n",
      "  8228/50000: episode: 147, duration: 0.124s, episode steps: 33, steps per second: 266, episode reward: 0.917, mean reward: 0.028 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.084 [-0.374, 0.130], loss: 0.000026, mean_absolute_error: 0.452486, mean_q: 0.558233\n",
      "  8276/50000: episode: 148, duration: 0.154s, episode steps: 48, steps per second: 312, episode reward: 0.828, mean reward: 0.017 [-0.006, 1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.130 [-0.576, 0.140], loss: 0.000148, mean_absolute_error: 0.447054, mean_q: 0.551996\n",
      "  8328/50000: episode: 149, duration: 0.157s, episode steps: 52, steps per second: 331, episode reward: 0.773, mean reward: 0.015 [-0.007, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.157 [-0.220, 0.743], loss: 0.000092, mean_absolute_error: 0.448868, mean_q: 0.548772\n",
      "  8387/50000: episode: 150, duration: 0.197s, episode steps: 59, steps per second: 299, episode reward: 0.687, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.198 [-0.896, 0.200], loss: 0.000352, mean_absolute_error: 0.448659, mean_q: 0.547830\n",
      "  8442/50000: episode: 151, duration: 0.156s, episode steps: 55, steps per second: 352, episode reward: 0.752, mean reward: 0.014 [-0.007, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.170 [-0.713, 0.160], loss: 0.000080, mean_absolute_error: 0.444630, mean_q: 0.542924\n",
      "  8491/50000: episode: 152, duration: 0.156s, episode steps: 49, steps per second: 314, episode reward: 0.792, mean reward: 0.016 [-0.007, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.147 [-0.240, 0.740], loss: 0.000086, mean_absolute_error: 0.447991, mean_q: 0.543067\n",
      "  8648/50000: episode: 153, duration: 0.463s, episode steps: 157, steps per second: 339, episode reward: 0.615, mean reward: 0.004 [-0.003, 1.000], mean action: 1.038 [0.000, 2.000], mean observation: -0.120 [-0.303, 0.110], loss: 0.000114, mean_absolute_error: 0.453656, mean_q: 0.561775\n",
      "  8694/50000: episode: 154, duration: 0.138s, episode steps: 46, steps per second: 334, episode reward: 0.838, mean reward: 0.018 [-0.006, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.126 [-0.190, 0.562], loss: 0.000636, mean_absolute_error: 0.450814, mean_q: 0.551515\n",
      "  8735/50000: episode: 155, duration: 0.163s, episode steps: 41, steps per second: 251, episode reward: 0.864, mean reward: 0.021 [-0.005, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.112 [-0.180, 0.546], loss: 0.000344, mean_absolute_error: 0.453925, mean_q: 0.558123\n",
      "  8784/50000: episode: 156, duration: 0.206s, episode steps: 49, steps per second: 238, episode reward: 0.787, mean reward: 0.016 [-0.008, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.151 [-0.240, 0.758], loss: 0.000031, mean_absolute_error: 0.454827, mean_q: 0.559800\n",
      "  8840/50000: episode: 157, duration: 0.322s, episode steps: 56, steps per second: 174, episode reward: 0.777, mean reward: 0.014 [-0.006, 1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.154 [-0.612, 0.190], loss: 0.000173, mean_absolute_error: 0.442209, mean_q: 0.538449\n",
      "  8849/50000: episode: 158, duration: 0.037s, episode steps: 9, steps per second: 246, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 1.667 [0.000, 2.000], mean observation: -0.037 [-0.136, 0.070], loss: 0.000028, mean_absolute_error: 0.445193, mean_q: 0.544102\n",
      "  8850/50000: episode: 159, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.000 [-0.011, 0.010], loss: 0.000020, mean_absolute_error: 0.485785, mean_q: 0.686597\n",
      "  8915/50000: episode: 160, duration: 0.313s, episode steps: 65, steps per second: 207, episode reward: 0.669, mean reward: 0.010 [-0.009, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: -0.195 [-0.876, 0.220], loss: 0.000555, mean_absolute_error: 0.458445, mean_q: 0.569286\n",
      "  8938/50000: episode: 161, duration: 0.123s, episode steps: 23, steps per second: 188, episode reward: 0.959, mean reward: 0.042 [-0.003, 1.000], mean action: 0.739 [0.000, 2.000], mean observation: 0.055 [-0.110, 0.260], loss: 0.000030, mean_absolute_error: 0.456753, mean_q: 0.556386\n",
      "  8987/50000: episode: 162, duration: 0.215s, episode steps: 49, steps per second: 228, episode reward: 0.815, mean reward: 0.017 [-0.006, 1.000], mean action: 0.878 [0.000, 2.000], mean observation: 0.134 [-0.210, 0.641], loss: 0.000022, mean_absolute_error: 0.457200, mean_q: 0.560842\n",
      "  9029/50000: episode: 163, duration: 0.202s, episode steps: 42, steps per second: 208, episode reward: 0.866, mean reward: 0.021 [-0.005, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.113 [-0.497, 0.160], loss: 0.000175, mean_absolute_error: 0.454713, mean_q: 0.542899\n",
      "  9067/50000: episode: 164, duration: 0.164s, episode steps: 38, steps per second: 231, episode reward: 0.888, mean reward: 0.023 [-0.005, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.098 [-0.468, 0.150], loss: 0.000183, mean_absolute_error: 0.459068, mean_q: 0.586438\n",
      "  9131/50000: episode: 165, duration: 0.266s, episode steps: 64, steps per second: 241, episode reward: 0.661, mean reward: 0.010 [-0.009, 1.000], mean action: 1.094 [0.000, 2.000], mean observation: -0.200 [-0.938, 0.180], loss: 0.000049, mean_absolute_error: 0.455244, mean_q: 0.555643\n",
      "  9182/50000: episode: 166, duration: 0.211s, episode steps: 51, steps per second: 242, episode reward: 0.777, mean reward: 0.015 [-0.007, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.157 [-0.220, 0.737], loss: 0.000346, mean_absolute_error: 0.458274, mean_q: 0.569092\n",
      "  9241/50000: episode: 167, duration: 0.204s, episode steps: 59, steps per second: 289, episode reward: 0.685, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.195 [-0.948, 0.220], loss: 0.000088, mean_absolute_error: 0.457057, mean_q: 0.564392\n",
      "  9246/50000: episode: 168, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [-0.050, 0.111], loss: 0.000044, mean_absolute_error: 0.452819, mean_q: 0.583370\n",
      "  9294/50000: episode: 169, duration: 0.229s, episode steps: 48, steps per second: 210, episode reward: 0.819, mean reward: 0.017 [-0.006, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.133 [-0.642, 0.170], loss: 0.000037, mean_absolute_error: 0.465936, mean_q: 0.596345\n",
      "  9345/50000: episode: 170, duration: 0.175s, episode steps: 51, steps per second: 292, episode reward: 0.748, mean reward: 0.015 [-0.008, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.175 [-0.838, 0.220], loss: 0.000143, mean_absolute_error: 0.459306, mean_q: 0.575323\n",
      "  9363/50000: episode: 171, duration: 0.056s, episode steps: 18, steps per second: 323, episode reward: 0.971, mean reward: 0.054 [-0.002, 1.000], mean action: 1.389 [0.000, 2.000], mean observation: -0.050 [-0.220, 0.100], loss: 0.000399, mean_absolute_error: 0.462504, mean_q: 0.560893\n",
      "  9425/50000: episode: 172, duration: 0.177s, episode steps: 62, steps per second: 351, episode reward: 0.665, mean reward: 0.011 [-0.010, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.201 [-0.961, 0.210], loss: 0.000385, mean_absolute_error: 0.456938, mean_q: 0.560768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9439/50000: episode: 173, duration: 0.044s, episode steps: 14, steps per second: 320, episode reward: 0.982, mean reward: 0.070 [-0.002, 1.000], mean action: 0.429 [0.000, 2.000], mean observation: 0.044 [-0.080, 0.162], loss: 0.000261, mean_absolute_error: 0.464538, mean_q: 0.594467\n",
      "  9496/50000: episode: 174, duration: 0.213s, episode steps: 57, steps per second: 268, episode reward: 0.692, mean reward: 0.012 [-0.010, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.194 [-0.280, 0.961], loss: 0.000153, mean_absolute_error: 0.461148, mean_q: 0.577292\n",
      "  9531/50000: episode: 175, duration: 0.137s, episode steps: 35, steps per second: 256, episode reward: 0.924, mean reward: 0.026 [-0.003, 1.000], mean action: 1.229 [0.000, 2.000], mean observation: -0.084 [-0.280, 0.120], loss: 0.000130, mean_absolute_error: 0.459347, mean_q: 0.566962\n",
      "  9572/50000: episode: 176, duration: 0.137s, episode steps: 41, steps per second: 299, episode reward: 0.887, mean reward: 0.022 [-0.004, 1.000], mean action: 1.171 [0.000, 2.000], mean observation: -0.102 [-0.394, 0.160], loss: 0.000044, mean_absolute_error: 0.461929, mean_q: 0.574431\n",
      "  9573/50000: episode: 177, duration: 0.006s, episode steps: 1, steps per second: 177, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [-0.010, 0.057], loss: 0.000016, mean_absolute_error: 0.428342, mean_q: 0.399855\n",
      "  9608/50000: episode: 178, duration: 0.133s, episode steps: 35, steps per second: 263, episode reward: 0.895, mean reward: 0.026 [-0.005, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.091 [-0.491, 0.190], loss: 0.000125, mean_absolute_error: 0.461153, mean_q: 0.583798\n",
      "  9655/50000: episode: 179, duration: 0.179s, episode steps: 47, steps per second: 263, episode reward: 0.817, mean reward: 0.017 [-0.007, 1.000], mean action: 1.149 [0.000, 2.000], mean observation: -0.135 [-0.673, 0.200], loss: 0.000229, mean_absolute_error: 0.458625, mean_q: 0.584181\n",
      "  9715/50000: episode: 180, duration: 0.186s, episode steps: 60, steps per second: 322, episode reward: 0.731, mean reward: 0.012 [-0.008, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.169 [-0.765, 0.200], loss: 0.000355, mean_absolute_error: 0.462747, mean_q: 0.584576\n",
      "  9754/50000: episode: 181, duration: 0.129s, episode steps: 39, steps per second: 303, episode reward: 0.881, mean reward: 0.023 [-0.005, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.103 [-0.170, 0.494], loss: 0.000440, mean_absolute_error: 0.467568, mean_q: 0.595107\n",
      "  9770/50000: episode: 182, duration: 0.051s, episode steps: 16, steps per second: 311, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.044 [-0.190, 0.090], loss: 0.000051, mean_absolute_error: 0.471151, mean_q: 0.624845\n",
      "  9812/50000: episode: 183, duration: 0.127s, episode steps: 42, steps per second: 331, episode reward: 0.867, mean reward: 0.021 [-0.005, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.106 [-0.543, 0.190], loss: 0.000172, mean_absolute_error: 0.466602, mean_q: 0.596343\n",
      "  9858/50000: episode: 184, duration: 0.187s, episode steps: 46, steps per second: 246, episode reward: 0.873, mean reward: 0.019 [-0.005, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.090 [-0.477, 0.180], loss: 0.000176, mean_absolute_error: 0.462462, mean_q: 0.582540\n",
      "  9914/50000: episode: 185, duration: 0.189s, episode steps: 56, steps per second: 296, episode reward: 0.685, mean reward: 0.012 [-0.010, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.204 [-0.270, 0.955], loss: 0.000200, mean_absolute_error: 0.466333, mean_q: 0.595767\n",
      "  9932/50000: episode: 186, duration: 0.060s, episode steps: 18, steps per second: 302, episode reward: 0.971, mean reward: 0.054 [-0.002, 1.000], mean action: 0.556 [0.000, 2.000], mean observation: 0.050 [-0.100, 0.213], loss: 0.000586, mean_absolute_error: 0.471369, mean_q: 0.598208\n",
      "  9985/50000: episode: 187, duration: 0.160s, episode steps: 53, steps per second: 331, episode reward: 0.842, mean reward: 0.016 [-0.005, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.109 [-0.497, 0.120], loss: 0.000279, mean_absolute_error: 0.464589, mean_q: 0.582619\n",
      " 10035/50000: episode: 188, duration: 0.147s, episode steps: 50, steps per second: 341, episode reward: 0.807, mean reward: 0.016 [-0.007, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.137 [-0.190, 0.661], loss: 0.000367, mean_absolute_error: 0.463759, mean_q: 0.579365\n",
      " 10068/50000: episode: 189, duration: 0.099s, episode steps: 33, steps per second: 332, episode reward: 0.926, mean reward: 0.028 [-0.003, 1.000], mean action: 1.242 [0.000, 2.000], mean observation: -0.077 [-0.338, 0.130], loss: 0.000456, mean_absolute_error: 0.472138, mean_q: 0.608241\n",
      " 10120/50000: episode: 190, duration: 0.155s, episode steps: 52, steps per second: 335, episode reward: 0.717, mean reward: 0.014 [-0.009, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.192 [-0.280, 0.923], loss: 0.000295, mean_absolute_error: 0.468887, mean_q: 0.606952\n",
      " 10172/50000: episode: 191, duration: 0.158s, episode steps: 52, steps per second: 330, episode reward: 0.776, mean reward: 0.015 [-0.007, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.155 [-0.731, 0.220], loss: 0.000184, mean_absolute_error: 0.465702, mean_q: 0.590040\n",
      " 10195/50000: episode: 192, duration: 0.103s, episode steps: 23, steps per second: 224, episode reward: 0.960, mean reward: 0.042 [-0.002, 1.000], mean action: 1.217 [0.000, 2.000], mean observation: -0.055 [-0.248, 0.100], loss: 0.000452, mean_absolute_error: 0.474466, mean_q: 0.627100\n",
      " 10208/50000: episode: 193, duration: 0.060s, episode steps: 13, steps per second: 217, episode reward: 0.983, mean reward: 0.076 [-0.002, 1.000], mean action: 1.538 [0.000, 2.000], mean observation: -0.041 [-0.167, 0.080], loss: 0.000078, mean_absolute_error: 0.473819, mean_q: 0.610493\n",
      " 10275/50000: episode: 194, duration: 0.300s, episode steps: 67, steps per second: 223, episode reward: 0.647, mean reward: 0.010 [-0.010, 1.000], mean action: 1.045 [0.000, 2.000], mean observation: -0.198 [-0.981, 0.230], loss: 0.000202, mean_absolute_error: 0.464713, mean_q: 0.590300\n",
      " 10312/50000: episode: 195, duration: 0.148s, episode steps: 37, steps per second: 251, episode reward: 0.901, mean reward: 0.024 [-0.005, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.074 [-0.190, 0.467], loss: 0.000580, mean_absolute_error: 0.471826, mean_q: 0.594412\n",
      " 10313/50000: episode: 196, duration: 0.007s, episode steps: 1, steps per second: 139, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.010, 0.082], loss: 0.000066, mean_absolute_error: 0.448841, mean_q: 0.574539\n",
      " 10339/50000: episode: 197, duration: 0.097s, episode steps: 26, steps per second: 267, episode reward: 0.967, mean reward: 0.037 [-0.002, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.021 [-0.221, 0.120], loss: 0.000123, mean_absolute_error: 0.471482, mean_q: 0.595580\n",
      " 10346/50000: episode: 198, duration: 0.032s, episode steps: 7, steps per second: 221, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.039 [-0.040, 0.119], loss: 0.000071, mean_absolute_error: 0.476862, mean_q: 0.607870\n",
      " 10407/50000: episode: 199, duration: 0.190s, episode steps: 61, steps per second: 321, episode reward: 0.709, mean reward: 0.012 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.167 [-0.270, 0.915], loss: 0.000045, mean_absolute_error: 0.471143, mean_q: 0.620390\n",
      " 10464/50000: episode: 200, duration: 0.171s, episode steps: 57, steps per second: 333, episode reward: 0.769, mean reward: 0.013 [-0.008, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.135 [-0.240, 0.766], loss: 0.000266, mean_absolute_error: 0.471289, mean_q: 0.608662\n",
      " 10515/50000: episode: 201, duration: 0.245s, episode steps: 51, steps per second: 209, episode reward: 0.770, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.158 [-0.250, 0.791], loss: 0.000122, mean_absolute_error: 0.467485, mean_q: 0.602267\n",
      " 10524/50000: episode: 202, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 1.778 [1.000, 2.000], mean observation: -0.039 [-0.139, 0.070], loss: 0.000050, mean_absolute_error: 0.458664, mean_q: 0.598338\n",
      " 10544/50000: episode: 203, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 0.967, mean reward: 0.048 [-0.002, 1.000], mean action: 1.350 [0.000, 2.000], mean observation: -0.051 [-0.231, 0.100], loss: 0.000049, mean_absolute_error: 0.458754, mean_q: 0.600702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10578/50000: episode: 204, duration: 0.143s, episode steps: 34, steps per second: 239, episode reward: 0.908, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.086 [-0.150, 0.419], loss: 0.000033, mean_absolute_error: 0.459831, mean_q: 0.595874\n",
      " 10642/50000: episode: 205, duration: 0.328s, episode steps: 64, steps per second: 195, episode reward: 0.659, mean reward: 0.010 [-0.010, 1.000], mean action: 1.109 [0.000, 2.000], mean observation: -0.199 [-0.971, 0.240], loss: 0.000066, mean_absolute_error: 0.464498, mean_q: 0.599360\n",
      " 10704/50000: episode: 206, duration: 0.255s, episode steps: 62, steps per second: 243, episode reward: 0.705, mean reward: 0.011 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.170 [-0.886, 0.230], loss: 0.000111, mean_absolute_error: 0.468860, mean_q: 0.613542\n",
      " 10749/50000: episode: 207, duration: 0.146s, episode steps: 45, steps per second: 308, episode reward: 0.836, mean reward: 0.019 [-0.006, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.127 [-0.200, 0.606], loss: 0.000325, mean_absolute_error: 0.463062, mean_q: 0.599538\n",
      " 10805/50000: episode: 208, duration: 0.200s, episode steps: 56, steps per second: 281, episode reward: 0.710, mean reward: 0.013 [-0.009, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.182 [-0.270, 0.918], loss: 0.000101, mean_absolute_error: 0.467426, mean_q: 0.613950\n",
      " 10873/50000: episode: 209, duration: 0.281s, episode steps: 68, steps per second: 242, episode reward: 0.629, mean reward: 0.009 [-0.009, 1.000], mean action: 1.118 [0.000, 2.000], mean observation: -0.212 [-0.932, 0.230], loss: 0.000334, mean_absolute_error: 0.462740, mean_q: 0.589813\n",
      " 10929/50000: episode: 210, duration: 0.288s, episode steps: 56, steps per second: 194, episode reward: 0.734, mean reward: 0.013 [-0.009, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.162 [-0.260, 0.873], loss: 0.000127, mean_absolute_error: 0.466348, mean_q: 0.612618\n",
      " 10945/50000: episode: 211, duration: 0.190s, episode steps: 16, steps per second: 84, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.049 [-0.182, 0.090], loss: 0.000047, mean_absolute_error: 0.466910, mean_q: 0.609936\n",
      " 10971/50000: episode: 212, duration: 0.167s, episode steps: 26, steps per second: 156, episode reward: 0.961, mean reward: 0.037 [-0.002, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.037 [-0.100, 0.239], loss: 0.000040, mean_absolute_error: 0.466564, mean_q: 0.612078\n",
      " 11033/50000: episode: 213, duration: 0.362s, episode steps: 62, steps per second: 171, episode reward: 0.797, mean reward: 0.013 [-0.005, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.132 [-0.498, 0.130], loss: 0.000441, mean_absolute_error: 0.465788, mean_q: 0.610357\n",
      " 11070/50000: episode: 214, duration: 0.258s, episode steps: 37, steps per second: 144, episode reward: 0.911, mean reward: 0.025 [-0.004, 1.000], mean action: 0.838 [0.000, 2.000], mean observation: 0.082 [-0.140, 0.388], loss: 0.000183, mean_absolute_error: 0.469791, mean_q: 0.627517\n",
      " 11114/50000: episode: 215, duration: 0.226s, episode steps: 44, steps per second: 195, episode reward: 0.863, mean reward: 0.020 [-0.005, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.099 [-0.190, 0.543], loss: 0.000046, mean_absolute_error: 0.462710, mean_q: 0.611060\n",
      " 11146/50000: episode: 216, duration: 0.131s, episode steps: 32, steps per second: 244, episode reward: 0.937, mean reward: 0.029 [-0.003, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.058 [-0.110, 0.309], loss: 0.000042, mean_absolute_error: 0.465153, mean_q: 0.622576\n",
      " 11172/50000: episode: 217, duration: 0.119s, episode steps: 26, steps per second: 219, episode reward: 0.960, mean reward: 0.037 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.032 [-0.258, 0.120], loss: 0.000062, mean_absolute_error: 0.470629, mean_q: 0.621665\n",
      " 11238/50000: episode: 218, duration: 0.281s, episode steps: 66, steps per second: 235, episode reward: 0.693, mean reward: 0.011 [-0.009, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.162 [-0.923, 0.240], loss: 0.000310, mean_absolute_error: 0.468202, mean_q: 0.625306\n",
      " 11294/50000: episode: 219, duration: 0.221s, episode steps: 56, steps per second: 254, episode reward: 0.746, mean reward: 0.013 [-0.008, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.162 [-0.240, 0.787], loss: 0.000044, mean_absolute_error: 0.465320, mean_q: 0.624135\n",
      " 11295/50000: episode: 220, duration: 0.008s, episode steps: 1, steps per second: 128, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.035 [-0.081, 0.010], loss: 0.000018, mean_absolute_error: 0.475260, mean_q: 0.638500\n",
      " 11312/50000: episode: 221, duration: 0.082s, episode steps: 17, steps per second: 208, episode reward: 0.973, mean reward: 0.057 [-0.002, 1.000], mean action: 1.529 [0.000, 2.000], mean observation: -0.050 [-0.208, 0.100], loss: 0.000033, mean_absolute_error: 0.470102, mean_q: 0.647725\n",
      " 11352/50000: episode: 222, duration: 0.191s, episode steps: 40, steps per second: 209, episode reward: 0.887, mean reward: 0.022 [-0.005, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.096 [-0.140, 0.459], loss: 0.000051, mean_absolute_error: 0.463421, mean_q: 0.617280\n",
      " 11366/50000: episode: 223, duration: 0.057s, episode steps: 14, steps per second: 244, episode reward: 0.981, mean reward: 0.070 [-0.002, 1.000], mean action: 1.357 [0.000, 2.000], mean observation: -0.042 [-0.176, 0.090], loss: 0.000902, mean_absolute_error: 0.464883, mean_q: 0.629405\n",
      " 11427/50000: episode: 224, duration: 0.401s, episode steps: 61, steps per second: 152, episode reward: 0.712, mean reward: 0.012 [-0.009, 1.000], mean action: 0.918 [0.000, 2.000], mean observation: 0.171 [-0.260, 0.897], loss: 0.000256, mean_absolute_error: 0.471029, mean_q: 0.641372\n",
      " 11485/50000: episode: 225, duration: 0.306s, episode steps: 58, steps per second: 189, episode reward: 0.768, mean reward: 0.013 [-0.008, 1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.131 [-0.758, 0.200], loss: 0.000229, mean_absolute_error: 0.465479, mean_q: 0.621992\n",
      " 11486/50000: episode: 226, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.009 [-0.027, 0.010], loss: 0.000066, mean_absolute_error: 0.434906, mean_q: 0.438931\n",
      " 11537/50000: episode: 227, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 0.795, mean reward: 0.016 [-0.007, 1.000], mean action: 0.843 [0.000, 2.000], mean observation: 0.142 [-0.210, 0.708], loss: 0.000453, mean_absolute_error: 0.468848, mean_q: 0.639626\n",
      " 11571/50000: episode: 228, duration: 0.177s, episode steps: 34, steps per second: 193, episode reward: 0.918, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.071 [-0.150, 0.394], loss: 0.000165, mean_absolute_error: 0.476026, mean_q: 0.643406\n",
      " 11628/50000: episode: 229, duration: 0.235s, episode steps: 57, steps per second: 243, episode reward: 0.804, mean reward: 0.014 [-0.006, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.118 [-0.636, 0.180], loss: 0.000105, mean_absolute_error: 0.463458, mean_q: 0.622931\n",
      " 11641/50000: episode: 230, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 0.984, mean reward: 0.076 [-0.002, 1.000], mean action: 1.538 [1.000, 2.000], mean observation: -0.042 [-0.159, 0.070], loss: 0.000356, mean_absolute_error: 0.461259, mean_q: 0.607535\n",
      " 11696/50000: episode: 231, duration: 0.282s, episode steps: 55, steps per second: 195, episode reward: 0.737, mean reward: 0.013 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.175 [-0.804, 0.210], loss: 0.000313, mean_absolute_error: 0.463118, mean_q: 0.612570\n",
      " 11729/50000: episode: 232, duration: 0.142s, episode steps: 33, steps per second: 232, episode reward: 0.908, mean reward: 0.028 [-0.004, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.089 [-0.170, 0.435], loss: 0.000054, mean_absolute_error: 0.468753, mean_q: 0.628956\n",
      " 11730/50000: episode: 233, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.014 [0.000, 0.029], loss: 0.000102, mean_absolute_error: 0.482709, mean_q: 0.585499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11784/50000: episode: 234, duration: 0.300s, episode steps: 54, steps per second: 180, episode reward: 0.790, mean reward: 0.015 [-0.007, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.132 [-0.724, 0.220], loss: 0.000050, mean_absolute_error: 0.464331, mean_q: 0.627544\n",
      " 11799/50000: episode: 235, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 1.600 [1.000, 2.000], mean observation: -0.042 [-0.185, 0.090], loss: 0.000071, mean_absolute_error: 0.459557, mean_q: 0.601711\n",
      " 11817/50000: episode: 236, duration: 0.084s, episode steps: 18, steps per second: 215, episode reward: 0.972, mean reward: 0.054 [-0.002, 1.000], mean action: 1.444 [0.000, 2.000], mean observation: -0.048 [-0.215, 0.090], loss: 0.000162, mean_absolute_error: 0.468739, mean_q: 0.646827\n",
      " 11845/50000: episode: 237, duration: 0.121s, episode steps: 28, steps per second: 232, episode reward: 0.949, mean reward: 0.034 [-0.003, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.062 [-0.261, 0.100], loss: 0.000118, mean_absolute_error: 0.473730, mean_q: 0.642593\n",
      " 11885/50000: episode: 238, duration: 0.225s, episode steps: 40, steps per second: 178, episode reward: 0.881, mean reward: 0.022 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.101 [-0.170, 0.486], loss: 0.000063, mean_absolute_error: 0.467867, mean_q: 0.643105\n",
      " 11926/50000: episode: 239, duration: 0.222s, episode steps: 41, steps per second: 184, episode reward: 0.881, mean reward: 0.021 [-0.005, 1.000], mean action: 1.171 [0.000, 2.000], mean observation: -0.102 [-0.464, 0.150], loss: 0.000127, mean_absolute_error: 0.466809, mean_q: 0.629000\n",
      " 11947/50000: episode: 240, duration: 0.163s, episode steps: 21, steps per second: 129, episode reward: 0.966, mean reward: 0.046 [-0.002, 1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.052 [-0.232, 0.100], loss: 0.000483, mean_absolute_error: 0.468625, mean_q: 0.647323\n",
      " 11948/50000: episode: 241, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.039 [-0.089, 0.010], loss: 0.000059, mean_absolute_error: 0.491430, mean_q: 0.686768\n",
      " 11959/50000: episode: 242, duration: 0.055s, episode steps: 11, steps per second: 198, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 1.545 [0.000, 2.000], mean observation: -0.041 [-0.151, 0.070], loss: 0.000085, mean_absolute_error: 0.473029, mean_q: 0.640050\n",
      " 12000/50000: episode: 243, duration: 0.233s, episode steps: 41, steps per second: 176, episode reward: 0.870, mean reward: 0.021 [-0.005, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.103 [-0.190, 0.524], loss: 0.000049, mean_absolute_error: 0.463359, mean_q: 0.623139\n",
      " 12019/50000: episode: 244, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 0.977, mean reward: 0.051 [-0.002, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.048 [-0.040, 0.157], loss: 0.000049, mean_absolute_error: 0.467578, mean_q: 0.626931\n",
      " 12070/50000: episode: 245, duration: 0.253s, episode steps: 51, steps per second: 202, episode reward: 0.763, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.156 [-0.270, 0.817], loss: 0.000341, mean_absolute_error: 0.461243, mean_q: 0.614707\n",
      " 12100/50000: episode: 246, duration: 0.123s, episode steps: 30, steps per second: 244, episode reward: 0.925, mean reward: 0.031 [-0.004, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.079 [-0.150, 0.386], loss: 0.000055, mean_absolute_error: 0.467040, mean_q: 0.647336\n",
      " 12101/50000: episode: 247, duration: 0.008s, episode steps: 1, steps per second: 124, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.004 [-0.008, 0.000], loss: 0.000058, mean_absolute_error: 0.474098, mean_q: 0.571524\n",
      " 12147/50000: episode: 248, duration: 0.224s, episode steps: 46, steps per second: 206, episode reward: 0.833, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.125 [-0.625, 0.190], loss: 0.000075, mean_absolute_error: 0.462873, mean_q: 0.627757\n",
      " 12173/50000: episode: 249, duration: 0.116s, episode steps: 26, steps per second: 224, episode reward: 0.940, mean reward: 0.036 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.069 [-0.160, 0.341], loss: 0.000219, mean_absolute_error: 0.467404, mean_q: 0.638482\n",
      " 12189/50000: episode: 250, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 0.975, mean reward: 0.061 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.046 [-0.205, 0.100], loss: 0.000115, mean_absolute_error: 0.463115, mean_q: 0.631731\n",
      " 12232/50000: episode: 251, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 0.850, mean reward: 0.020 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.109 [-0.220, 0.595], loss: 0.000563, mean_absolute_error: 0.468649, mean_q: 0.636150\n",
      " 12271/50000: episode: 252, duration: 0.235s, episode steps: 39, steps per second: 166, episode reward: 0.892, mean reward: 0.023 [-0.005, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.082 [-0.200, 0.488], loss: 0.000089, mean_absolute_error: 0.464148, mean_q: 0.634081\n",
      " 12286/50000: episode: 253, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.045 [-0.181, 0.090], loss: 0.000053, mean_absolute_error: 0.472245, mean_q: 0.659072\n",
      " 12355/50000: episode: 254, duration: 0.463s, episode steps: 69, steps per second: 149, episode reward: 0.654, mean reward: 0.009 [-0.009, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.186 [-0.935, 0.200], loss: 0.000123, mean_absolute_error: 0.465934, mean_q: 0.635681\n",
      " 12409/50000: episode: 255, duration: 0.507s, episode steps: 54, steps per second: 107, episode reward: 0.718, mean reward: 0.013 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.185 [-0.240, 0.903], loss: 0.000459, mean_absolute_error: 0.467328, mean_q: 0.637966\n",
      " 12435/50000: episode: 256, duration: 0.213s, episode steps: 26, steps per second: 122, episode reward: 0.950, mean reward: 0.037 [-0.003, 1.000], mean action: 0.692 [0.000, 2.000], mean observation: 0.061 [-0.110, 0.290], loss: 0.000104, mean_absolute_error: 0.462178, mean_q: 0.621862\n",
      " 12448/50000: episode: 257, duration: 0.146s, episode steps: 13, steps per second: 89, episode reward: 0.984, mean reward: 0.076 [-0.002, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.043 [-0.060, 0.155], loss: 0.000049, mean_absolute_error: 0.467994, mean_q: 0.637416\n",
      " 12449/50000: episode: 258, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.016 [-0.031, 0.000], loss: 0.000023, mean_absolute_error: 0.440812, mean_q: 0.646132\n",
      " 12476/50000: episode: 259, duration: 0.270s, episode steps: 27, steps per second: 100, episode reward: 0.942, mean reward: 0.035 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.059 [-0.150, 0.334], loss: 0.000072, mean_absolute_error: 0.456314, mean_q: 0.621558\n",
      " 12492/50000: episode: 260, duration: 0.103s, episode steps: 16, steps per second: 155, episode reward: 0.982, mean reward: 0.061 [-0.001, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.046 [-0.140, 0.050], loss: 0.000135, mean_absolute_error: 0.470480, mean_q: 0.637748\n",
      " 12537/50000: episode: 261, duration: 0.384s, episode steps: 45, steps per second: 117, episode reward: 0.815, mean reward: 0.018 [-0.007, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.132 [-0.250, 0.706], loss: 0.000272, mean_absolute_error: 0.468358, mean_q: 0.638769\n",
      " 12559/50000: episode: 262, duration: 0.181s, episode steps: 22, steps per second: 121, episode reward: 0.970, mean reward: 0.044 [-0.002, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.050 [-0.182, 0.060], loss: 0.000103, mean_absolute_error: 0.462401, mean_q: 0.633574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12560/50000: episode: 263, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.030 [-0.071, 0.010], loss: 0.000037, mean_absolute_error: 0.487637, mean_q: 0.725005\n",
      " 12577/50000: episode: 264, duration: 0.121s, episode steps: 17, steps per second: 140, episode reward: 0.975, mean reward: 0.057 [-0.002, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: 0.048 [-0.070, 0.191], loss: 0.000058, mean_absolute_error: 0.463709, mean_q: 0.614548\n",
      " 12589/50000: episode: 265, duration: 0.119s, episode steps: 12, steps per second: 101, episode reward: 0.985, mean reward: 0.082 [-0.002, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.039 [-0.070, 0.156], loss: 0.000169, mean_absolute_error: 0.456147, mean_q: 0.628594\n",
      " 12659/50000: episode: 266, duration: 0.620s, episode steps: 70, steps per second: 113, episode reward: 0.728, mean reward: 0.010 [-0.008, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.146 [-0.772, 0.170], loss: 0.000111, mean_absolute_error: 0.462114, mean_q: 0.632851\n",
      " 12698/50000: episode: 267, duration: 0.490s, episode steps: 39, steps per second: 80, episode reward: 0.860, mean reward: 0.022 [-0.006, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.120 [-0.220, 0.561], loss: 0.000182, mean_absolute_error: 0.460579, mean_q: 0.625313\n",
      " 12782/50000: episode: 268, duration: 0.736s, episode steps: 84, steps per second: 114, episode reward: 0.802, mean reward: 0.010 [-0.005, 1.000], mean action: 0.905 [0.000, 2.000], mean observation: -0.026 [-0.516, 0.180], loss: 0.000272, mean_absolute_error: 0.468701, mean_q: 0.650942\n",
      " 12843/50000: episode: 269, duration: 0.429s, episode steps: 61, steps per second: 142, episode reward: 0.729, mean reward: 0.012 [-0.008, 1.000], mean action: 1.066 [0.000, 2.000], mean observation: -0.165 [-0.814, 0.190], loss: 0.000128, mean_absolute_error: 0.451531, mean_q: 0.612352\n",
      " 12893/50000: episode: 270, duration: 0.358s, episode steps: 50, steps per second: 140, episode reward: 0.837, mean reward: 0.017 [-0.005, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.128 [-0.458, 0.140], loss: 0.000070, mean_absolute_error: 0.470703, mean_q: 0.658189\n",
      " 12906/50000: episode: 271, duration: 0.089s, episode steps: 13, steps per second: 146, episode reward: 0.986, mean reward: 0.076 [-0.001, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.046 [-0.126, 0.030], loss: 0.000051, mean_absolute_error: 0.464414, mean_q: 0.653653\n",
      " 12915/50000: episode: 272, duration: 0.112s, episode steps: 9, steps per second: 81, episode reward: 0.991, mean reward: 0.110 [-0.001, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.040 [-0.131, 0.060], loss: 0.000046, mean_absolute_error: 0.469239, mean_q: 0.647733\n",
      " 12940/50000: episode: 273, duration: 0.225s, episode steps: 25, steps per second: 111, episode reward: 0.943, mean reward: 0.038 [-0.003, 1.000], mean action: 0.640 [0.000, 2.000], mean observation: 0.066 [-0.160, 0.336], loss: 0.000237, mean_absolute_error: 0.470410, mean_q: 0.658449\n",
      " 12941/50000: episode: 274, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [-0.010, 0.050], loss: 0.000029, mean_absolute_error: 0.487047, mean_q: 0.701592\n",
      " 12986/50000: episode: 275, duration: 0.301s, episode steps: 45, steps per second: 149, episode reward: 0.854, mean reward: 0.019 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.106 [-0.200, 0.555], loss: 0.000051, mean_absolute_error: 0.463493, mean_q: 0.648782\n",
      " 13048/50000: episode: 276, duration: 0.501s, episode steps: 62, steps per second: 124, episode reward: 0.650, mean reward: 0.010 [-0.010, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.211 [-0.976, 0.200], loss: 0.000124, mean_absolute_error: 0.462328, mean_q: 0.642376\n",
      " 13082/50000: episode: 277, duration: 0.247s, episode steps: 34, steps per second: 138, episode reward: 0.918, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.071 [-0.150, 0.396], loss: 0.000036, mean_absolute_error: 0.466818, mean_q: 0.657831\n",
      " 13149/50000: episode: 278, duration: 0.426s, episode steps: 67, steps per second: 157, episode reward: 0.658, mean reward: 0.010 [-0.009, 1.000], mean action: 1.134 [0.000, 2.000], mean observation: -0.190 [-0.906, 0.190], loss: 0.000052, mean_absolute_error: 0.464919, mean_q: 0.648449\n",
      " 13173/50000: episode: 279, duration: 0.153s, episode steps: 24, steps per second: 157, episode reward: 0.968, mean reward: 0.040 [-0.002, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.044 [-0.110, 0.176], loss: 0.000075, mean_absolute_error: 0.473407, mean_q: 0.655552\n",
      " 13216/50000: episode: 280, duration: 0.204s, episode steps: 43, steps per second: 211, episode reward: 0.862, mean reward: 0.020 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.100 [-0.210, 0.563], loss: 0.000082, mean_absolute_error: 0.463936, mean_q: 0.642447\n",
      " 13255/50000: episode: 281, duration: 0.154s, episode steps: 39, steps per second: 254, episode reward: 0.899, mean reward: 0.023 [-0.004, 1.000], mean action: 1.103 [0.000, 2.000], mean observation: -0.093 [-0.389, 0.110], loss: 0.000050, mean_absolute_error: 0.461244, mean_q: 0.642223\n",
      " 13296/50000: episode: 282, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 0.858, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.121 [-0.532, 0.160], loss: 0.000285, mean_absolute_error: 0.463572, mean_q: 0.650077\n",
      " 13350/50000: episode: 283, duration: 0.248s, episode steps: 54, steps per second: 218, episode reward: 0.776, mean reward: 0.014 [-0.007, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.144 [-0.240, 0.720], loss: 0.000051, mean_absolute_error: 0.460712, mean_q: 0.642110\n",
      " 13396/50000: episode: 284, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 0.790, mean reward: 0.017 [-0.007, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.157 [-0.250, 0.749], loss: 0.000046, mean_absolute_error: 0.465120, mean_q: 0.645030\n",
      " 13410/50000: episode: 285, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 0.983, mean reward: 0.070 [-0.001, 1.000], mean action: 0.643 [0.000, 2.000], mean observation: 0.047 [-0.050, 0.150], loss: 0.000108, mean_absolute_error: 0.465105, mean_q: 0.633807\n",
      " 13439/50000: episode: 286, duration: 0.140s, episode steps: 29, steps per second: 206, episode reward: 0.933, mean reward: 0.032 [-0.003, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.073 [-0.150, 0.340], loss: 0.000373, mean_absolute_error: 0.473047, mean_q: 0.670333\n",
      " 13460/50000: episode: 287, duration: 0.100s, episode steps: 21, steps per second: 211, episode reward: 0.967, mean reward: 0.046 [-0.002, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.057 [-0.204, 0.070], loss: 0.000078, mean_absolute_error: 0.458101, mean_q: 0.631575\n",
      " 13483/50000: episode: 288, duration: 0.109s, episode steps: 23, steps per second: 212, episode reward: 0.961, mean reward: 0.042 [-0.002, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.046 [-0.130, 0.249], loss: 0.000045, mean_absolute_error: 0.464587, mean_q: 0.654172\n",
      " 13527/50000: episode: 289, duration: 0.182s, episode steps: 44, steps per second: 242, episode reward: 0.843, mean reward: 0.019 [-0.006, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.117 [-0.220, 0.615], loss: 0.000042, mean_absolute_error: 0.463385, mean_q: 0.642856\n",
      " 13618/50000: episode: 290, duration: 0.463s, episode steps: 91, steps per second: 197, episode reward: 0.614, mean reward: 0.007 [-0.010, 1.000], mean action: 0.901 [0.000, 2.000], mean observation: -0.124 [-0.967, 0.200], loss: 0.000108, mean_absolute_error: 0.463573, mean_q: 0.649160\n",
      " 13662/50000: episode: 291, duration: 0.274s, episode steps: 44, steps per second: 160, episode reward: 0.842, mean reward: 0.019 [-0.006, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.114 [-0.609, 0.190], loss: 0.000137, mean_absolute_error: 0.464594, mean_q: 0.652145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13725/50000: episode: 292, duration: 0.273s, episode steps: 63, steps per second: 231, episode reward: 0.720, mean reward: 0.011 [-0.008, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.159 [-0.818, 0.180], loss: 0.000131, mean_absolute_error: 0.468806, mean_q: 0.653251\n",
      " 13747/50000: episode: 293, duration: 0.130s, episode steps: 22, steps per second: 170, episode reward: 0.962, mean reward: 0.044 [-0.002, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.054 [-0.110, 0.240], loss: 0.000053, mean_absolute_error: 0.456679, mean_q: 0.639463\n",
      " 13801/50000: episode: 294, duration: 0.282s, episode steps: 54, steps per second: 192, episode reward: 0.718, mean reward: 0.013 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.188 [-0.270, 0.868], loss: 0.000066, mean_absolute_error: 0.465736, mean_q: 0.652224\n",
      " 13857/50000: episode: 295, duration: 0.295s, episode steps: 56, steps per second: 190, episode reward: 0.683, mean reward: 0.012 [-0.010, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.204 [-0.270, 0.985], loss: 0.000063, mean_absolute_error: 0.470476, mean_q: 0.666720\n",
      " 13880/50000: episode: 296, duration: 0.187s, episode steps: 23, steps per second: 123, episode reward: 0.953, mean reward: 0.041 [-0.003, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.061 [-0.140, 0.296], loss: 0.000261, mean_absolute_error: 0.470120, mean_q: 0.654525\n",
      " 13949/50000: episode: 297, duration: 0.370s, episode steps: 69, steps per second: 187, episode reward: 0.764, mean reward: 0.011 [-0.007, 1.000], mean action: 1.087 [0.000, 2.000], mean observation: -0.130 [-0.670, 0.180], loss: 0.000110, mean_absolute_error: 0.463160, mean_q: 0.657598\n",
      " 13950/50000: episode: 298, duration: 0.008s, episode steps: 1, steps per second: 130, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.006 [-0.013, 0.000], loss: 0.000061, mean_absolute_error: 0.459334, mean_q: 0.670768\n",
      " 14010/50000: episode: 299, duration: 0.382s, episode steps: 60, steps per second: 157, episode reward: 0.687, mean reward: 0.011 [-0.009, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.197 [-0.864, 0.190], loss: 0.000110, mean_absolute_error: 0.463280, mean_q: 0.652256\n",
      " 14057/50000: episode: 300, duration: 0.213s, episode steps: 47, steps per second: 220, episode reward: 0.855, mean reward: 0.018 [-0.006, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.090 [-0.564, 0.190], loss: 0.000044, mean_absolute_error: 0.457478, mean_q: 0.643547\n",
      " 14058/50000: episode: 301, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.035 [-0.081, 0.010], loss: 0.000011, mean_absolute_error: 0.491210, mean_q: 0.731045\n",
      " 14100/50000: episode: 302, duration: 0.383s, episode steps: 42, steps per second: 110, episode reward: 0.858, mean reward: 0.020 [-0.006, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.106 [-0.220, 0.585], loss: 0.000048, mean_absolute_error: 0.462571, mean_q: 0.643321\n",
      " 14149/50000: episode: 303, duration: 0.368s, episode steps: 49, steps per second: 133, episode reward: 0.780, mean reward: 0.016 [-0.008, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.156 [-0.240, 0.771], loss: 0.000124, mean_absolute_error: 0.467497, mean_q: 0.664017\n",
      " 14168/50000: episode: 304, duration: 0.124s, episode steps: 19, steps per second: 153, episode reward: 0.976, mean reward: 0.051 [-0.002, 1.000], mean action: 1.263 [0.000, 2.000], mean observation: -0.050 [-0.156, 0.060], loss: 0.000044, mean_absolute_error: 0.461418, mean_q: 0.656588\n",
      " 14198/50000: episode: 305, duration: 0.325s, episode steps: 30, steps per second: 92, episode reward: 0.947, mean reward: 0.032 [-0.003, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.060 [-0.256, 0.110], loss: 0.000459, mean_absolute_error: 0.461606, mean_q: 0.648174\n",
      " 14256/50000: episode: 306, duration: 0.457s, episode steps: 58, steps per second: 127, episode reward: 0.810, mean reward: 0.014 [-0.005, 1.000], mean action: 1.034 [0.000, 2.000], mean observation: -0.126 [-0.541, 0.170], loss: 0.000054, mean_absolute_error: 0.458797, mean_q: 0.646445\n",
      " 14316/50000: episode: 307, duration: 0.443s, episode steps: 60, steps per second: 135, episode reward: 0.757, mean reward: 0.013 [-0.007, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.150 [-0.733, 0.190], loss: 0.000039, mean_absolute_error: 0.462445, mean_q: 0.658124\n",
      " 14331/50000: episode: 308, duration: 0.108s, episode steps: 15, steps per second: 138, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 0.467 [0.000, 2.000], mean observation: 0.043 [-0.090, 0.187], loss: 0.000393, mean_absolute_error: 0.464336, mean_q: 0.650571\n",
      " 14390/50000: episode: 309, duration: 0.393s, episode steps: 59, steps per second: 150, episode reward: 0.775, mean reward: 0.013 [-0.006, 1.000], mean action: 1.034 [0.000, 2.000], mean observation: -0.151 [-0.573, 0.170], loss: 0.000081, mean_absolute_error: 0.467753, mean_q: 0.663282\n",
      " 14411/50000: episode: 310, duration: 0.192s, episode steps: 21, steps per second: 109, episode reward: 0.964, mean reward: 0.046 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.057 [-0.100, 0.224], loss: 0.000055, mean_absolute_error: 0.468771, mean_q: 0.673110\n",
      " 14441/50000: episode: 311, duration: 0.150s, episode steps: 30, steps per second: 201, episode reward: 0.922, mean reward: 0.031 [-0.004, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.081 [-0.150, 0.398], loss: 0.000050, mean_absolute_error: 0.470836, mean_q: 0.674074\n",
      " 14496/50000: episode: 312, duration: 0.200s, episode steps: 55, steps per second: 275, episode reward: 0.708, mean reward: 0.013 [-0.009, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.191 [-0.270, 0.933], loss: 0.000054, mean_absolute_error: 0.463553, mean_q: 0.660237\n",
      " 14552/50000: episode: 313, duration: 0.185s, episode steps: 56, steps per second: 303, episode reward: 0.735, mean reward: 0.013 [-0.008, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.175 [-0.795, 0.200], loss: 0.000059, mean_absolute_error: 0.463811, mean_q: 0.656855\n",
      " 14574/50000: episode: 314, duration: 0.091s, episode steps: 22, steps per second: 242, episode reward: 0.960, mean reward: 0.044 [-0.002, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.059 [-0.100, 0.244], loss: 0.000071, mean_absolute_error: 0.460657, mean_q: 0.656252\n",
      " 14592/50000: episode: 315, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 0.971, mean reward: 0.054 [-0.002, 1.000], mean action: 0.556 [0.000, 2.000], mean observation: 0.049 [-0.100, 0.216], loss: 0.000041, mean_absolute_error: 0.458846, mean_q: 0.657647\n",
      " 14644/50000: episode: 316, duration: 0.241s, episode steps: 52, steps per second: 216, episode reward: 0.761, mean reward: 0.015 [-0.008, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.159 [-0.250, 0.820], loss: 0.000185, mean_absolute_error: 0.465358, mean_q: 0.661095\n",
      " 14665/50000: episode: 317, duration: 0.091s, episode steps: 21, steps per second: 230, episode reward: 0.965, mean reward: 0.046 [-0.002, 1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.055 [-0.233, 0.090], loss: 0.000061, mean_absolute_error: 0.464560, mean_q: 0.656128\n",
      " 14689/50000: episode: 318, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 0.955, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.052 [-0.276, 0.110], loss: 0.000034, mean_absolute_error: 0.462462, mean_q: 0.658163\n",
      " 14697/50000: episode: 319, duration: 0.032s, episode steps: 8, steps per second: 248, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.036 [-0.070, 0.129], loss: 0.000084, mean_absolute_error: 0.460400, mean_q: 0.660643\n",
      " 14740/50000: episode: 320, duration: 0.145s, episode steps: 43, steps per second: 296, episode reward: 0.882, mean reward: 0.021 [-0.004, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.100 [-0.404, 0.130], loss: 0.000352, mean_absolute_error: 0.467139, mean_q: 0.666438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14794/50000: episode: 321, duration: 0.183s, episode steps: 54, steps per second: 295, episode reward: 0.734, mean reward: 0.014 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.176 [-0.250, 0.857], loss: 0.000203, mean_absolute_error: 0.462862, mean_q: 0.654264\n",
      " 14846/50000: episode: 322, duration: 0.161s, episode steps: 52, steps per second: 324, episode reward: 0.779, mean reward: 0.015 [-0.007, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.154 [-0.220, 0.710], loss: 0.000071, mean_absolute_error: 0.464165, mean_q: 0.659854\n",
      " 14913/50000: episode: 323, duration: 0.271s, episode steps: 67, steps per second: 247, episode reward: 0.731, mean reward: 0.011 [-0.006, 1.000], mean action: 1.134 [0.000, 2.000], mean observation: -0.161 [-0.569, 0.180], loss: 0.000218, mean_absolute_error: 0.467966, mean_q: 0.668183\n",
      " 14959/50000: episode: 324, duration: 0.179s, episode steps: 46, steps per second: 257, episode reward: 0.848, mean reward: 0.018 [-0.006, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.112 [-0.180, 0.562], loss: 0.000255, mean_absolute_error: 0.469198, mean_q: 0.665817\n",
      " 15003/50000: episode: 325, duration: 0.191s, episode steps: 44, steps per second: 231, episode reward: 0.844, mean reward: 0.019 [-0.006, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.120 [-0.200, 0.603], loss: 0.000059, mean_absolute_error: 0.467453, mean_q: 0.667177\n",
      " 15041/50000: episode: 326, duration: 0.114s, episode steps: 38, steps per second: 334, episode reward: 0.881, mean reward: 0.023 [-0.005, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.107 [-0.170, 0.484], loss: 0.000050, mean_absolute_error: 0.464978, mean_q: 0.663190\n",
      " 15083/50000: episode: 327, duration: 0.128s, episode steps: 42, steps per second: 327, episode reward: 0.863, mean reward: 0.021 [-0.005, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.116 [-0.170, 0.501], loss: 0.000037, mean_absolute_error: 0.468368, mean_q: 0.665120\n",
      " 15120/50000: episode: 328, duration: 0.113s, episode steps: 37, steps per second: 328, episode reward: 0.892, mean reward: 0.024 [-0.005, 1.000], mean action: 0.784 [0.000, 2.000], mean observation: 0.096 [-0.170, 0.474], loss: 0.000045, mean_absolute_error: 0.468075, mean_q: 0.671492\n",
      " 15121/50000: episode: 329, duration: 0.006s, episode steps: 1, steps per second: 155, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.034 [-0.078, 0.010], loss: 0.000010, mean_absolute_error: 0.513144, mean_q: 0.778782\n",
      " 15161/50000: episode: 330, duration: 0.147s, episode steps: 40, steps per second: 272, episode reward: 0.870, mean reward: 0.022 [-0.005, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.111 [-0.519, 0.150], loss: 0.000041, mean_absolute_error: 0.463256, mean_q: 0.660386\n",
      " 15187/50000: episode: 331, duration: 0.112s, episode steps: 26, steps per second: 232, episode reward: 0.952, mean reward: 0.037 [-0.003, 1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.060 [-0.271, 0.110], loss: 0.000030, mean_absolute_error: 0.467992, mean_q: 0.671194\n",
      " 15200/50000: episode: 332, duration: 0.057s, episode steps: 13, steps per second: 227, episode reward: 0.984, mean reward: 0.076 [-0.002, 1.000], mean action: 1.385 [0.000, 2.000], mean observation: -0.042 [-0.161, 0.080], loss: 0.000028, mean_absolute_error: 0.469613, mean_q: 0.662710\n",
      " 15201/50000: episode: 333, duration: 0.008s, episode steps: 1, steps per second: 132, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.041 [-0.010, 0.091], loss: 0.000033, mean_absolute_error: 0.501440, mean_q: 0.748202\n",
      " 15220/50000: episode: 334, duration: 0.071s, episode steps: 19, steps per second: 268, episode reward: 0.967, mean reward: 0.051 [-0.002, 1.000], mean action: 0.579 [0.000, 2.000], mean observation: 0.052 [-0.110, 0.239], loss: 0.000048, mean_absolute_error: 0.474460, mean_q: 0.678762\n",
      " 15249/50000: episode: 335, duration: 0.113s, episode steps: 29, steps per second: 257, episode reward: 0.942, mean reward: 0.032 [-0.003, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.061 [-0.301, 0.110], loss: 0.000073, mean_absolute_error: 0.471299, mean_q: 0.676924\n",
      " 15295/50000: episode: 336, duration: 0.196s, episode steps: 46, steps per second: 234, episode reward: 0.827, mean reward: 0.018 [-0.006, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.130 [-0.210, 0.644], loss: 0.000041, mean_absolute_error: 0.472590, mean_q: 0.675542\n",
      " 15346/50000: episode: 337, duration: 0.189s, episode steps: 51, steps per second: 270, episode reward: 0.786, mean reward: 0.015 [-0.007, 1.000], mean action: 0.843 [0.000, 2.000], mean observation: 0.149 [-0.200, 0.725], loss: 0.000290, mean_absolute_error: 0.463487, mean_q: 0.666406\n",
      " 15379/50000: episode: 338, duration: 0.115s, episode steps: 33, steps per second: 287, episode reward: 0.906, mean reward: 0.027 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.092 [-0.431, 0.140], loss: 0.000075, mean_absolute_error: 0.467035, mean_q: 0.663331\n",
      " 15434/50000: episode: 339, duration: 0.167s, episode steps: 55, steps per second: 330, episode reward: 0.755, mean reward: 0.014 [-0.008, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.163 [-0.210, 0.762], loss: 0.000047, mean_absolute_error: 0.471032, mean_q: 0.674403\n",
      " 15435/50000: episode: 340, duration: 0.007s, episode steps: 1, steps per second: 148, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.033 [-0.077, 0.010], loss: 0.000011, mean_absolute_error: 0.503253, mean_q: 0.766488\n",
      " 15474/50000: episode: 341, duration: 0.137s, episode steps: 39, steps per second: 285, episode reward: 0.869, mean reward: 0.022 [-0.005, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.113 [-0.180, 0.535], loss: 0.000128, mean_absolute_error: 0.475092, mean_q: 0.678607\n",
      " 15506/50000: episode: 342, duration: 0.111s, episode steps: 32, steps per second: 289, episode reward: 0.926, mean reward: 0.029 [-0.003, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.077 [-0.120, 0.339], loss: 0.000061, mean_absolute_error: 0.470308, mean_q: 0.678196\n",
      " 15579/50000: episode: 343, duration: 0.233s, episode steps: 73, steps per second: 313, episode reward: 0.614, mean reward: 0.008 [-0.009, 1.000], mean action: 1.123 [0.000, 2.000], mean observation: -0.211 [-0.879, 0.180], loss: 0.000093, mean_absolute_error: 0.469265, mean_q: 0.671615\n",
      " 15654/50000: episode: 344, duration: 0.259s, episode steps: 75, steps per second: 290, episode reward: 0.558, mean reward: 0.007 [-0.010, 1.000], mean action: 1.067 [0.000, 2.000], mean observation: -0.235 [-0.997, 0.210], loss: 0.000247, mean_absolute_error: 0.466916, mean_q: 0.672025\n",
      " 15718/50000: episode: 345, duration: 0.204s, episode steps: 64, steps per second: 314, episode reward: 0.645, mean reward: 0.010 [-0.010, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.207 [-0.270, 0.996], loss: 0.000035, mean_absolute_error: 0.466715, mean_q: 0.666426\n",
      " 15756/50000: episode: 346, duration: 0.156s, episode steps: 38, steps per second: 244, episode reward: 0.899, mean reward: 0.024 [-0.004, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.081 [-0.435, 0.160], loss: 0.000080, mean_absolute_error: 0.472278, mean_q: 0.679293\n",
      " 15810/50000: episode: 347, duration: 0.255s, episode steps: 54, steps per second: 212, episode reward: 0.764, mean reward: 0.014 [-0.008, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.159 [-0.210, 0.754], loss: 0.000176, mean_absolute_error: 0.473622, mean_q: 0.676643\n",
      " 15811/50000: episode: 348, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [-0.010, 0.099], loss: 0.000066, mean_absolute_error: 0.459275, mean_q: 0.671144\n",
      " 15853/50000: episode: 349, duration: 0.153s, episode steps: 42, steps per second: 275, episode reward: 0.859, mean reward: 0.020 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.115 [-0.527, 0.160], loss: 0.000043, mean_absolute_error: 0.473943, mean_q: 0.680159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15908/50000: episode: 350, duration: 0.171s, episode steps: 55, steps per second: 321, episode reward: 0.756, mean reward: 0.014 [-0.008, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.162 [-0.220, 0.768], loss: 0.000045, mean_absolute_error: 0.470414, mean_q: 0.674627\n",
      " 15909/50000: episode: 351, duration: 0.007s, episode steps: 1, steps per second: 152, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [-0.010, 0.025], loss: 0.000021, mean_absolute_error: 0.455315, mean_q: 0.641007\n",
      " 15922/50000: episode: 352, duration: 0.043s, episode steps: 13, steps per second: 305, episode reward: 0.984, mean reward: 0.076 [-0.001, 1.000], mean action: 1.462 [0.000, 2.000], mean observation: -0.044 [-0.148, 0.080], loss: 0.000048, mean_absolute_error: 0.468418, mean_q: 0.672510\n",
      " 15955/50000: episode: 353, duration: 0.113s, episode steps: 33, steps per second: 291, episode reward: 0.930, mean reward: 0.028 [-0.003, 1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.073 [-0.320, 0.110], loss: 0.000050, mean_absolute_error: 0.471653, mean_q: 0.680146\n",
      " 16015/50000: episode: 354, duration: 0.195s, episode steps: 60, steps per second: 308, episode reward: 0.677, mean reward: 0.011 [-0.010, 1.000], mean action: 0.883 [0.000, 2.000], mean observation: 0.197 [-0.250, 0.964], loss: 0.000051, mean_absolute_error: 0.468006, mean_q: 0.668980\n",
      " 16075/50000: episode: 355, duration: 0.190s, episode steps: 60, steps per second: 316, episode reward: 0.701, mean reward: 0.012 [-0.009, 1.000], mean action: 0.883 [0.000, 2.000], mean observation: 0.183 [-0.250, 0.900], loss: 0.000042, mean_absolute_error: 0.462146, mean_q: 0.660013\n",
      " 16183/50000: episode: 356, duration: 0.391s, episode steps: 108, steps per second: 276, episode reward: 0.398, mean reward: 0.004 [-0.007, 1.000], mean action: 1.083 [0.000, 2.000], mean observation: -0.250 [-0.684, 0.200], loss: 0.000050, mean_absolute_error: 0.470494, mean_q: 0.682116\n",
      " 16219/50000: episode: 357, duration: 0.168s, episode steps: 36, steps per second: 214, episode reward: 0.901, mean reward: 0.025 [-0.004, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.093 [-0.140, 0.432], loss: 0.000438, mean_absolute_error: 0.475444, mean_q: 0.687533\n",
      " 16220/50000: episode: 358, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.037 [-0.085, 0.010], loss: 0.000062, mean_absolute_error: 0.475264, mean_q: 0.678492\n",
      " 16364/50000: episode: 359, duration: 0.501s, episode steps: 144, steps per second: 287, episode reward: 0.175, mean reward: 0.001 [-0.007, 1.000], mean action: 1.062 [0.000, 2.000], mean observation: -0.267 [-0.724, 0.210], loss: 0.000143, mean_absolute_error: 0.468660, mean_q: 0.675099\n",
      " 16410/50000: episode: 360, duration: 0.143s, episode steps: 46, steps per second: 322, episode reward: 0.832, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.119 [-0.619, 0.180], loss: 0.000066, mean_absolute_error: 0.465037, mean_q: 0.674023\n",
      " 16448/50000: episode: 361, duration: 0.145s, episode steps: 38, steps per second: 262, episode reward: 0.884, mean reward: 0.023 [-0.005, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.104 [-0.450, 0.170], loss: 0.000386, mean_absolute_error: 0.470235, mean_q: 0.680849\n",
      " 16458/50000: episode: 362, duration: 0.041s, episode steps: 10, steps per second: 243, episode reward: 0.988, mean reward: 0.099 [-0.001, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.039 [-0.070, 0.146], loss: 0.000103, mean_absolute_error: 0.467357, mean_q: 0.679329\n",
      " 16496/50000: episode: 363, duration: 0.121s, episode steps: 38, steps per second: 315, episode reward: 0.925, mean reward: 0.024 [-0.004, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.035 [-0.390, 0.170], loss: 0.000186, mean_absolute_error: 0.471623, mean_q: 0.682687\n",
      " 16527/50000: episode: 364, duration: 0.093s, episode steps: 31, steps per second: 332, episode reward: 0.930, mean reward: 0.030 [-0.003, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.074 [-0.342, 0.110], loss: 0.000053, mean_absolute_error: 0.474539, mean_q: 0.687735\n",
      " 16588/50000: episode: 365, duration: 0.216s, episode steps: 61, steps per second: 283, episode reward: 0.694, mean reward: 0.011 [-0.008, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.190 [-0.220, 0.848], loss: 0.000041, mean_absolute_error: 0.467458, mean_q: 0.678355\n",
      " 16630/50000: episode: 366, duration: 0.161s, episode steps: 42, steps per second: 261, episode reward: 0.875, mean reward: 0.021 [-0.005, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.103 [-0.160, 0.487], loss: 0.000043, mean_absolute_error: 0.463953, mean_q: 0.671213\n",
      " 16631/50000: episode: 367, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.015 [-0.040, 0.010], loss: 0.000112, mean_absolute_error: 0.487038, mean_q: 0.700527\n",
      " 16688/50000: episode: 368, duration: 0.183s, episode steps: 57, steps per second: 312, episode reward: 0.741, mean reward: 0.013 [-0.008, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.150 [-0.830, 0.220], loss: 0.000038, mean_absolute_error: 0.464631, mean_q: 0.663068\n",
      " 16715/50000: episode: 369, duration: 0.088s, episode steps: 27, steps per second: 306, episode reward: 0.948, mean reward: 0.035 [-0.003, 1.000], mean action: 1.296 [0.000, 2.000], mean observation: -0.064 [-0.280, 0.100], loss: 0.000062, mean_absolute_error: 0.477338, mean_q: 0.696855\n",
      " 16744/50000: episode: 370, duration: 0.088s, episode steps: 29, steps per second: 329, episode reward: 0.938, mean reward: 0.032 [-0.003, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.064 [-0.330, 0.110], loss: 0.000054, mean_absolute_error: 0.469701, mean_q: 0.680817\n",
      " 16799/50000: episode: 371, duration: 0.193s, episode steps: 55, steps per second: 285, episode reward: 0.725, mean reward: 0.013 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.180 [-0.833, 0.210], loss: 0.000054, mean_absolute_error: 0.470797, mean_q: 0.677158\n",
      " 16812/50000: episode: 372, duration: 0.040s, episode steps: 13, steps per second: 328, episode reward: 0.982, mean reward: 0.076 [-0.002, 1.000], mean action: 1.615 [0.000, 2.000], mean observation: -0.041 [-0.178, 0.100], loss: 0.000066, mean_absolute_error: 0.467318, mean_q: 0.664182\n",
      " 16876/50000: episode: 373, duration: 0.203s, episode steps: 64, steps per second: 315, episode reward: 0.693, mean reward: 0.011 [-0.009, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.167 [-0.884, 0.200], loss: 0.000053, mean_absolute_error: 0.469564, mean_q: 0.679734\n",
      " 16958/50000: episode: 374, duration: 0.286s, episode steps: 82, steps per second: 287, episode reward: 0.697, mean reward: 0.008 [-0.009, 1.000], mean action: 0.915 [0.000, 2.000], mean observation: -0.079 [-0.890, 0.240], loss: 0.000057, mean_absolute_error: 0.468605, mean_q: 0.676577\n",
      " 16994/50000: episode: 375, duration: 0.175s, episode steps: 36, steps per second: 205, episode reward: 0.901, mean reward: 0.025 [-0.004, 1.000], mean action: 0.806 [0.000, 2.000], mean observation: 0.092 [-0.150, 0.436], loss: 0.000047, mean_absolute_error: 0.469126, mean_q: 0.683396\n",
      " 17051/50000: episode: 376, duration: 0.222s, episode steps: 57, steps per second: 256, episode reward: 0.718, mean reward: 0.013 [-0.009, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.171 [-0.893, 0.220], loss: 0.000060, mean_absolute_error: 0.470301, mean_q: 0.678299\n",
      " 17082/50000: episode: 377, duration: 0.105s, episode steps: 31, steps per second: 295, episode reward: 0.918, mean reward: 0.030 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.077 [-0.420, 0.170], loss: 0.000055, mean_absolute_error: 0.474702, mean_q: 0.696739\n",
      " 17114/50000: episode: 378, duration: 0.122s, episode steps: 32, steps per second: 263, episode reward: 0.935, mean reward: 0.029 [-0.003, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.069 [-0.315, 0.120], loss: 0.000040, mean_absolute_error: 0.469286, mean_q: 0.684145\n",
      " 17135/50000: episode: 379, duration: 0.087s, episode steps: 21, steps per second: 243, episode reward: 0.962, mean reward: 0.046 [-0.003, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.051 [-0.257, 0.110], loss: 0.000035, mean_absolute_error: 0.471463, mean_q: 0.690321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17187/50000: episode: 380, duration: 0.191s, episode steps: 52, steps per second: 272, episode reward: 0.814, mean reward: 0.016 [-0.006, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.122 [-0.645, 0.200], loss: 0.000244, mean_absolute_error: 0.471032, mean_q: 0.684010\n",
      " 17188/50000: episode: 381, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.012 [-0.014, -0.010], loss: 0.000069, mean_absolute_error: 0.517226, mean_q: 0.767349\n",
      " 17244/50000: episode: 382, duration: 0.236s, episode steps: 56, steps per second: 237, episode reward: 0.738, mean reward: 0.013 [-0.008, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.168 [-0.220, 0.831], loss: 0.000066, mean_absolute_error: 0.477745, mean_q: 0.692398\n",
      " 17308/50000: episode: 383, duration: 0.458s, episode steps: 64, steps per second: 140, episode reward: 0.664, mean reward: 0.010 [-0.009, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.197 [-0.250, 0.945], loss: 0.000073, mean_absolute_error: 0.471476, mean_q: 0.683741\n",
      " 17327/50000: episode: 384, duration: 0.094s, episode steps: 19, steps per second: 201, episode reward: 0.969, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.049 [-0.100, 0.220], loss: 0.000071, mean_absolute_error: 0.474165, mean_q: 0.680441\n",
      " 17388/50000: episode: 385, duration: 0.257s, episode steps: 61, steps per second: 237, episode reward: 0.666, mean reward: 0.011 [-0.010, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.202 [-0.230, 0.979], loss: 0.000245, mean_absolute_error: 0.476382, mean_q: 0.691521\n",
      " 17429/50000: episode: 386, duration: 0.173s, episode steps: 41, steps per second: 237, episode reward: 0.854, mean reward: 0.021 [-0.006, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.115 [-0.587, 0.190], loss: 0.000426, mean_absolute_error: 0.467140, mean_q: 0.677624\n",
      " 17475/50000: episode: 387, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 0.829, mean reward: 0.018 [-0.006, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.129 [-0.200, 0.635], loss: 0.000074, mean_absolute_error: 0.467316, mean_q: 0.677204\n",
      " 17510/50000: episode: 388, duration: 0.138s, episode steps: 35, steps per second: 253, episode reward: 0.915, mean reward: 0.026 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.064 [-0.416, 0.180], loss: 0.000055, mean_absolute_error: 0.466206, mean_q: 0.676240\n",
      " 17550/50000: episode: 389, duration: 0.138s, episode steps: 40, steps per second: 289, episode reward: 0.906, mean reward: 0.023 [-0.005, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.049 [-0.451, 0.180], loss: 0.000052, mean_absolute_error: 0.468811, mean_q: 0.683139\n",
      " 17596/50000: episode: 390, duration: 0.150s, episode steps: 46, steps per second: 307, episode reward: 0.849, mean reward: 0.018 [-0.005, 1.000], mean action: 0.848 [0.000, 2.000], mean observation: 0.116 [-0.170, 0.545], loss: 0.000071, mean_absolute_error: 0.480670, mean_q: 0.703014\n",
      " 17649/50000: episode: 391, duration: 0.293s, episode steps: 53, steps per second: 181, episode reward: 0.860, mean reward: 0.016 [-0.005, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.079 [-0.526, 0.150], loss: 0.000144, mean_absolute_error: 0.473215, mean_q: 0.686628\n",
      " 17687/50000: episode: 392, duration: 0.305s, episode steps: 38, steps per second: 124, episode reward: 0.904, mean reward: 0.024 [-0.005, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.063 [-0.466, 0.180], loss: 0.000055, mean_absolute_error: 0.474173, mean_q: 0.693583\n",
      " 17738/50000: episode: 393, duration: 0.239s, episode steps: 51, steps per second: 214, episode reward: 0.771, mean reward: 0.015 [-0.008, 1.000], mean action: 1.118 [0.000, 2.000], mean observation: -0.160 [-0.771, 0.190], loss: 0.000325, mean_absolute_error: 0.477869, mean_q: 0.699860\n",
      " 17769/50000: episode: 394, duration: 0.101s, episode steps: 31, steps per second: 306, episode reward: 0.917, mean reward: 0.030 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.077 [-0.424, 0.190], loss: 0.000103, mean_absolute_error: 0.474542, mean_q: 0.690527\n",
      " 17797/50000: episode: 395, duration: 0.088s, episode steps: 28, steps per second: 319, episode reward: 0.942, mean reward: 0.034 [-0.003, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.066 [-0.130, 0.316], loss: 0.000211, mean_absolute_error: 0.474339, mean_q: 0.687887\n",
      " 17798/50000: episode: 396, duration: 0.006s, episode steps: 1, steps per second: 160, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.006 [-0.010, -0.003], loss: 0.000173, mean_absolute_error: 0.468771, mean_q: 0.704614\n",
      " 17859/50000: episode: 397, duration: 0.183s, episode steps: 61, steps per second: 333, episode reward: 0.655, mean reward: 0.011 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.218 [-0.900, 0.200], loss: 0.000105, mean_absolute_error: 0.472574, mean_q: 0.686775\n",
      " 17869/50000: episode: 398, duration: 0.034s, episode steps: 10, steps per second: 296, episode reward: 0.988, mean reward: 0.099 [-0.001, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.041 [-0.090, 0.146], loss: 0.000082, mean_absolute_error: 0.481020, mean_q: 0.700369\n",
      " 17913/50000: episode: 399, duration: 0.143s, episode steps: 44, steps per second: 308, episode reward: 0.827, mean reward: 0.019 [-0.006, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.136 [-0.630, 0.170], loss: 0.000076, mean_absolute_error: 0.471259, mean_q: 0.681630\n",
      " 17977/50000: episode: 400, duration: 0.244s, episode steps: 64, steps per second: 263, episode reward: 0.629, mean reward: 0.010 [-0.010, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.222 [-0.956, 0.210], loss: 0.000056, mean_absolute_error: 0.472057, mean_q: 0.685809\n",
      " 18004/50000: episode: 401, duration: 0.083s, episode steps: 27, steps per second: 326, episode reward: 0.935, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.076 [-0.347, 0.150], loss: 0.000063, mean_absolute_error: 0.471682, mean_q: 0.679659\n",
      " 18056/50000: episode: 402, duration: 0.204s, episode steps: 52, steps per second: 255, episode reward: 0.805, mean reward: 0.015 [-0.007, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.135 [-0.180, 0.654], loss: 0.000051, mean_absolute_error: 0.470750, mean_q: 0.680197\n",
      " 18145/50000: episode: 403, duration: 0.385s, episode steps: 89, steps per second: 231, episode reward: 0.452, mean reward: 0.005 [-0.010, 1.000], mean action: 1.101 [0.000, 2.000], mean observation: -0.252 [-0.997, 0.190], loss: 0.000147, mean_absolute_error: 0.473772, mean_q: 0.692969\n",
      " 18197/50000: episode: 404, duration: 0.157s, episode steps: 52, steps per second: 331, episode reward: 0.807, mean reward: 0.016 [-0.007, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.133 [-0.160, 0.652], loss: 0.000080, mean_absolute_error: 0.476722, mean_q: 0.700309\n",
      " 18243/50000: episode: 405, duration: 0.197s, episode steps: 46, steps per second: 234, episode reward: 0.824, mean reward: 0.018 [-0.007, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.128 [-0.651, 0.170], loss: 0.000066, mean_absolute_error: 0.472359, mean_q: 0.688729\n",
      " 18292/50000: episode: 406, duration: 0.196s, episode steps: 49, steps per second: 250, episode reward: 0.794, mean reward: 0.016 [-0.007, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.151 [-0.684, 0.190], loss: 0.000055, mean_absolute_error: 0.466986, mean_q: 0.678349\n",
      " 18305/50000: episode: 407, duration: 0.054s, episode steps: 13, steps per second: 241, episode reward: 0.985, mean reward: 0.076 [-0.001, 1.000], mean action: 1.385 [0.000, 2.000], mean observation: -0.049 [-0.137, 0.070], loss: 0.000047, mean_absolute_error: 0.474582, mean_q: 0.703755\n",
      " 18351/50000: episode: 408, duration: 0.135s, episode steps: 46, steps per second: 340, episode reward: 0.852, mean reward: 0.019 [-0.005, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.113 [-0.150, 0.546], loss: 0.000049, mean_absolute_error: 0.461361, mean_q: 0.667272\n",
      " 18370/50000: episode: 409, duration: 0.062s, episode steps: 19, steps per second: 304, episode reward: 0.967, mean reward: 0.051 [-0.002, 1.000], mean action: 0.579 [0.000, 2.000], mean observation: 0.053 [-0.110, 0.231], loss: 0.000606, mean_absolute_error: 0.469962, mean_q: 0.686913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18417/50000: episode: 410, duration: 0.160s, episode steps: 47, steps per second: 293, episode reward: 0.852, mean reward: 0.018 [-0.005, 1.000], mean action: 0.872 [0.000, 2.000], mean observation: 0.113 [-0.160, 0.523], loss: 0.000117, mean_absolute_error: 0.469297, mean_q: 0.684063\n",
      " 18451/50000: episode: 411, duration: 0.164s, episode steps: 34, steps per second: 207, episode reward: 0.918, mean reward: 0.027 [-0.004, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.081 [-0.120, 0.371], loss: 0.000068, mean_absolute_error: 0.467812, mean_q: 0.681811\n",
      " 18467/50000: episode: 412, duration: 0.108s, episode steps: 16, steps per second: 148, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 0.562 [0.000, 2.000], mean observation: 0.045 [-0.100, 0.192], loss: 0.000072, mean_absolute_error: 0.476133, mean_q: 0.698704\n",
      " 18468/50000: episode: 413, duration: 0.007s, episode steps: 1, steps per second: 137, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [-0.010, 0.016], loss: 0.000021, mean_absolute_error: 0.496259, mean_q: 0.743742\n",
      " 18527/50000: episode: 414, duration: 0.179s, episode steps: 59, steps per second: 330, episode reward: 0.738, mean reward: 0.013 [-0.008, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.163 [-0.806, 0.220], loss: 0.000444, mean_absolute_error: 0.472970, mean_q: 0.689674\n",
      " 18566/50000: episode: 415, duration: 0.115s, episode steps: 39, steps per second: 339, episode reward: 0.890, mean reward: 0.023 [-0.004, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.101 [-0.140, 0.423], loss: 0.000083, mean_absolute_error: 0.470009, mean_q: 0.689003\n",
      " 18611/50000: episode: 416, duration: 0.178s, episode steps: 45, steps per second: 253, episode reward: 0.871, mean reward: 0.019 [-0.005, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.091 [-0.515, 0.170], loss: 0.000289, mean_absolute_error: 0.461666, mean_q: 0.675120\n",
      " 18626/50000: episode: 417, duration: 0.111s, episode steps: 15, steps per second: 135, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 0.533 [0.000, 2.000], mean observation: 0.043 [-0.090, 0.187], loss: 0.000063, mean_absolute_error: 0.468372, mean_q: 0.682756\n",
      " 18680/50000: episode: 418, duration: 0.304s, episode steps: 54, steps per second: 178, episode reward: 0.737, mean reward: 0.014 [-0.008, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.175 [-0.846, 0.210], loss: 0.000049, mean_absolute_error: 0.472563, mean_q: 0.688883\n",
      " 18724/50000: episode: 419, duration: 0.313s, episode steps: 44, steps per second: 140, episode reward: 0.852, mean reward: 0.019 [-0.006, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.116 [-0.170, 0.562], loss: 0.000046, mean_absolute_error: 0.470906, mean_q: 0.681941\n",
      " 18770/50000: episode: 420, duration: 0.203s, episode steps: 46, steps per second: 227, episode reward: 0.914, mean reward: 0.020 [-0.003, 1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.075 [-0.281, 0.090], loss: 0.000169, mean_absolute_error: 0.473252, mean_q: 0.688067\n",
      " 18827/50000: episode: 421, duration: 0.332s, episode steps: 57, steps per second: 172, episode reward: 0.754, mean reward: 0.013 [-0.008, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.156 [-0.765, 0.210], loss: 0.000067, mean_absolute_error: 0.463718, mean_q: 0.673882\n",
      " 18880/50000: episode: 422, duration: 0.260s, episode steps: 53, steps per second: 204, episode reward: 0.794, mean reward: 0.015 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.141 [-0.190, 0.653], loss: 0.000390, mean_absolute_error: 0.473444, mean_q: 0.695550\n",
      " 18944/50000: episode: 423, duration: 0.294s, episode steps: 64, steps per second: 217, episode reward: 0.663, mean reward: 0.010 [-0.009, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.198 [-0.250, 0.937], loss: 0.000272, mean_absolute_error: 0.471204, mean_q: 0.692721\n",
      " 18968/50000: episode: 424, duration: 0.081s, episode steps: 24, steps per second: 298, episode reward: 0.967, mean reward: 0.040 [-0.002, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.051 [-0.196, 0.080], loss: 0.000051, mean_absolute_error: 0.471335, mean_q: 0.693128\n",
      " 19020/50000: episode: 425, duration: 0.159s, episode steps: 52, steps per second: 327, episode reward: 0.832, mean reward: 0.016 [-0.006, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: -0.114 [-0.596, 0.180], loss: 0.000055, mean_absolute_error: 0.469958, mean_q: 0.689488\n",
      " 19040/50000: episode: 426, duration: 0.067s, episode steps: 20, steps per second: 301, episode reward: 0.973, mean reward: 0.049 [-0.002, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.052 [-0.167, 0.070], loss: 0.000034, mean_absolute_error: 0.470348, mean_q: 0.685554\n",
      " 19072/50000: episode: 427, duration: 0.094s, episode steps: 32, steps per second: 340, episode reward: 0.910, mean reward: 0.028 [-0.004, 1.000], mean action: 1.219 [0.000, 2.000], mean observation: -0.090 [-0.429, 0.160], loss: 0.000050, mean_absolute_error: 0.466664, mean_q: 0.681327\n",
      " 19133/50000: episode: 428, duration: 0.206s, episode steps: 61, steps per second: 296, episode reward: 0.741, mean reward: 0.012 [-0.008, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.157 [-0.210, 0.786], loss: 0.000100, mean_absolute_error: 0.471666, mean_q: 0.694374\n",
      " 19144/50000: episode: 429, duration: 0.044s, episode steps: 11, steps per second: 249, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 1.636 [0.000, 2.000], mean observation: -0.040 [-0.154, 0.080], loss: 0.000041, mean_absolute_error: 0.464507, mean_q: 0.687963\n",
      " 19145/50000: episode: 430, duration: 0.008s, episode steps: 1, steps per second: 120, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.085, 0.010], loss: 0.000020, mean_absolute_error: 0.449757, mean_q: 0.677749\n",
      " 19178/50000: episode: 431, duration: 0.231s, episode steps: 33, steps per second: 143, episode reward: 0.922, mean reward: 0.028 [-0.004, 1.000], mean action: 0.788 [0.000, 2.000], mean observation: 0.078 [-0.130, 0.370], loss: 0.000052, mean_absolute_error: 0.465383, mean_q: 0.678418\n",
      " 19242/50000: episode: 432, duration: 0.236s, episode steps: 64, steps per second: 271, episode reward: 0.684, mean reward: 0.011 [-0.009, 1.000], mean action: 0.859 [0.000, 2.000], mean observation: 0.184 [-0.210, 0.907], loss: 0.000048, mean_absolute_error: 0.464586, mean_q: 0.681484\n",
      " 19285/50000: episode: 433, duration: 0.132s, episode steps: 43, steps per second: 325, episode reward: 0.864, mean reward: 0.020 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.108 [-0.160, 0.531], loss: 0.000048, mean_absolute_error: 0.465307, mean_q: 0.682708\n",
      " 19346/50000: episode: 434, duration: 0.252s, episode steps: 61, steps per second: 242, episode reward: 0.743, mean reward: 0.012 [-0.008, 1.000], mean action: 0.918 [0.000, 2.000], mean observation: 0.155 [-0.200, 0.781], loss: 0.000046, mean_absolute_error: 0.470964, mean_q: 0.694757\n",
      " 19385/50000: episode: 435, duration: 0.133s, episode steps: 39, steps per second: 294, episode reward: 0.889, mean reward: 0.023 [-0.004, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.099 [-0.130, 0.447], loss: 0.000059, mean_absolute_error: 0.470258, mean_q: 0.691791\n",
      " 19418/50000: episode: 436, duration: 0.141s, episode steps: 33, steps per second: 233, episode reward: 0.941, mean reward: 0.029 [-0.003, 1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.065 [-0.271, 0.090], loss: 0.000047, mean_absolute_error: 0.466133, mean_q: 0.684008\n",
      " 19448/50000: episode: 437, duration: 0.100s, episode steps: 30, steps per second: 300, episode reward: 0.931, mean reward: 0.031 [-0.003, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.076 [-0.345, 0.130], loss: 0.000056, mean_absolute_error: 0.470836, mean_q: 0.697557\n",
      " 19497/50000: episode: 438, duration: 0.174s, episode steps: 49, steps per second: 281, episode reward: 0.829, mean reward: 0.017 [-0.006, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.124 [-0.170, 0.599], loss: 0.000062, mean_absolute_error: 0.466737, mean_q: 0.685629\n",
      " 19534/50000: episode: 439, duration: 0.108s, episode steps: 37, steps per second: 343, episode reward: 0.904, mean reward: 0.024 [-0.004, 1.000], mean action: 1.216 [0.000, 2.000], mean observation: -0.090 [-0.393, 0.140], loss: 0.000048, mean_absolute_error: 0.470897, mean_q: 0.689129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19587/50000: episode: 440, duration: 0.156s, episode steps: 53, steps per second: 341, episode reward: 0.824, mean reward: 0.016 [-0.006, 1.000], mean action: 1.094 [0.000, 2.000], mean observation: -0.124 [-0.551, 0.160], loss: 0.000310, mean_absolute_error: 0.469406, mean_q: 0.692055\n",
      " 19655/50000: episode: 441, duration: 0.211s, episode steps: 68, steps per second: 323, episode reward: 0.659, mean reward: 0.010 [-0.009, 1.000], mean action: 0.926 [0.000, 2.000], mean observation: 0.189 [-0.210, 0.948], loss: 0.000048, mean_absolute_error: 0.471361, mean_q: 0.698535\n",
      " 19656/50000: episode: 442, duration: 0.008s, episode steps: 1, steps per second: 133, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.000 [-0.010, 0.011], loss: 0.000040, mean_absolute_error: 0.470266, mean_q: 0.705774\n",
      " 19718/50000: episode: 443, duration: 0.200s, episode steps: 62, steps per second: 310, episode reward: 0.698, mean reward: 0.011 [-0.009, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.180 [-0.200, 0.890], loss: 0.000105, mean_absolute_error: 0.467183, mean_q: 0.686571\n",
      " 19742/50000: episode: 444, duration: 0.071s, episode steps: 24, steps per second: 337, episode reward: 0.958, mean reward: 0.040 [-0.002, 1.000], mean action: 1.292 [0.000, 2.000], mean observation: -0.057 [-0.248, 0.110], loss: 0.000051, mean_absolute_error: 0.456617, mean_q: 0.671797\n",
      " 19759/50000: episode: 445, duration: 0.050s, episode steps: 17, steps per second: 341, episode reward: 0.976, mean reward: 0.057 [-0.002, 1.000], mean action: 0.471 [0.000, 2.000], mean observation: 0.044 [-0.100, 0.182], loss: 0.000049, mean_absolute_error: 0.470603, mean_q: 0.698253\n",
      " 19799/50000: episode: 446, duration: 0.119s, episode steps: 40, steps per second: 335, episode reward: 0.889, mean reward: 0.022 [-0.005, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.093 [-0.463, 0.160], loss: 0.000123, mean_absolute_error: 0.466011, mean_q: 0.688094\n",
      " 19855/50000: episode: 447, duration: 0.195s, episode steps: 56, steps per second: 287, episode reward: 0.767, mean reward: 0.014 [-0.007, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.151 [-0.190, 0.742], loss: 0.000072, mean_absolute_error: 0.465377, mean_q: 0.687443\n",
      " 19890/50000: episode: 448, duration: 0.107s, episode steps: 35, steps per second: 326, episode reward: 0.922, mean reward: 0.026 [-0.004, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.054 [-0.170, 0.399], loss: 0.000456, mean_absolute_error: 0.477644, mean_q: 0.706937\n",
      " 19925/50000: episode: 449, duration: 0.104s, episode steps: 35, steps per second: 337, episode reward: 0.908, mean reward: 0.026 [-0.004, 1.000], mean action: 0.771 [0.000, 2.000], mean observation: 0.087 [-0.130, 0.411], loss: 0.000052, mean_absolute_error: 0.467605, mean_q: 0.693124\n",
      " 19953/50000: episode: 450, duration: 0.081s, episode steps: 28, steps per second: 345, episode reward: 0.952, mean reward: 0.034 [-0.003, 1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.060 [-0.251, 0.100], loss: 0.000061, mean_absolute_error: 0.467776, mean_q: 0.691434\n",
      " 20014/50000: episode: 451, duration: 0.213s, episode steps: 61, steps per second: 287, episode reward: 0.717, mean reward: 0.012 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.169 [-0.220, 0.858], loss: 0.000054, mean_absolute_error: 0.469533, mean_q: 0.693124\n",
      " 20063/50000: episode: 452, duration: 0.169s, episode steps: 49, steps per second: 290, episode reward: 0.808, mean reward: 0.016 [-0.006, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.142 [-0.627, 0.170], loss: 0.000359, mean_absolute_error: 0.463709, mean_q: 0.685967\n",
      " 20112/50000: episode: 453, duration: 0.140s, episode steps: 49, steps per second: 349, episode reward: 0.824, mean reward: 0.017 [-0.006, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.126 [-0.160, 0.628], loss: 0.000047, mean_absolute_error: 0.461839, mean_q: 0.682936\n",
      " 20154/50000: episode: 454, duration: 0.181s, episode steps: 42, steps per second: 232, episode reward: 0.869, mean reward: 0.021 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.099 [-0.536, 0.190], loss: 0.000148, mean_absolute_error: 0.467692, mean_q: 0.690893\n",
      " 20196/50000: episode: 455, duration: 0.155s, episode steps: 42, steps per second: 270, episode reward: 0.884, mean reward: 0.021 [-0.005, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.096 [-0.130, 0.454], loss: 0.000059, mean_absolute_error: 0.461422, mean_q: 0.682230\n",
      " 20249/50000: episode: 456, duration: 0.166s, episode steps: 53, steps per second: 319, episode reward: 0.779, mean reward: 0.015 [-0.007, 1.000], mean action: 0.906 [0.000, 2.000], mean observation: 0.149 [-0.210, 0.736], loss: 0.000047, mean_absolute_error: 0.459951, mean_q: 0.681962\n",
      " 20250/50000: episode: 457, duration: 0.007s, episode steps: 1, steps per second: 154, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [-0.010, 0.066], loss: 0.000010, mean_absolute_error: 0.460247, mean_q: 0.693183\n",
      " 20273/50000: episode: 458, duration: 0.073s, episode steps: 23, steps per second: 317, episode reward: 0.958, mean reward: 0.042 [-0.002, 1.000], mean action: 1.391 [0.000, 2.000], mean observation: -0.060 [-0.249, 0.110], loss: 0.000050, mean_absolute_error: 0.466057, mean_q: 0.692463\n",
      " 20347/50000: episode: 459, duration: 0.273s, episode steps: 74, steps per second: 271, episode reward: 0.631, mean reward: 0.009 [-0.010, 1.000], mean action: 1.027 [0.000, 2.000], mean observation: -0.190 [-0.988, 0.210], loss: 0.000040, mean_absolute_error: 0.460716, mean_q: 0.684159\n",
      " 20388/50000: episode: 460, duration: 0.201s, episode steps: 41, steps per second: 204, episode reward: 0.878, mean reward: 0.021 [-0.005, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.101 [-0.140, 0.487], loss: 0.000112, mean_absolute_error: 0.462812, mean_q: 0.688435\n",
      " 20447/50000: episode: 461, duration: 0.262s, episode steps: 59, steps per second: 225, episode reward: 0.716, mean reward: 0.012 [-0.008, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.179 [-0.210, 0.844], loss: 0.000051, mean_absolute_error: 0.463332, mean_q: 0.687935\n",
      " 20506/50000: episode: 462, duration: 0.282s, episode steps: 59, steps per second: 209, episode reward: 0.698, mean reward: 0.012 [-0.009, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.184 [-0.230, 0.928], loss: 0.000378, mean_absolute_error: 0.466639, mean_q: 0.690931\n",
      " 20558/50000: episode: 463, duration: 0.179s, episode steps: 52, steps per second: 291, episode reward: 0.769, mean reward: 0.015 [-0.008, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.159 [-0.764, 0.190], loss: 0.000054, mean_absolute_error: 0.470192, mean_q: 0.701788\n",
      " 20560/50000: episode: 464, duration: 0.009s, episode steps: 2, steps per second: 225, episode reward: 0.999, mean reward: 0.499 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.042 [-0.020, 0.101], loss: 0.000031, mean_absolute_error: 0.482879, mean_q: 0.713153\n",
      " 20561/50000: episode: 465, duration: 0.006s, episode steps: 1, steps per second: 158, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.002 [0.000, 0.003], loss: 0.000181, mean_absolute_error: 0.457447, mean_q: 0.686243\n",
      " 20562/50000: episode: 466, duration: 0.006s, episode steps: 1, steps per second: 168, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.026 [0.000, 0.052], loss: 0.000095, mean_absolute_error: 0.440021, mean_q: 0.653390\n",
      " 20587/50000: episode: 467, duration: 0.091s, episode steps: 25, steps per second: 275, episode reward: 0.965, mean reward: 0.039 [-0.002, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.037 [-0.199, 0.120], loss: 0.000067, mean_absolute_error: 0.468097, mean_q: 0.695181\n",
      " 20653/50000: episode: 468, duration: 0.257s, episode steps: 66, steps per second: 257, episode reward: 0.669, mean reward: 0.010 [-0.009, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.184 [-0.220, 0.911], loss: 0.000044, mean_absolute_error: 0.463690, mean_q: 0.688080\n",
      " 20700/50000: episode: 469, duration: 0.161s, episode steps: 47, steps per second: 292, episode reward: 0.851, mean reward: 0.018 [-0.005, 1.000], mean action: 0.851 [0.000, 2.000], mean observation: 0.115 [-0.160, 0.521], loss: 0.000052, mean_absolute_error: 0.475524, mean_q: 0.708827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20750/50000: episode: 470, duration: 0.163s, episode steps: 50, steps per second: 308, episode reward: 0.888, mean reward: 0.018 [-0.004, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.086 [-0.372, 0.120], loss: 0.000104, mean_absolute_error: 0.461530, mean_q: 0.686790\n",
      " 20806/50000: episode: 471, duration: 0.187s, episode steps: 56, steps per second: 300, episode reward: 0.727, mean reward: 0.013 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.174 [-0.861, 0.230], loss: 0.000052, mean_absolute_error: 0.466443, mean_q: 0.692937\n",
      " 20832/50000: episode: 472, duration: 0.101s, episode steps: 26, steps per second: 258, episode reward: 0.941, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.071 [-0.330, 0.130], loss: 0.000542, mean_absolute_error: 0.466482, mean_q: 0.692720\n",
      " 20835/50000: episode: 473, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: 0.998, mean reward: 0.333 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.041 [-0.030, 0.103], loss: 0.000071, mean_absolute_error: 0.459111, mean_q: 0.684367\n",
      " 20899/50000: episode: 474, duration: 0.207s, episode steps: 64, steps per second: 309, episode reward: 0.715, mean reward: 0.011 [-0.008, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.168 [-0.200, 0.812], loss: 0.000074, mean_absolute_error: 0.468727, mean_q: 0.696538\n",
      " 20917/50000: episode: 475, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 0.973, mean reward: 0.054 [-0.002, 1.000], mean action: 0.556 [0.000, 2.000], mean observation: 0.049 [-0.080, 0.205], loss: 0.000054, mean_absolute_error: 0.474045, mean_q: 0.703137\n",
      " 20981/50000: episode: 476, duration: 0.235s, episode steps: 64, steps per second: 272, episode reward: 0.749, mean reward: 0.012 [-0.008, 1.000], mean action: 0.859 [0.000, 2.000], mean observation: 0.132 [-0.190, 0.788], loss: 0.000273, mean_absolute_error: 0.465739, mean_q: 0.694039\n",
      " 21050/50000: episode: 477, duration: 0.194s, episode steps: 69, steps per second: 356, episode reward: 0.622, mean reward: 0.009 [-0.009, 1.000], mean action: 1.087 [0.000, 2.000], mean observation: -0.215 [-0.920, 0.220], loss: 0.000060, mean_absolute_error: 0.463465, mean_q: 0.690783\n",
      " 21080/50000: episode: 478, duration: 0.088s, episode steps: 30, steps per second: 343, episode reward: 0.922, mean reward: 0.031 [-0.004, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.080 [-0.401, 0.170], loss: 0.000059, mean_absolute_error: 0.463750, mean_q: 0.689168\n",
      " 21083/50000: episode: 479, duration: 0.011s, episode steps: 3, steps per second: 262, episode reward: 0.998, mean reward: 0.333 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.040 [-0.102, 0.030], loss: 0.000066, mean_absolute_error: 0.469513, mean_q: 0.701093\n",
      " 21084/50000: episode: 480, duration: 0.006s, episode steps: 1, steps per second: 177, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [-0.010, 0.072], loss: 0.000016, mean_absolute_error: 0.499009, mean_q: 0.750621\n",
      " 21133/50000: episode: 481, duration: 0.146s, episode steps: 49, steps per second: 336, episode reward: 0.789, mean reward: 0.016 [-0.007, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.150 [-0.220, 0.739], loss: 0.000289, mean_absolute_error: 0.463366, mean_q: 0.687761\n",
      " 21134/50000: episode: 482, duration: 0.008s, episode steps: 1, steps per second: 131, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.009 [0.000, 0.018], loss: 0.000184, mean_absolute_error: 0.509761, mean_q: 0.765319\n",
      " 21195/50000: episode: 483, duration: 0.220s, episode steps: 61, steps per second: 278, episode reward: 0.719, mean reward: 0.012 [-0.009, 1.000], mean action: 1.131 [0.000, 2.000], mean observation: -0.169 [-0.856, 0.200], loss: 0.000048, mean_absolute_error: 0.471895, mean_q: 0.706450\n",
      " 21237/50000: episode: 484, duration: 0.138s, episode steps: 42, steps per second: 304, episode reward: 0.866, mean reward: 0.021 [-0.005, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.111 [-0.140, 0.521], loss: 0.000058, mean_absolute_error: 0.468029, mean_q: 0.698209\n",
      " 21283/50000: episode: 485, duration: 0.151s, episode steps: 46, steps per second: 306, episode reward: 0.809, mean reward: 0.018 [-0.007, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.143 [-0.698, 0.200], loss: 0.000046, mean_absolute_error: 0.464146, mean_q: 0.693291\n",
      " 21303/50000: episode: 486, duration: 0.059s, episode steps: 20, steps per second: 339, episode reward: 0.970, mean reward: 0.049 [-0.002, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.051 [-0.090, 0.200], loss: 0.000059, mean_absolute_error: 0.461856, mean_q: 0.687084\n",
      " 21304/50000: episode: 487, duration: 0.006s, episode steps: 1, steps per second: 180, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.030 [-0.069, 0.010], loss: 0.000014, mean_absolute_error: 0.506855, mean_q: 0.764333\n",
      " 21361/50000: episode: 488, duration: 0.165s, episode steps: 57, steps per second: 345, episode reward: 0.703, mean reward: 0.012 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.188 [-0.230, 0.911], loss: 0.000293, mean_absolute_error: 0.457791, mean_q: 0.678246\n",
      " 21362/50000: episode: 489, duration: 0.006s, episode steps: 1, steps per second: 162, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [-0.010, 0.075], loss: 0.000138, mean_absolute_error: 0.419843, mean_q: 0.632395\n",
      " 21377/50000: episode: 490, duration: 0.046s, episode steps: 15, steps per second: 328, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.043 [-0.100, 0.187], loss: 0.000079, mean_absolute_error: 0.457180, mean_q: 0.678752\n",
      " 21378/50000: episode: 491, duration: 0.006s, episode steps: 1, steps per second: 161, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.042 [-0.094, 0.010], loss: 0.000060, mean_absolute_error: 0.444556, mean_q: 0.644041\n",
      " 21409/50000: episode: 492, duration: 0.092s, episode steps: 31, steps per second: 337, episode reward: 0.931, mean reward: 0.030 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.063 [-0.355, 0.140], loss: 0.000052, mean_absolute_error: 0.476543, mean_q: 0.709365\n",
      " 21469/50000: episode: 493, duration: 0.181s, episode steps: 60, steps per second: 332, episode reward: 0.673, mean reward: 0.011 [-0.009, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.203 [-0.935, 0.200], loss: 0.000057, mean_absolute_error: 0.472859, mean_q: 0.704220\n",
      " 21525/50000: episode: 494, duration: 0.188s, episode steps: 56, steps per second: 298, episode reward: 0.710, mean reward: 0.013 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.178 [-0.936, 0.240], loss: 0.000115, mean_absolute_error: 0.466003, mean_q: 0.696374\n",
      " 21551/50000: episode: 495, duration: 0.080s, episode steps: 26, steps per second: 324, episode reward: 0.953, mean reward: 0.037 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.054 [-0.120, 0.262], loss: 0.000057, mean_absolute_error: 0.466296, mean_q: 0.696718\n",
      " 21615/50000: episode: 496, duration: 0.228s, episode steps: 64, steps per second: 281, episode reward: 0.663, mean reward: 0.010 [-0.010, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.193 [-0.962, 0.210], loss: 0.000048, mean_absolute_error: 0.462513, mean_q: 0.686564\n",
      " 21677/50000: episode: 497, duration: 0.217s, episode steps: 62, steps per second: 286, episode reward: 0.742, mean reward: 0.012 [-0.007, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.154 [-0.170, 0.738], loss: 0.000189, mean_absolute_error: 0.465920, mean_q: 0.689860\n",
      " 21717/50000: episode: 498, duration: 0.113s, episode steps: 40, steps per second: 354, episode reward: 0.877, mean reward: 0.022 [-0.005, 1.000], mean action: 0.825 [0.000, 2.000], mean observation: 0.106 [-0.140, 0.491], loss: 0.000066, mean_absolute_error: 0.463593, mean_q: 0.691647\n",
      " 21718/50000: episode: 499, duration: 0.006s, episode steps: 1, steps per second: 159, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [-0.010, 0.096], loss: 0.000030, mean_absolute_error: 0.460880, mean_q: 0.697220\n",
      " 21719/50000: episode: 500, duration: 0.006s, episode steps: 1, steps per second: 167, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.086, 0.010], loss: 0.000025, mean_absolute_error: 0.466707, mean_q: 0.690770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21757/50000: episode: 501, duration: 0.115s, episode steps: 38, steps per second: 330, episode reward: 0.888, mean reward: 0.023 [-0.005, 1.000], mean action: 1.211 [0.000, 2.000], mean observation: -0.099 [-0.476, 0.160], loss: 0.000045, mean_absolute_error: 0.462377, mean_q: 0.689871\n",
      " 21825/50000: episode: 502, duration: 0.196s, episode steps: 68, steps per second: 348, episode reward: 0.648, mean reward: 0.010 [-0.010, 1.000], mean action: 0.926 [0.000, 2.000], mean observation: 0.194 [-0.230, 0.984], loss: 0.000043, mean_absolute_error: 0.462412, mean_q: 0.689051\n",
      " 21874/50000: episode: 503, duration: 0.143s, episode steps: 49, steps per second: 342, episode reward: 0.810, mean reward: 0.017 [-0.007, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.136 [-0.190, 0.667], loss: 0.000060, mean_absolute_error: 0.462872, mean_q: 0.689460\n",
      " 21893/50000: episode: 504, duration: 0.058s, episode steps: 19, steps per second: 329, episode reward: 0.968, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.049 [-0.110, 0.235], loss: 0.000040, mean_absolute_error: 0.463911, mean_q: 0.699348\n",
      " 21945/50000: episode: 505, duration: 0.168s, episode steps: 52, steps per second: 309, episode reward: 0.781, mean reward: 0.015 [-0.007, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.151 [-0.190, 0.716], loss: 0.000048, mean_absolute_error: 0.460505, mean_q: 0.685989\n",
      " 21983/50000: episode: 506, duration: 0.121s, episode steps: 38, steps per second: 313, episode reward: 0.896, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.091 [-0.150, 0.443], loss: 0.000036, mean_absolute_error: 0.469459, mean_q: 0.702923\n",
      " 21984/50000: episode: 507, duration: 0.006s, episode steps: 1, steps per second: 162, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.022 [-0.010, 0.055], loss: 0.000020, mean_absolute_error: 0.411213, mean_q: 0.595047\n",
      " 22033/50000: episode: 508, duration: 0.196s, episode steps: 49, steps per second: 250, episode reward: 0.828, mean reward: 0.017 [-0.006, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.124 [-0.160, 0.607], loss: 0.000039, mean_absolute_error: 0.468093, mean_q: 0.700591\n",
      " 22090/50000: episode: 509, duration: 0.256s, episode steps: 57, steps per second: 223, episode reward: 0.696, mean reward: 0.012 [-0.009, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.200 [-0.871, 0.190], loss: 0.000082, mean_absolute_error: 0.468125, mean_q: 0.697371\n",
      " 22124/50000: episode: 510, duration: 0.146s, episode steps: 34, steps per second: 232, episode reward: 0.929, mean reward: 0.027 [-0.003, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.060 [-0.120, 0.345], loss: 0.000062, mean_absolute_error: 0.469237, mean_q: 0.700053\n",
      " 22151/50000: episode: 511, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 0.947, mean reward: 0.035 [-0.003, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.064 [-0.110, 0.284], loss: 0.000085, mean_absolute_error: 0.466883, mean_q: 0.693860\n",
      " 22175/50000: episode: 512, duration: 0.095s, episode steps: 24, steps per second: 253, episode reward: 0.948, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.063 [-0.310, 0.150], loss: 0.000052, mean_absolute_error: 0.467342, mean_q: 0.699928\n",
      " 22212/50000: episode: 513, duration: 0.163s, episode steps: 37, steps per second: 228, episode reward: 0.911, mean reward: 0.025 [-0.004, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.067 [-0.423, 0.170], loss: 0.000055, mean_absolute_error: 0.467236, mean_q: 0.699214\n",
      " 22240/50000: episode: 514, duration: 0.108s, episode steps: 28, steps per second: 259, episode reward: 0.942, mean reward: 0.034 [-0.003, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: -0.070 [-0.297, 0.110], loss: 0.000035, mean_absolute_error: 0.466952, mean_q: 0.690842\n",
      " 22241/50000: episode: 515, duration: 0.006s, episode steps: 1, steps per second: 166, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.003 [-0.004, 0.010], loss: 0.000015, mean_absolute_error: 0.480721, mean_q: 0.725581\n",
      " 22306/50000: episode: 516, duration: 0.263s, episode steps: 65, steps per second: 247, episode reward: 0.680, mean reward: 0.010 [-0.009, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.178 [-0.921, 0.190], loss: 0.000037, mean_absolute_error: 0.472955, mean_q: 0.704932\n",
      " 22322/50000: episode: 517, duration: 0.054s, episode steps: 16, steps per second: 294, episode reward: 0.978, mean reward: 0.061 [-0.002, 1.000], mean action: 1.438 [0.000, 2.000], mean observation: -0.046 [-0.182, 0.080], loss: 0.000045, mean_absolute_error: 0.476567, mean_q: 0.711751\n",
      " 22343/50000: episode: 518, duration: 0.066s, episode steps: 21, steps per second: 316, episode reward: 0.973, mean reward: 0.046 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.033 [-0.110, 0.176], loss: 0.000066, mean_absolute_error: 0.475076, mean_q: 0.711477\n",
      " 22370/50000: episode: 519, duration: 0.082s, episode steps: 27, steps per second: 328, episode reward: 0.950, mean reward: 0.035 [-0.002, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.067 [-0.249, 0.090], loss: 0.000048, mean_absolute_error: 0.471454, mean_q: 0.704576\n",
      " 22411/50000: episode: 520, duration: 0.151s, episode steps: 41, steps per second: 272, episode reward: 0.883, mean reward: 0.022 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.088 [-0.490, 0.190], loss: 0.000025, mean_absolute_error: 0.468701, mean_q: 0.699760\n",
      " 22468/50000: episode: 521, duration: 0.241s, episode steps: 57, steps per second: 236, episode reward: 0.768, mean reward: 0.013 [-0.008, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.144 [-0.210, 0.754], loss: 0.000036, mean_absolute_error: 0.465099, mean_q: 0.693582\n",
      " 22506/50000: episode: 522, duration: 0.140s, episode steps: 38, steps per second: 271, episode reward: 0.907, mean reward: 0.024 [-0.004, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.077 [-0.407, 0.130], loss: 0.000036, mean_absolute_error: 0.464731, mean_q: 0.691906\n",
      " 22526/50000: episode: 523, duration: 0.087s, episode steps: 20, steps per second: 231, episode reward: 0.979, mean reward: 0.049 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.013 [-0.174, 0.110], loss: 0.000054, mean_absolute_error: 0.473322, mean_q: 0.707544\n",
      " 22569/50000: episode: 524, duration: 0.233s, episode steps: 43, steps per second: 185, episode reward: 0.856, mean reward: 0.020 [-0.005, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.118 [-0.528, 0.170], loss: 0.000033, mean_absolute_error: 0.460503, mean_q: 0.684814\n",
      " 22611/50000: episode: 525, duration: 0.187s, episode steps: 42, steps per second: 225, episode reward: 0.872, mean reward: 0.021 [-0.005, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.107 [-0.140, 0.481], loss: 0.000272, mean_absolute_error: 0.468100, mean_q: 0.694603\n",
      " 22674/50000: episode: 526, duration: 0.267s, episode steps: 63, steps per second: 236, episode reward: 0.766, mean reward: 0.012 [-0.007, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.128 [-0.200, 0.729], loss: 0.000068, mean_absolute_error: 0.466886, mean_q: 0.694967\n",
      " 22696/50000: episode: 527, duration: 0.077s, episode steps: 22, steps per second: 288, episode reward: 0.965, mean reward: 0.044 [-0.002, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.043 [-0.100, 0.233], loss: 0.000101, mean_absolute_error: 0.477653, mean_q: 0.710776\n",
      " 22723/50000: episode: 528, duration: 0.105s, episode steps: 27, steps per second: 256, episode reward: 0.945, mean reward: 0.035 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.066 [-0.110, 0.293], loss: 0.000033, mean_absolute_error: 0.463908, mean_q: 0.693528\n",
      " 22733/50000: episode: 529, duration: 0.043s, episode steps: 10, steps per second: 231, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 0.400 [0.000, 2.000], mean observation: 0.039 [-0.070, 0.135], loss: 0.000030, mean_absolute_error: 0.485269, mean_q: 0.728685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22785/50000: episode: 530, duration: 0.211s, episode steps: 52, steps per second: 246, episode reward: 0.756, mean reward: 0.015 [-0.008, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.160 [-0.220, 0.827], loss: 0.000110, mean_absolute_error: 0.463602, mean_q: 0.692266\n",
      " 22816/50000: episode: 531, duration: 0.148s, episode steps: 31, steps per second: 209, episode reward: 0.933, mean reward: 0.030 [-0.003, 1.000], mean action: 0.774 [0.000, 2.000], mean observation: 0.071 [-0.130, 0.336], loss: 0.000038, mean_absolute_error: 0.459730, mean_q: 0.684690\n",
      " 22818/50000: episode: 532, duration: 0.012s, episode steps: 2, steps per second: 167, episode reward: 0.999, mean reward: 0.499 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.042 [-0.100, 0.020], loss: 0.000036, mean_absolute_error: 0.472605, mean_q: 0.714024\n",
      " 22862/50000: episode: 533, duration: 0.163s, episode steps: 44, steps per second: 271, episode reward: 0.836, mean reward: 0.019 [-0.006, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.128 [-0.160, 0.614], loss: 0.000037, mean_absolute_error: 0.464306, mean_q: 0.690830\n",
      " 22926/50000: episode: 534, duration: 0.225s, episode steps: 64, steps per second: 285, episode reward: 0.679, mean reward: 0.011 [-0.009, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.174 [-0.944, 0.220], loss: 0.000333, mean_absolute_error: 0.465443, mean_q: 0.691753\n",
      " 22986/50000: episode: 535, duration: 0.214s, episode steps: 60, steps per second: 281, episode reward: 0.685, mean reward: 0.011 [-0.009, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.195 [-0.917, 0.210], loss: 0.000066, mean_absolute_error: 0.462997, mean_q: 0.696343\n",
      " 23035/50000: episode: 536, duration: 0.141s, episode steps: 49, steps per second: 348, episode reward: 0.783, mean reward: 0.016 [-0.007, 1.000], mean action: 1.163 [0.000, 2.000], mean observation: -0.156 [-0.743, 0.180], loss: 0.000033, mean_absolute_error: 0.470150, mean_q: 0.705112\n",
      " 23070/50000: episode: 537, duration: 0.102s, episode steps: 35, steps per second: 343, episode reward: 0.917, mean reward: 0.026 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.073 [-0.367, 0.150], loss: 0.000035, mean_absolute_error: 0.465906, mean_q: 0.692502\n",
      " 23136/50000: episode: 538, duration: 0.233s, episode steps: 66, steps per second: 284, episode reward: 0.635, mean reward: 0.010 [-0.010, 1.000], mean action: 0.924 [0.000, 2.000], mean observation: 0.209 [-0.260, 0.999], loss: 0.000028, mean_absolute_error: 0.465966, mean_q: 0.698774\n",
      " 23164/50000: episode: 539, duration: 0.097s, episode steps: 28, steps per second: 287, episode reward: 0.943, mean reward: 0.034 [-0.003, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.067 [-0.090, 0.303], loss: 0.000035, mean_absolute_error: 0.466975, mean_q: 0.699038\n",
      " 23211/50000: episode: 540, duration: 0.175s, episode steps: 47, steps per second: 268, episode reward: 0.853, mean reward: 0.018 [-0.006, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.086 [-0.200, 0.602], loss: 0.000322, mean_absolute_error: 0.471659, mean_q: 0.706262\n",
      " 23212/50000: episode: 541, duration: 0.008s, episode steps: 1, steps per second: 128, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.014 [-0.038, 0.010], loss: 0.000076, mean_absolute_error: 0.456679, mean_q: 0.690480\n",
      " 23213/50000: episode: 542, duration: 0.008s, episode steps: 1, steps per second: 130, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [-0.010, 0.098], loss: 0.000017, mean_absolute_error: 0.450237, mean_q: 0.680670\n",
      " 23262/50000: episode: 543, duration: 0.192s, episode steps: 49, steps per second: 255, episode reward: 0.819, mean reward: 0.017 [-0.006, 1.000], mean action: 0.898 [0.000, 2.000], mean observation: 0.132 [-0.180, 0.623], loss: 0.000033, mean_absolute_error: 0.461928, mean_q: 0.687382\n",
      " 23298/50000: episode: 544, duration: 0.160s, episode steps: 36, steps per second: 225, episode reward: 0.916, mean reward: 0.025 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.066 [-0.407, 0.160], loss: 0.000033, mean_absolute_error: 0.467220, mean_q: 0.701477\n",
      " 23346/50000: episode: 545, duration: 0.173s, episode steps: 48, steps per second: 278, episode reward: 0.805, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.143 [-0.674, 0.220], loss: 0.000040, mean_absolute_error: 0.466135, mean_q: 0.696165\n",
      " 23382/50000: episode: 546, duration: 0.103s, episode steps: 36, steps per second: 350, episode reward: 0.902, mean reward: 0.025 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.087 [-0.443, 0.170], loss: 0.000035, mean_absolute_error: 0.469294, mean_q: 0.697972\n",
      " 23395/50000: episode: 547, duration: 0.040s, episode steps: 13, steps per second: 326, episode reward: 0.984, mean reward: 0.076 [-0.002, 1.000], mean action: 1.692 [0.000, 2.000], mean observation: -0.033 [-0.165, 0.110], loss: 0.000020, mean_absolute_error: 0.454292, mean_q: 0.674772\n",
      " 23444/50000: episode: 548, duration: 0.143s, episode steps: 49, steps per second: 344, episode reward: 0.805, mean reward: 0.016 [-0.007, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.140 [-0.210, 0.691], loss: 0.000035, mean_absolute_error: 0.462614, mean_q: 0.689631\n",
      " 23450/50000: episode: 549, duration: 0.031s, episode steps: 6, steps per second: 194, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [-0.060, 0.119], loss: 0.000034, mean_absolute_error: 0.450903, mean_q: 0.682113\n",
      " 23507/50000: episode: 550, duration: 0.209s, episode steps: 57, steps per second: 272, episode reward: 0.742, mean reward: 0.013 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.147 [-0.250, 0.867], loss: 0.000316, mean_absolute_error: 0.460517, mean_q: 0.684886\n",
      " 23541/50000: episode: 551, duration: 0.161s, episode steps: 34, steps per second: 212, episode reward: 0.894, mean reward: 0.026 [-0.005, 1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.095 [-0.498, 0.210], loss: 0.000042, mean_absolute_error: 0.467939, mean_q: 0.700034\n",
      " 23602/50000: episode: 552, duration: 0.261s, episode steps: 61, steps per second: 234, episode reward: 0.671, mean reward: 0.011 [-0.010, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.194 [-0.981, 0.220], loss: 0.000031, mean_absolute_error: 0.467079, mean_q: 0.698679\n",
      " 23603/50000: episode: 553, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [-0.010, 0.086], loss: 0.000006, mean_absolute_error: 0.486158, mean_q: 0.732843\n",
      " 23659/50000: episode: 554, duration: 0.204s, episode steps: 56, steps per second: 275, episode reward: 0.701, mean reward: 0.013 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.192 [-0.922, 0.250], loss: 0.000053, mean_absolute_error: 0.467100, mean_q: 0.696427\n",
      " 23701/50000: episode: 555, duration: 0.148s, episode steps: 42, steps per second: 284, episode reward: 0.863, mean reward: 0.021 [-0.005, 1.000], mean action: 0.929 [0.000, 2.000], mean observation: 0.111 [-0.180, 0.545], loss: 0.000032, mean_absolute_error: 0.471039, mean_q: 0.702778\n",
      " 23749/50000: episode: 556, duration: 0.207s, episode steps: 48, steps per second: 231, episode reward: 0.861, mean reward: 0.018 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.078 [-0.570, 0.200], loss: 0.000393, mean_absolute_error: 0.469523, mean_q: 0.697870\n",
      " 23812/50000: episode: 557, duration: 0.244s, episode steps: 63, steps per second: 258, episode reward: 0.735, mean reward: 0.012 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.137 [-0.210, 0.855], loss: 0.000248, mean_absolute_error: 0.464082, mean_q: 0.692278\n",
      " 23854/50000: episode: 558, duration: 0.178s, episode steps: 42, steps per second: 236, episode reward: 0.873, mean reward: 0.021 [-0.005, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.103 [-0.140, 0.509], loss: 0.000147, mean_absolute_error: 0.464172, mean_q: 0.690665\n",
      " 23855/50000: episode: 559, duration: 0.007s, episode steps: 1, steps per second: 141, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.027 [-0.064, 0.010], loss: 0.000011, mean_absolute_error: 0.457276, mean_q: 0.694034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23905/50000: episode: 560, duration: 0.162s, episode steps: 50, steps per second: 308, episode reward: 0.809, mean reward: 0.016 [-0.007, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.135 [-0.666, 0.220], loss: 0.000063, mean_absolute_error: 0.470429, mean_q: 0.702890\n",
      " 23957/50000: episode: 561, duration: 0.153s, episode steps: 52, steps per second: 340, episode reward: 0.741, mean reward: 0.014 [-0.008, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.178 [-0.843, 0.210], loss: 0.000311, mean_absolute_error: 0.463658, mean_q: 0.690002\n",
      " 24007/50000: episode: 562, duration: 0.151s, episode steps: 50, steps per second: 331, episode reward: 0.816, mean reward: 0.016 [-0.006, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.121 [-0.190, 0.632], loss: 0.000079, mean_absolute_error: 0.470018, mean_q: 0.702214\n",
      " 24066/50000: episode: 563, duration: 0.199s, episode steps: 59, steps per second: 297, episode reward: 0.727, mean reward: 0.012 [-0.009, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.166 [-0.230, 0.866], loss: 0.000029, mean_absolute_error: 0.462872, mean_q: 0.691249\n",
      " 24093/50000: episode: 564, duration: 0.101s, episode steps: 27, steps per second: 268, episode reward: 0.948, mean reward: 0.035 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.058 [-0.120, 0.289], loss: 0.000084, mean_absolute_error: 0.474191, mean_q: 0.708302\n",
      " 24110/50000: episode: 565, duration: 0.053s, episode steps: 17, steps per second: 318, episode reward: 0.975, mean reward: 0.057 [-0.002, 1.000], mean action: 0.588 [0.000, 2.000], mean observation: 0.048 [-0.090, 0.198], loss: 0.000049, mean_absolute_error: 0.468711, mean_q: 0.706026\n",
      " 24145/50000: episode: 566, duration: 0.135s, episode steps: 35, steps per second: 260, episode reward: 0.939, mean reward: 0.027 [-0.003, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.061 [-0.284, 0.110], loss: 0.000396, mean_absolute_error: 0.461337, mean_q: 0.688071\n",
      " 24202/50000: episode: 567, duration: 0.227s, episode steps: 57, steps per second: 251, episode reward: 0.691, mean reward: 0.012 [-0.009, 1.000], mean action: 0.877 [0.000, 2.000], mean observation: 0.198 [-0.260, 0.932], loss: 0.000038, mean_absolute_error: 0.461326, mean_q: 0.690751\n",
      " 24203/50000: episode: 568, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.011 [-0.010, 0.032], loss: 0.000018, mean_absolute_error: 0.481922, mean_q: 0.730711\n",
      " 24250/50000: episode: 569, duration: 0.191s, episode steps: 47, steps per second: 246, episode reward: 0.806, mean reward: 0.017 [-0.006, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.149 [-0.638, 0.190], loss: 0.000026, mean_absolute_error: 0.465785, mean_q: 0.691864\n",
      " 24275/50000: episode: 570, duration: 0.099s, episode steps: 25, steps per second: 253, episode reward: 0.947, mean reward: 0.038 [-0.003, 1.000], mean action: 0.720 [0.000, 2.000], mean observation: 0.066 [-0.130, 0.307], loss: 0.000041, mean_absolute_error: 0.475755, mean_q: 0.709888\n",
      " 24315/50000: episode: 571, duration: 0.131s, episode steps: 40, steps per second: 306, episode reward: 0.868, mean reward: 0.022 [-0.005, 1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.110 [-0.549, 0.190], loss: 0.000037, mean_absolute_error: 0.473710, mean_q: 0.707051\n",
      " 24316/50000: episode: 572, duration: 0.006s, episode steps: 1, steps per second: 173, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.029 [-0.069, 0.010], loss: 0.000039, mean_absolute_error: 0.516491, mean_q: 0.746666\n",
      " 24371/50000: episode: 573, duration: 0.162s, episode steps: 55, steps per second: 339, episode reward: 0.745, mean reward: 0.014 [-0.008, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.166 [-0.220, 0.806], loss: 0.000026, mean_absolute_error: 0.469626, mean_q: 0.704197\n",
      " 24420/50000: episode: 574, duration: 0.159s, episode steps: 49, steps per second: 309, episode reward: 0.776, mean reward: 0.016 [-0.008, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.161 [-0.769, 0.220], loss: 0.000025, mean_absolute_error: 0.460567, mean_q: 0.688897\n",
      " 24467/50000: episode: 575, duration: 0.133s, episode steps: 47, steps per second: 353, episode reward: 0.876, mean reward: 0.019 [-0.005, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.080 [-0.498, 0.180], loss: 0.000166, mean_absolute_error: 0.478481, mean_q: 0.715094\n",
      " 24484/50000: episode: 576, duration: 0.054s, episode steps: 17, steps per second: 315, episode reward: 0.975, mean reward: 0.057 [-0.002, 1.000], mean action: 1.529 [0.000, 2.000], mean observation: -0.048 [-0.189, 0.110], loss: 0.000045, mean_absolute_error: 0.472362, mean_q: 0.712067\n",
      " 24536/50000: episode: 577, duration: 0.159s, episode steps: 52, steps per second: 326, episode reward: 0.814, mean reward: 0.016 [-0.007, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.111 [-0.220, 0.686], loss: 0.000035, mean_absolute_error: 0.470822, mean_q: 0.701662\n",
      " 24568/50000: episode: 578, duration: 0.092s, episode steps: 32, steps per second: 346, episode reward: 0.957, mean reward: 0.030 [-0.002, 1.000], mean action: 1.219 [0.000, 2.000], mean observation: -0.055 [-0.187, 0.080], loss: 0.000479, mean_absolute_error: 0.463148, mean_q: 0.687660\n",
      " 24622/50000: episode: 579, duration: 0.180s, episode steps: 54, steps per second: 300, episode reward: 0.701, mean reward: 0.013 [-0.009, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.199 [-0.936, 0.270], loss: 0.000060, mean_absolute_error: 0.468196, mean_q: 0.700476\n",
      " 24672/50000: episode: 580, duration: 0.155s, episode steps: 50, steps per second: 323, episode reward: 0.793, mean reward: 0.016 [-0.007, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.137 [-0.210, 0.733], loss: 0.000040, mean_absolute_error: 0.478629, mean_q: 0.715838\n",
      " 24715/50000: episode: 581, duration: 0.136s, episode steps: 43, steps per second: 316, episode reward: 0.821, mean reward: 0.019 [-0.007, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.140 [-0.220, 0.685], loss: 0.000234, mean_absolute_error: 0.472363, mean_q: 0.708639\n",
      " 24752/50000: episode: 582, duration: 0.125s, episode steps: 37, steps per second: 295, episode reward: 0.892, mean reward: 0.024 [-0.005, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.094 [-0.460, 0.160], loss: 0.000034, mean_absolute_error: 0.476841, mean_q: 0.715404\n",
      " 24800/50000: episode: 583, duration: 0.142s, episode steps: 48, steps per second: 338, episode reward: 0.816, mean reward: 0.017 [-0.006, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.134 [-0.200, 0.649], loss: 0.000040, mean_absolute_error: 0.471573, mean_q: 0.706246\n",
      " 24809/50000: episode: 584, duration: 0.029s, episode steps: 9, steps per second: 315, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.090, 0.140], loss: 0.000036, mean_absolute_error: 0.459464, mean_q: 0.667772\n",
      " 24850/50000: episode: 585, duration: 0.142s, episode steps: 41, steps per second: 289, episode reward: 0.847, mean reward: 0.021 [-0.006, 1.000], mean action: 1.195 [0.000, 2.000], mean observation: -0.126 [-0.603, 0.200], loss: 0.000027, mean_absolute_error: 0.477053, mean_q: 0.717036\n",
      " 24878/50000: episode: 586, duration: 0.090s, episode steps: 28, steps per second: 311, episode reward: 0.972, mean reward: 0.035 [-0.002, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.002 [-0.150, 0.188], loss: 0.000036, mean_absolute_error: 0.465814, mean_q: 0.697001\n",
      " 24926/50000: episode: 587, duration: 0.143s, episode steps: 48, steps per second: 335, episode reward: 0.794, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.150 [-0.720, 0.220], loss: 0.000050, mean_absolute_error: 0.473786, mean_q: 0.707182\n",
      " 24964/50000: episode: 588, duration: 0.114s, episode steps: 38, steps per second: 332, episode reward: 0.892, mean reward: 0.023 [-0.005, 1.000], mean action: 0.895 [0.000, 2.000], mean observation: 0.095 [-0.170, 0.470], loss: 0.000056, mean_absolute_error: 0.466354, mean_q: 0.696171\n",
      " 25015/50000: episode: 589, duration: 0.140s, episode steps: 51, steps per second: 363, episode reward: 0.793, mean reward: 0.016 [-0.007, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.134 [-0.190, 0.729], loss: 0.000041, mean_absolute_error: 0.472504, mean_q: 0.706975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25066/50000: episode: 590, duration: 0.161s, episode steps: 51, steps per second: 317, episode reward: 0.809, mean reward: 0.016 [-0.007, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.117 [-0.210, 0.696], loss: 0.000305, mean_absolute_error: 0.469857, mean_q: 0.701501\n",
      " 25114/50000: episode: 591, duration: 0.188s, episode steps: 48, steps per second: 256, episode reward: 0.822, mean reward: 0.017 [-0.006, 1.000], mean action: 0.896 [0.000, 2.000], mean observation: 0.130 [-0.180, 0.639], loss: 0.000077, mean_absolute_error: 0.472929, mean_q: 0.705327\n",
      " 25127/50000: episode: 592, duration: 0.055s, episode steps: 13, steps per second: 238, episode reward: 0.983, mean reward: 0.076 [-0.002, 1.000], mean action: 1.538 [0.000, 2.000], mean observation: -0.041 [-0.171, 0.090], loss: 0.000024, mean_absolute_error: 0.466027, mean_q: 0.699764\n",
      " 25163/50000: episode: 593, duration: 0.143s, episode steps: 36, steps per second: 251, episode reward: 0.908, mean reward: 0.025 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.075 [-0.429, 0.160], loss: 0.000027, mean_absolute_error: 0.475412, mean_q: 0.712728\n",
      " 25192/50000: episode: 594, duration: 0.129s, episode steps: 29, steps per second: 224, episode reward: 0.939, mean reward: 0.032 [-0.003, 1.000], mean action: 0.724 [0.000, 2.000], mean observation: 0.068 [-0.130, 0.317], loss: 0.000029, mean_absolute_error: 0.482582, mean_q: 0.723719\n",
      " 25238/50000: episode: 595, duration: 0.161s, episode steps: 46, steps per second: 286, episode reward: 0.828, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.129 [-0.626, 0.200], loss: 0.000025, mean_absolute_error: 0.475804, mean_q: 0.711213\n",
      " 25264/50000: episode: 596, duration: 0.083s, episode steps: 26, steps per second: 314, episode reward: 0.943, mean reward: 0.036 [-0.003, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.067 [-0.326, 0.140], loss: 0.000029, mean_absolute_error: 0.474589, mean_q: 0.710136\n",
      " 25281/50000: episode: 597, duration: 0.054s, episode steps: 17, steps per second: 317, episode reward: 0.975, mean reward: 0.057 [-0.002, 1.000], mean action: 0.706 [0.000, 2.000], mean observation: 0.046 [-0.100, 0.196], loss: 0.000019, mean_absolute_error: 0.463234, mean_q: 0.699446\n",
      " 25311/50000: episode: 598, duration: 0.086s, episode steps: 30, steps per second: 347, episode reward: 0.920, mean reward: 0.031 [-0.004, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.081 [-0.180, 0.406], loss: 0.000302, mean_absolute_error: 0.468312, mean_q: 0.700081\n",
      " 25345/50000: episode: 599, duration: 0.111s, episode steps: 34, steps per second: 306, episode reward: 0.939, mean reward: 0.028 [-0.002, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.074 [-0.227, 0.090], loss: 0.000147, mean_absolute_error: 0.478772, mean_q: 0.717944\n",
      " 25422/50000: episode: 600, duration: 0.259s, episode steps: 77, steps per second: 297, episode reward: 0.832, mean reward: 0.011 [-0.005, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: 0.019 [-0.164, 0.519], loss: 0.000037, mean_absolute_error: 0.472562, mean_q: 0.709266\n",
      " 25468/50000: episode: 601, duration: 0.159s, episode steps: 46, steps per second: 289, episode reward: 0.846, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.105 [-0.605, 0.200], loss: 0.000025, mean_absolute_error: 0.471748, mean_q: 0.705530\n",
      " 25527/50000: episode: 602, duration: 0.181s, episode steps: 59, steps per second: 325, episode reward: 0.687, mean reward: 0.012 [-0.009, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.190 [-0.270, 0.946], loss: 0.000029, mean_absolute_error: 0.478110, mean_q: 0.718538\n",
      " 25538/50000: episode: 603, duration: 0.035s, episode steps: 11, steps per second: 314, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 1.818 [0.000, 2.000], mean observation: -0.035 [-0.152, 0.100], loss: 0.000024, mean_absolute_error: 0.477277, mean_q: 0.712796\n",
      " 25604/50000: episode: 604, duration: 0.198s, episode steps: 66, steps per second: 333, episode reward: 0.661, mean reward: 0.010 [-0.010, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.185 [-0.260, 0.956], loss: 0.000031, mean_absolute_error: 0.475996, mean_q: 0.709078\n",
      " 25645/50000: episode: 605, duration: 0.127s, episode steps: 41, steps per second: 322, episode reward: 0.859, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.114 [-0.190, 0.571], loss: 0.000029, mean_absolute_error: 0.477077, mean_q: 0.711705\n",
      " 25702/50000: episode: 606, duration: 0.175s, episode steps: 57, steps per second: 325, episode reward: 0.759, mean reward: 0.013 [-0.008, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.146 [-0.790, 0.210], loss: 0.000043, mean_absolute_error: 0.470419, mean_q: 0.705475\n",
      " 25754/50000: episode: 607, duration: 0.153s, episode steps: 52, steps per second: 340, episode reward: 0.757, mean reward: 0.015 [-0.008, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.168 [-0.786, 0.200], loss: 0.000074, mean_absolute_error: 0.471313, mean_q: 0.706230\n",
      " 25850/50000: episode: 608, duration: 0.281s, episode steps: 96, steps per second: 342, episode reward: 0.660, mean reward: 0.007 [-0.009, 1.000], mean action: 1.094 [0.000, 2.000], mean observation: 0.064 [-0.250, 0.907], loss: 0.000123, mean_absolute_error: 0.473181, mean_q: 0.708554\n",
      " 25913/50000: episode: 609, duration: 0.189s, episode steps: 63, steps per second: 333, episode reward: 0.720, mean reward: 0.011 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.150 [-0.250, 0.863], loss: 0.000041, mean_absolute_error: 0.472060, mean_q: 0.703386\n",
      " 25937/50000: episode: 610, duration: 0.072s, episode steps: 24, steps per second: 335, episode reward: 0.957, mean reward: 0.040 [-0.003, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.059 [-0.100, 0.255], loss: 0.000028, mean_absolute_error: 0.474982, mean_q: 0.709096\n",
      " 25982/50000: episode: 611, duration: 0.148s, episode steps: 45, steps per second: 305, episode reward: 0.824, mean reward: 0.018 [-0.006, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.136 [-0.637, 0.200], loss: 0.000053, mean_absolute_error: 0.473001, mean_q: 0.707179\n",
      " 26026/50000: episode: 612, duration: 0.138s, episode steps: 44, steps per second: 319, episode reward: 0.831, mean reward: 0.019 [-0.006, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.131 [-0.190, 0.634], loss: 0.000034, mean_absolute_error: 0.473068, mean_q: 0.706165\n",
      " 26069/50000: episode: 613, duration: 0.130s, episode steps: 43, steps per second: 331, episode reward: 0.890, mean reward: 0.021 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.060 [-0.180, 0.498], loss: 0.000117, mean_absolute_error: 0.468903, mean_q: 0.705031\n",
      " 26109/50000: episode: 614, duration: 0.121s, episode steps: 40, steps per second: 330, episode reward: 0.892, mean reward: 0.022 [-0.004, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.092 [-0.438, 0.150], loss: 0.000033, mean_absolute_error: 0.475245, mean_q: 0.714274\n",
      " 26175/50000: episode: 615, duration: 0.196s, episode steps: 66, steps per second: 337, episode reward: 0.632, mean reward: 0.010 [-0.010, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.211 [-0.995, 0.210], loss: 0.000027, mean_absolute_error: 0.472030, mean_q: 0.708059\n",
      " 26218/50000: episode: 616, duration: 0.128s, episode steps: 43, steps per second: 335, episode reward: 0.859, mean reward: 0.020 [-0.005, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.112 [-0.539, 0.160], loss: 0.000025, mean_absolute_error: 0.479801, mean_q: 0.721083\n",
      " 26238/50000: episode: 617, duration: 0.070s, episode steps: 20, steps per second: 286, episode reward: 0.974, mean reward: 0.049 [-0.002, 1.000], mean action: 0.550 [0.000, 2.000], mean observation: 0.029 [-0.110, 0.196], loss: 0.000017, mean_absolute_error: 0.468925, mean_q: 0.705301\n",
      " 26260/50000: episode: 618, duration: 0.063s, episode steps: 22, steps per second: 351, episode reward: 0.957, mean reward: 0.044 [-0.003, 1.000], mean action: 1.409 [0.000, 2.000], mean observation: -0.056 [-0.278, 0.120], loss: 0.000016, mean_absolute_error: 0.470799, mean_q: 0.705477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26299/50000: episode: 619, duration: 0.117s, episode steps: 39, steps per second: 335, episode reward: 0.891, mean reward: 0.023 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.087 [-0.473, 0.170], loss: 0.000030, mean_absolute_error: 0.475335, mean_q: 0.714991\n",
      " 26352/50000: episode: 620, duration: 0.153s, episode steps: 53, steps per second: 347, episode reward: 0.826, mean reward: 0.016 [-0.006, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.101 [-0.180, 0.637], loss: 0.000035, mean_absolute_error: 0.470452, mean_q: 0.704315\n",
      " 26401/50000: episode: 621, duration: 0.140s, episode steps: 49, steps per second: 349, episode reward: 0.859, mean reward: 0.018 [-0.006, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.078 [-0.180, 0.566], loss: 0.000032, mean_absolute_error: 0.472009, mean_q: 0.707742\n",
      " 26419/50000: episode: 622, duration: 0.053s, episode steps: 18, steps per second: 337, episode reward: 0.971, mean reward: 0.054 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.050 [-0.220, 0.100], loss: 0.000021, mean_absolute_error: 0.465308, mean_q: 0.696552\n",
      " 26469/50000: episode: 623, duration: 0.144s, episode steps: 50, steps per second: 346, episode reward: 0.790, mean reward: 0.016 [-0.007, 1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.153 [-0.684, 0.180], loss: 0.000050, mean_absolute_error: 0.477907, mean_q: 0.713560\n",
      " 26520/50000: episode: 624, duration: 0.188s, episode steps: 51, steps per second: 271, episode reward: 0.768, mean reward: 0.015 [-0.008, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.162 [-0.763, 0.220], loss: 0.000026, mean_absolute_error: 0.469359, mean_q: 0.704536\n",
      " 26551/50000: episode: 625, duration: 0.108s, episode steps: 31, steps per second: 287, episode reward: 0.931, mean reward: 0.030 [-0.003, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.069 [-0.350, 0.140], loss: 0.000024, mean_absolute_error: 0.474663, mean_q: 0.707567\n",
      " 26613/50000: episode: 626, duration: 0.313s, episode steps: 62, steps per second: 198, episode reward: 0.671, mean reward: 0.011 [-0.010, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.196 [-0.967, 0.220], loss: 0.000031, mean_absolute_error: 0.476198, mean_q: 0.714378\n",
      " 26666/50000: episode: 627, duration: 0.227s, episode steps: 53, steps per second: 233, episode reward: 0.765, mean reward: 0.014 [-0.008, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.157 [-0.789, 0.210], loss: 0.000024, mean_absolute_error: 0.481031, mean_q: 0.719158\n",
      " 26726/50000: episode: 628, duration: 0.269s, episode steps: 60, steps per second: 223, episode reward: 0.742, mean reward: 0.012 [-0.008, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.145 [-0.230, 0.839], loss: 0.000027, mean_absolute_error: 0.475263, mean_q: 0.711946\n",
      " 26787/50000: episode: 629, duration: 0.416s, episode steps: 61, steps per second: 147, episode reward: 0.693, mean reward: 0.011 [-0.009, 1.000], mean action: 1.066 [0.000, 2.000], mean observation: -0.188 [-0.878, 0.230], loss: 0.000095, mean_absolute_error: 0.473615, mean_q: 0.709123\n",
      " 26846/50000: episode: 630, duration: 0.379s, episode steps: 59, steps per second: 156, episode reward: 0.675, mean reward: 0.011 [-0.010, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.201 [-0.260, 0.977], loss: 0.000075, mean_absolute_error: 0.478712, mean_q: 0.720589\n",
      " 26882/50000: episode: 631, duration: 0.211s, episode steps: 36, steps per second: 171, episode reward: 0.898, mean reward: 0.025 [-0.004, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.089 [-0.160, 0.448], loss: 0.000033, mean_absolute_error: 0.476557, mean_q: 0.713726\n",
      " 26922/50000: episode: 632, duration: 0.236s, episode steps: 40, steps per second: 170, episode reward: 0.889, mean reward: 0.022 [-0.005, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.080 [-0.160, 0.485], loss: 0.000029, mean_absolute_error: 0.476348, mean_q: 0.712381\n",
      " 26923/50000: episode: 633, duration: 0.007s, episode steps: 1, steps per second: 135, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.019 [-0.049, 0.010], loss: 0.000019, mean_absolute_error: 0.418795, mean_q: 0.630879\n",
      " 26930/50000: episode: 634, duration: 0.030s, episode steps: 7, steps per second: 233, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.070, 0.126], loss: 0.000017, mean_absolute_error: 0.463275, mean_q: 0.686573\n",
      " 26951/50000: episode: 635, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 0.964, mean reward: 0.046 [-0.002, 1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.054 [-0.234, 0.100], loss: 0.000046, mean_absolute_error: 0.475433, mean_q: 0.707691\n",
      " 26952/50000: episode: 636, duration: 0.007s, episode steps: 1, steps per second: 135, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.022 [-0.034, -0.010], loss: 0.000012, mean_absolute_error: 0.452505, mean_q: 0.686075\n",
      " 27048/50000: episode: 637, duration: 0.564s, episode steps: 96, steps per second: 170, episode reward: 0.698, mean reward: 0.007 [-0.008, 1.000], mean action: 1.062 [0.000, 2.000], mean observation: 0.058 [-0.220, 0.810], loss: 0.000032, mean_absolute_error: 0.479305, mean_q: 0.719167\n",
      " 27099/50000: episode: 638, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 0.791, mean reward: 0.016 [-0.007, 1.000], mean action: 1.098 [0.000, 2.000], mean observation: -0.145 [-0.717, 0.210], loss: 0.000043, mean_absolute_error: 0.480756, mean_q: 0.720649\n",
      " 27144/50000: episode: 639, duration: 0.249s, episode steps: 45, steps per second: 181, episode reward: 0.824, mean reward: 0.018 [-0.007, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.135 [-0.650, 0.200], loss: 0.000037, mean_absolute_error: 0.479275, mean_q: 0.716908\n",
      " 27166/50000: episode: 640, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 0.967, mean reward: 0.044 [-0.002, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.039 [-0.130, 0.212], loss: 0.000128, mean_absolute_error: 0.478432, mean_q: 0.721740\n",
      " 27227/50000: episode: 641, duration: 0.329s, episode steps: 61, steps per second: 186, episode reward: 0.693, mean reward: 0.011 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.178 [-0.240, 0.926], loss: 0.000026, mean_absolute_error: 0.474638, mean_q: 0.715107\n",
      " 27253/50000: episode: 642, duration: 0.183s, episode steps: 26, steps per second: 142, episode reward: 0.946, mean reward: 0.036 [-0.003, 1.000], mean action: 1.192 [0.000, 2.000], mean observation: -0.064 [-0.311, 0.120], loss: 0.000026, mean_absolute_error: 0.480335, mean_q: 0.722681\n",
      " 27314/50000: episode: 643, duration: 0.365s, episode steps: 61, steps per second: 167, episode reward: 0.758, mean reward: 0.012 [-0.008, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.133 [-0.210, 0.787], loss: 0.000190, mean_absolute_error: 0.481120, mean_q: 0.722687\n",
      " 27338/50000: episode: 644, duration: 0.164s, episode steps: 24, steps per second: 146, episode reward: 0.960, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.047 [-0.250, 0.110], loss: 0.000036, mean_absolute_error: 0.480196, mean_q: 0.723152\n",
      " 27382/50000: episode: 645, duration: 0.235s, episode steps: 44, steps per second: 188, episode reward: 0.843, mean reward: 0.019 [-0.006, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.116 [-0.190, 0.612], loss: 0.000027, mean_absolute_error: 0.478578, mean_q: 0.717557\n",
      " 27529/50000: episode: 646, duration: 0.954s, episode steps: 147, steps per second: 154, episode reward: 0.693, mean reward: 0.005 [-0.005, 1.000], mean action: 1.034 [0.000, 2.000], mean observation: -0.093 [-0.456, 0.170], loss: 0.000214, mean_absolute_error: 0.480066, mean_q: 0.719708\n",
      " 27557/50000: episode: 647, duration: 0.167s, episode steps: 28, steps per second: 168, episode reward: 0.951, mean reward: 0.034 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.040 [-0.150, 0.289], loss: 0.000021, mean_absolute_error: 0.481299, mean_q: 0.722425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27578/50000: episode: 648, duration: 0.156s, episode steps: 21, steps per second: 134, episode reward: 0.963, mean reward: 0.046 [-0.003, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.047 [-0.140, 0.255], loss: 0.000034, mean_absolute_error: 0.490912, mean_q: 0.740421\n",
      " 27616/50000: episode: 649, duration: 0.170s, episode steps: 38, steps per second: 223, episode reward: 0.904, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.074 [-0.150, 0.428], loss: 0.000024, mean_absolute_error: 0.485211, mean_q: 0.729828\n",
      " 27665/50000: episode: 650, duration: 0.309s, episode steps: 49, steps per second: 159, episode reward: 0.790, mean reward: 0.016 [-0.007, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.150 [-0.734, 0.220], loss: 0.000024, mean_absolute_error: 0.483769, mean_q: 0.725011\n",
      " 27716/50000: episode: 651, duration: 0.304s, episode steps: 51, steps per second: 168, episode reward: 0.778, mean reward: 0.015 [-0.008, 1.000], mean action: 1.098 [0.000, 2.000], mean observation: -0.154 [-0.755, 0.180], loss: 0.000297, mean_absolute_error: 0.477553, mean_q: 0.715300\n",
      " 27743/50000: episode: 652, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 0.947, mean reward: 0.035 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.048 [-0.160, 0.312], loss: 0.000028, mean_absolute_error: 0.481515, mean_q: 0.723945\n",
      " 27744/50000: episode: 653, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [-0.010, 0.034], loss: 0.000015, mean_absolute_error: 0.489233, mean_q: 0.691347\n",
      " 27745/50000: episode: 654, duration: 0.008s, episode steps: 1, steps per second: 119, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.092, 0.010], loss: 0.000014, mean_absolute_error: 0.483693, mean_q: 0.679467\n",
      " 27746/50000: episode: 655, duration: 0.008s, episode steps: 1, steps per second: 120, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [-0.010, 0.019], loss: 0.000007, mean_absolute_error: 0.457046, mean_q: 0.667145\n",
      " 27788/50000: episode: 656, duration: 0.187s, episode steps: 42, steps per second: 224, episode reward: 0.852, mean reward: 0.020 [-0.006, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.120 [-0.579, 0.190], loss: 0.000030, mean_absolute_error: 0.481512, mean_q: 0.724755\n",
      " 27789/50000: episode: 657, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [-0.010, 0.035], loss: 0.000030, mean_absolute_error: 0.460910, mean_q: 0.697334\n",
      " 27850/50000: episode: 658, duration: 0.312s, episode steps: 61, steps per second: 195, episode reward: 0.668, mean reward: 0.011 [-0.010, 1.000], mean action: 1.082 [0.000, 2.000], mean observation: -0.202 [-0.961, 0.220], loss: 0.000022, mean_absolute_error: 0.485196, mean_q: 0.729156\n",
      " 27875/50000: episode: 659, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 0.948, mean reward: 0.038 [-0.003, 1.000], mean action: 1.320 [0.000, 2.000], mean observation: -0.066 [-0.288, 0.130], loss: 0.000022, mean_absolute_error: 0.473390, mean_q: 0.711630\n",
      " 27914/50000: episode: 660, duration: 0.237s, episode steps: 39, steps per second: 165, episode reward: 0.879, mean reward: 0.023 [-0.005, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.103 [-0.515, 0.180], loss: 0.000021, mean_absolute_error: 0.486917, mean_q: 0.732351\n",
      " 27964/50000: episode: 661, duration: 0.249s, episode steps: 50, steps per second: 201, episode reward: 0.809, mean reward: 0.016 [-0.007, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.128 [-0.190, 0.675], loss: 0.000024, mean_absolute_error: 0.483350, mean_q: 0.724787\n",
      " 28023/50000: episode: 662, duration: 0.369s, episode steps: 59, steps per second: 160, episode reward: 0.695, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.188 [-0.895, 0.230], loss: 0.000374, mean_absolute_error: 0.482129, mean_q: 0.725282\n",
      " 28076/50000: episode: 663, duration: 0.263s, episode steps: 53, steps per second: 202, episode reward: 0.764, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.154 [-0.230, 0.789], loss: 0.000032, mean_absolute_error: 0.485523, mean_q: 0.730699\n",
      " 28135/50000: episode: 664, duration: 0.285s, episode steps: 59, steps per second: 207, episode reward: 0.710, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.176 [-0.873, 0.220], loss: 0.000041, mean_absolute_error: 0.476859, mean_q: 0.712591\n",
      " 28136/50000: episode: 665, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [-0.010, 0.050], loss: 0.000034, mean_absolute_error: 0.519114, mean_q: 0.784950\n",
      " 28137/50000: episode: 666, duration: 0.007s, episode steps: 1, steps per second: 134, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.015 [-0.020, -0.010], loss: 0.000010, mean_absolute_error: 0.470226, mean_q: 0.714933\n",
      " 28189/50000: episode: 667, duration: 0.286s, episode steps: 52, steps per second: 182, episode reward: 0.786, mean reward: 0.015 [-0.007, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.139 [-0.736, 0.200], loss: 0.000070, mean_absolute_error: 0.485141, mean_q: 0.730360\n",
      " 28252/50000: episode: 668, duration: 0.299s, episode steps: 63, steps per second: 211, episode reward: 0.669, mean reward: 0.011 [-0.010, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.190 [-0.250, 0.973], loss: 0.000036, mean_absolute_error: 0.479283, mean_q: 0.719433\n",
      " 28310/50000: episode: 669, duration: 0.302s, episode steps: 58, steps per second: 192, episode reward: 0.736, mean reward: 0.013 [-0.008, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.160 [-0.230, 0.830], loss: 0.000020, mean_absolute_error: 0.479139, mean_q: 0.721288\n",
      " 28374/50000: episode: 670, duration: 0.309s, episode steps: 64, steps per second: 207, episode reward: 0.639, mean reward: 0.010 [-0.010, 1.000], mean action: 1.109 [0.000, 2.000], mean observation: -0.215 [-0.975, 0.210], loss: 0.000323, mean_absolute_error: 0.480840, mean_q: 0.721540\n",
      " 28375/50000: episode: 671, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [-0.010, 0.036], loss: 0.000117, mean_absolute_error: 0.464926, mean_q: 0.703273\n",
      " 28402/50000: episode: 672, duration: 0.116s, episode steps: 27, steps per second: 233, episode reward: 0.942, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.067 [-0.306, 0.140], loss: 0.000065, mean_absolute_error: 0.487057, mean_q: 0.731553\n",
      " 28413/50000: episode: 673, duration: 0.040s, episode steps: 11, steps per second: 274, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.039 [-0.090, 0.153], loss: 0.000027, mean_absolute_error: 0.479664, mean_q: 0.722190\n",
      " 28460/50000: episode: 674, duration: 0.181s, episode steps: 47, steps per second: 260, episode reward: 0.819, mean reward: 0.017 [-0.006, 1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.138 [-0.620, 0.180], loss: 0.000031, mean_absolute_error: 0.491878, mean_q: 0.739126\n",
      " 28519/50000: episode: 675, duration: 0.228s, episode steps: 59, steps per second: 259, episode reward: 0.787, mean reward: 0.013 [-0.007, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.114 [-0.220, 0.732], loss: 0.000277, mean_absolute_error: 0.480316, mean_q: 0.720313\n",
      " 28549/50000: episode: 676, duration: 0.138s, episode steps: 30, steps per second: 217, episode reward: 0.921, mean reward: 0.031 [-0.004, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.080 [-0.408, 0.180], loss: 0.000062, mean_absolute_error: 0.487740, mean_q: 0.735871\n",
      " 28550/50000: episode: 677, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.017 [-0.023, -0.010], loss: 0.000014, mean_absolute_error: 0.488782, mean_q: 0.715488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28572/50000: episode: 678, duration: 0.165s, episode steps: 22, steps per second: 134, episode reward: 0.963, mean reward: 0.044 [-0.003, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.043 [-0.140, 0.254], loss: 0.000034, mean_absolute_error: 0.491555, mean_q: 0.740211\n",
      " 28622/50000: episode: 679, duration: 0.311s, episode steps: 50, steps per second: 161, episode reward: 0.787, mean reward: 0.016 [-0.007, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.149 [-0.190, 0.734], loss: 0.000024, mean_absolute_error: 0.484370, mean_q: 0.729645\n",
      " 28668/50000: episode: 680, duration: 0.294s, episode steps: 46, steps per second: 157, episode reward: 0.886, mean reward: 0.019 [-0.005, 1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.086 [-0.455, 0.170], loss: 0.000030, mean_absolute_error: 0.481114, mean_q: 0.725535\n",
      " 28691/50000: episode: 681, duration: 0.183s, episode steps: 23, steps per second: 126, episode reward: 0.958, mean reward: 0.042 [-0.003, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.054 [-0.110, 0.267], loss: 0.000037, mean_absolute_error: 0.486815, mean_q: 0.733159\n",
      " 28745/50000: episode: 682, duration: 0.426s, episode steps: 54, steps per second: 127, episode reward: 0.768, mean reward: 0.014 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.147 [-0.779, 0.230], loss: 0.000217, mean_absolute_error: 0.492347, mean_q: 0.738240\n",
      " 28806/50000: episode: 683, duration: 0.435s, episode steps: 61, steps per second: 140, episode reward: 0.684, mean reward: 0.011 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.190 [-0.250, 0.936], loss: 0.000028, mean_absolute_error: 0.479459, mean_q: 0.721199\n",
      " 28854/50000: episode: 684, duration: 0.387s, episode steps: 48, steps per second: 124, episode reward: 0.794, mean reward: 0.017 [-0.007, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.150 [-0.200, 0.725], loss: 0.000021, mean_absolute_error: 0.481048, mean_q: 0.722134\n",
      " 28855/50000: episode: 685, duration: 0.008s, episode steps: 1, steps per second: 118, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [-0.010, 0.039], loss: 0.000010, mean_absolute_error: 0.512555, mean_q: 0.775325\n",
      " 28856/50000: episode: 686, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [-0.010, 0.017], loss: 0.000028, mean_absolute_error: 0.482713, mean_q: 0.731874\n",
      " 28902/50000: episode: 687, duration: 0.266s, episode steps: 46, steps per second: 173, episode reward: 0.832, mean reward: 0.018 [-0.006, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.125 [-0.190, 0.629], loss: 0.000047, mean_absolute_error: 0.489054, mean_q: 0.733464\n",
      " 28958/50000: episode: 688, duration: 0.249s, episode steps: 56, steps per second: 224, episode reward: 0.704, mean reward: 0.013 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.190 [-0.250, 0.933], loss: 0.000023, mean_absolute_error: 0.482478, mean_q: 0.726482\n",
      " 28968/50000: episode: 689, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 0.988, mean reward: 0.099 [-0.001, 1.000], mean action: 0.200 [0.000, 2.000], mean observation: 0.036 [-0.090, 0.146], loss: 0.000045, mean_absolute_error: 0.489665, mean_q: 0.730303\n",
      " 28978/50000: episode: 690, duration: 0.068s, episode steps: 10, steps per second: 146, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 0.300 [0.000, 2.000], mean observation: 0.038 [-0.070, 0.133], loss: 0.000029, mean_absolute_error: 0.469074, mean_q: 0.698872\n",
      " 29001/50000: episode: 691, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 0.961, mean reward: 0.042 [-0.002, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.050 [-0.110, 0.240], loss: 0.000644, mean_absolute_error: 0.490659, mean_q: 0.737410\n",
      " 29002/50000: episode: 692, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.044 [-0.098, 0.010], loss: 0.000314, mean_absolute_error: 0.477739, mean_q: 0.686122\n",
      " 29041/50000: episode: 693, duration: 0.134s, episode steps: 39, steps per second: 292, episode reward: 0.900, mean reward: 0.023 [-0.004, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.077 [-0.160, 0.423], loss: 0.000072, mean_absolute_error: 0.486538, mean_q: 0.733108\n",
      " 29084/50000: episode: 694, duration: 0.126s, episode steps: 43, steps per second: 342, episode reward: 0.903, mean reward: 0.021 [-0.004, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.079 [-0.394, 0.130], loss: 0.000088, mean_absolute_error: 0.492368, mean_q: 0.740017\n",
      " 29134/50000: episode: 695, duration: 0.184s, episode steps: 50, steps per second: 272, episode reward: 0.819, mean reward: 0.016 [-0.007, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.126 [-0.652, 0.200], loss: 0.000029, mean_absolute_error: 0.485512, mean_q: 0.729590\n",
      " 29165/50000: episode: 696, duration: 0.105s, episode steps: 31, steps per second: 295, episode reward: 0.943, mean reward: 0.030 [-0.003, 1.000], mean action: 0.710 [0.000, 2.000], mean observation: 0.049 [-0.140, 0.308], loss: 0.000037, mean_absolute_error: 0.486983, mean_q: 0.729618\n",
      " 29166/50000: episode: 697, duration: 0.006s, episode steps: 1, steps per second: 163, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [-0.010, 0.045], loss: 0.000038, mean_absolute_error: 0.493847, mean_q: 0.747421\n",
      " 29188/50000: episode: 698, duration: 0.065s, episode steps: 22, steps per second: 338, episode reward: 0.963, mean reward: 0.044 [-0.002, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.055 [-0.110, 0.221], loss: 0.000044, mean_absolute_error: 0.487098, mean_q: 0.729515\n",
      " 29218/50000: episode: 699, duration: 0.086s, episode steps: 30, steps per second: 350, episode reward: 0.924, mean reward: 0.031 [-0.004, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.081 [-0.160, 0.372], loss: 0.000119, mean_absolute_error: 0.485686, mean_q: 0.728595\n",
      " 29240/50000: episode: 700, duration: 0.066s, episode steps: 22, steps per second: 333, episode reward: 0.960, mean reward: 0.044 [-0.003, 1.000], mean action: 0.636 [0.000, 2.000], mean observation: 0.056 [-0.100, 0.257], loss: 0.000028, mean_absolute_error: 0.484188, mean_q: 0.728693\n",
      " 29271/50000: episode: 701, duration: 0.103s, episode steps: 31, steps per second: 301, episode reward: 0.920, mean reward: 0.030 [-0.004, 1.000], mean action: 0.774 [0.000, 2.000], mean observation: 0.081 [-0.170, 0.403], loss: 0.000020, mean_absolute_error: 0.488258, mean_q: 0.733951\n",
      " 29298/50000: episode: 702, duration: 0.083s, episode steps: 27, steps per second: 324, episode reward: 0.947, mean reward: 0.035 [-0.003, 1.000], mean action: 1.074 [0.000, 2.000], mean observation: -0.063 [-0.297, 0.140], loss: 0.000047, mean_absolute_error: 0.488961, mean_q: 0.735290\n",
      " 29335/50000: episode: 703, duration: 0.149s, episode steps: 37, steps per second: 248, episode reward: 0.885, mean reward: 0.024 [-0.005, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.090 [-0.200, 0.523], loss: 0.000080, mean_absolute_error: 0.484871, mean_q: 0.729339\n",
      " 29359/50000: episode: 704, duration: 0.082s, episode steps: 24, steps per second: 294, episode reward: 0.953, mean reward: 0.040 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.060 [-0.110, 0.281], loss: 0.000025, mean_absolute_error: 0.493098, mean_q: 0.744581\n",
      " 29392/50000: episode: 705, duration: 0.104s, episode steps: 33, steps per second: 317, episode reward: 0.923, mean reward: 0.028 [-0.004, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.060 [-0.180, 0.391], loss: 0.000499, mean_absolute_error: 0.483800, mean_q: 0.726500\n",
      " 29437/50000: episode: 706, duration: 0.155s, episode steps: 45, steps per second: 291, episode reward: 0.840, mean reward: 0.019 [-0.006, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.122 [-0.609, 0.190], loss: 0.000039, mean_absolute_error: 0.479262, mean_q: 0.722763\n",
      " 29491/50000: episode: 707, duration: 0.178s, episode steps: 54, steps per second: 303, episode reward: 0.758, mean reward: 0.014 [-0.008, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.157 [-0.210, 0.803], loss: 0.000031, mean_absolute_error: 0.486412, mean_q: 0.731336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29549/50000: episode: 708, duration: 0.175s, episode steps: 58, steps per second: 332, episode reward: 0.684, mean reward: 0.012 [-0.010, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.199 [-0.240, 0.962], loss: 0.000023, mean_absolute_error: 0.486415, mean_q: 0.732149\n",
      " 29550/50000: episode: 709, duration: 0.006s, episode steps: 1, steps per second: 167, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.021 [-0.031, -0.010], loss: 0.000027, mean_absolute_error: 0.473963, mean_q: 0.692849\n",
      " 29606/50000: episode: 710, duration: 0.169s, episode steps: 56, steps per second: 331, episode reward: 0.769, mean reward: 0.014 [-0.007, 1.000], mean action: 1.089 [0.000, 2.000], mean observation: -0.152 [-0.712, 0.180], loss: 0.000027, mean_absolute_error: 0.485207, mean_q: 0.727729\n",
      " 29660/50000: episode: 711, duration: 0.172s, episode steps: 54, steps per second: 314, episode reward: 0.726, mean reward: 0.013 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.181 [-0.240, 0.882], loss: 0.000021, mean_absolute_error: 0.487315, mean_q: 0.731762\n",
      " 29708/50000: episode: 712, duration: 0.163s, episode steps: 48, steps per second: 295, episode reward: 0.815, mean reward: 0.017 [-0.007, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.129 [-0.180, 0.675], loss: 0.000051, mean_absolute_error: 0.484337, mean_q: 0.728351\n",
      " 29718/50000: episode: 713, duration: 0.038s, episode steps: 10, steps per second: 261, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 0.300 [0.000, 2.000], mean observation: 0.042 [-0.070, 0.137], loss: 0.000270, mean_absolute_error: 0.491677, mean_q: 0.739200\n",
      " 29759/50000: episode: 714, duration: 0.146s, episode steps: 41, steps per second: 280, episode reward: 0.859, mean reward: 0.021 [-0.005, 1.000], mean action: 1.171 [0.000, 2.000], mean observation: -0.118 [-0.545, 0.170], loss: 0.000028, mean_absolute_error: 0.484944, mean_q: 0.729281\n",
      " 29779/50000: episode: 715, duration: 0.067s, episode steps: 20, steps per second: 300, episode reward: 0.970, mean reward: 0.048 [-0.002, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.049 [-0.210, 0.100], loss: 0.000026, mean_absolute_error: 0.487970, mean_q: 0.734530\n",
      " 29780/50000: episode: 716, duration: 0.006s, episode steps: 1, steps per second: 159, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [-0.010, 0.040], loss: 0.000025, mean_absolute_error: 0.488013, mean_q: 0.740032\n",
      " 29837/50000: episode: 717, duration: 0.204s, episode steps: 57, steps per second: 279, episode reward: 0.743, mean reward: 0.013 [-0.008, 1.000], mean action: 1.070 [0.000, 2.000], mean observation: -0.162 [-0.830, 0.220], loss: 0.000027, mean_absolute_error: 0.483829, mean_q: 0.726888\n",
      " 29867/50000: episode: 718, duration: 0.103s, episode steps: 30, steps per second: 292, episode reward: 0.922, mean reward: 0.031 [-0.004, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.082 [-0.160, 0.392], loss: 0.000021, mean_absolute_error: 0.481958, mean_q: 0.725156\n",
      " 29892/50000: episode: 719, duration: 0.080s, episode steps: 25, steps per second: 313, episode reward: 0.946, mean reward: 0.038 [-0.003, 1.000], mean action: 0.640 [0.000, 2.000], mean observation: 0.064 [-0.140, 0.316], loss: 0.000024, mean_absolute_error: 0.473818, mean_q: 0.711935\n",
      " 29945/50000: episode: 720, duration: 0.170s, episode steps: 53, steps per second: 312, episode reward: 0.763, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.162 [-0.200, 0.756], loss: 0.000039, mean_absolute_error: 0.480892, mean_q: 0.723073\n",
      " 29961/50000: episode: 721, duration: 0.051s, episode steps: 16, steps per second: 314, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.045 [-0.090, 0.195], loss: 0.000016, mean_absolute_error: 0.480069, mean_q: 0.723392\n",
      " 30012/50000: episode: 722, duration: 0.176s, episode steps: 51, steps per second: 289, episode reward: 0.752, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.176 [-0.220, 0.786], loss: 0.000029, mean_absolute_error: 0.486367, mean_q: 0.731721\n",
      " 30050/50000: episode: 723, duration: 0.152s, episode steps: 38, steps per second: 250, episode reward: 0.872, mean reward: 0.023 [-0.005, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.113 [-0.190, 0.522], loss: 0.000178, mean_absolute_error: 0.488213, mean_q: 0.734450\n",
      " 30100/50000: episode: 724, duration: 0.206s, episode steps: 50, steps per second: 243, episode reward: 0.789, mean reward: 0.016 [-0.007, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.152 [-0.693, 0.190], loss: 0.000042, mean_absolute_error: 0.486601, mean_q: 0.731616\n",
      " 30123/50000: episode: 725, duration: 0.081s, episode steps: 23, steps per second: 286, episode reward: 0.957, mean reward: 0.042 [-0.003, 1.000], mean action: 0.696 [0.000, 2.000], mean observation: 0.061 [-0.100, 0.263], loss: 0.000033, mean_absolute_error: 0.483451, mean_q: 0.726340\n",
      " 30165/50000: episode: 726, duration: 0.173s, episode steps: 42, steps per second: 243, episode reward: 0.885, mean reward: 0.021 [-0.005, 1.000], mean action: 1.095 [0.000, 2.000], mean observation: -0.094 [-0.467, 0.170], loss: 0.000076, mean_absolute_error: 0.486016, mean_q: 0.731164\n",
      " 30189/50000: episode: 727, duration: 0.135s, episode steps: 24, steps per second: 177, episode reward: 0.959, mean reward: 0.040 [-0.002, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.057 [-0.235, 0.110], loss: 0.000033, mean_absolute_error: 0.488710, mean_q: 0.737616\n",
      " 30231/50000: episode: 728, duration: 0.149s, episode steps: 42, steps per second: 282, episode reward: 0.881, mean reward: 0.021 [-0.005, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.088 [-0.180, 0.489], loss: 0.000041, mean_absolute_error: 0.479440, mean_q: 0.720580\n",
      " 30286/50000: episode: 729, duration: 0.180s, episode steps: 55, steps per second: 305, episode reward: 0.750, mean reward: 0.014 [-0.008, 1.000], mean action: 1.109 [0.000, 2.000], mean observation: -0.167 [-0.764, 0.190], loss: 0.000032, mean_absolute_error: 0.492284, mean_q: 0.740862\n",
      " 30335/50000: episode: 730, duration: 0.170s, episode steps: 49, steps per second: 289, episode reward: 0.780, mean reward: 0.016 [-0.007, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.160 [-0.210, 0.742], loss: 0.000030, mean_absolute_error: 0.483892, mean_q: 0.728418\n",
      " 30372/50000: episode: 731, duration: 0.121s, episode steps: 37, steps per second: 305, episode reward: 0.904, mean reward: 0.024 [-0.004, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.081 [-0.150, 0.423], loss: 0.000299, mean_absolute_error: 0.487152, mean_q: 0.733002\n",
      " 30424/50000: episode: 732, duration: 0.250s, episode steps: 52, steps per second: 208, episode reward: 0.758, mean reward: 0.015 [-0.008, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.168 [-0.775, 0.230], loss: 0.000029, mean_absolute_error: 0.488234, mean_q: 0.734775\n",
      " 30464/50000: episode: 733, duration: 0.194s, episode steps: 40, steps per second: 206, episode reward: 0.859, mean reward: 0.021 [-0.006, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.116 [-0.220, 0.584], loss: 0.000028, mean_absolute_error: 0.491143, mean_q: 0.737961\n",
      " 30480/50000: episode: 734, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 0.976, mean reward: 0.061 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.044 [-0.100, 0.199], loss: 0.000020, mean_absolute_error: 0.489408, mean_q: 0.737303\n",
      " 30541/50000: episode: 735, duration: 0.292s, episode steps: 61, steps per second: 209, episode reward: 0.666, mean reward: 0.011 [-0.009, 1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.207 [-0.925, 0.210], loss: 0.000075, mean_absolute_error: 0.485866, mean_q: 0.729423\n",
      " 30565/50000: episode: 736, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 0.956, mean reward: 0.040 [-0.003, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.058 [-0.100, 0.263], loss: 0.000043, mean_absolute_error: 0.488859, mean_q: 0.734684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30622/50000: episode: 737, duration: 0.248s, episode steps: 57, steps per second: 230, episode reward: 0.678, mean reward: 0.012 [-0.010, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.204 [-0.260, 0.988], loss: 0.000033, mean_absolute_error: 0.491627, mean_q: 0.738183\n",
      " 30656/50000: episode: 738, duration: 0.146s, episode steps: 34, steps per second: 233, episode reward: 0.932, mean reward: 0.027 [-0.003, 1.000], mean action: 1.206 [0.000, 2.000], mean observation: -0.069 [-0.306, 0.100], loss: 0.000039, mean_absolute_error: 0.488363, mean_q: 0.729670\n",
      " 30716/50000: episode: 739, duration: 0.255s, episode steps: 60, steps per second: 236, episode reward: 0.688, mean reward: 0.011 [-0.010, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.184 [-0.250, 0.967], loss: 0.000028, mean_absolute_error: 0.489738, mean_q: 0.735120\n",
      " 30771/50000: episode: 740, duration: 0.253s, episode steps: 55, steps per second: 217, episode reward: 0.712, mean reward: 0.013 [-0.009, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.187 [-0.250, 0.921], loss: 0.000031, mean_absolute_error: 0.486691, mean_q: 0.731651\n",
      " 30822/50000: episode: 741, duration: 0.218s, episode steps: 51, steps per second: 234, episode reward: 0.772, mean reward: 0.015 [-0.007, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.158 [-0.220, 0.739], loss: 0.000028, mean_absolute_error: 0.491149, mean_q: 0.736818\n",
      " 30860/50000: episode: 742, duration: 0.165s, episode steps: 38, steps per second: 230, episode reward: 0.907, mean reward: 0.024 [-0.004, 1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.082 [-0.409, 0.140], loss: 0.000025, mean_absolute_error: 0.487483, mean_q: 0.733853\n",
      " 30872/50000: episode: 743, duration: 0.057s, episode steps: 12, steps per second: 212, episode reward: 0.985, mean reward: 0.082 [-0.002, 1.000], mean action: 0.333 [0.000, 2.000], mean observation: 0.039 [-0.090, 0.165], loss: 0.000031, mean_absolute_error: 0.485611, mean_q: 0.734613\n",
      " 30922/50000: episode: 744, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 0.771, mean reward: 0.015 [-0.008, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.162 [-0.780, 0.220], loss: 0.000028, mean_absolute_error: 0.477957, mean_q: 0.719340\n",
      " 30935/50000: episode: 745, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 0.982, mean reward: 0.076 [-0.002, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.042 [-0.090, 0.178], loss: 0.000021, mean_absolute_error: 0.483606, mean_q: 0.729676\n",
      " 30961/50000: episode: 746, duration: 0.122s, episode steps: 26, steps per second: 213, episode reward: 0.947, mean reward: 0.036 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.057 [-0.130, 0.313], loss: 0.000158, mean_absolute_error: 0.485062, mean_q: 0.725806\n",
      " 31011/50000: episode: 747, duration: 0.268s, episode steps: 50, steps per second: 187, episode reward: 0.760, mean reward: 0.015 [-0.008, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.169 [-0.250, 0.815], loss: 0.000034, mean_absolute_error: 0.484025, mean_q: 0.728664\n",
      " 31031/50000: episode: 748, duration: 0.089s, episode steps: 20, steps per second: 224, episode reward: 0.967, mean reward: 0.048 [-0.002, 1.000], mean action: 0.600 [0.000, 2.000], mean observation: 0.051 [-0.090, 0.229], loss: 0.000031, mean_absolute_error: 0.488414, mean_q: 0.730896\n",
      " 31089/50000: episode: 749, duration: 0.265s, episode steps: 58, steps per second: 219, episode reward: 0.780, mean reward: 0.013 [-0.006, 1.000], mean action: 1.103 [0.000, 2.000], mean observation: -0.145 [-0.625, 0.160], loss: 0.000299, mean_absolute_error: 0.490463, mean_q: 0.738324\n",
      " 31120/50000: episode: 750, duration: 0.152s, episode steps: 31, steps per second: 205, episode reward: 0.940, mean reward: 0.030 [-0.003, 1.000], mean action: 1.097 [0.000, 2.000], mean observation: -0.065 [-0.302, 0.110], loss: 0.000026, mean_absolute_error: 0.470882, mean_q: 0.704866\n",
      " 31176/50000: episode: 751, duration: 0.333s, episode steps: 56, steps per second: 168, episode reward: 0.718, mean reward: 0.013 [-0.009, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.182 [-0.230, 0.859], loss: 0.000031, mean_absolute_error: 0.483328, mean_q: 0.727395\n",
      " 31236/50000: episode: 752, duration: 0.314s, episode steps: 60, steps per second: 191, episode reward: 0.742, mean reward: 0.012 [-0.008, 1.000], mean action: 1.067 [0.000, 2.000], mean observation: -0.157 [-0.809, 0.210], loss: 0.000023, mean_absolute_error: 0.485805, mean_q: 0.730676\n",
      " 31290/50000: episode: 753, duration: 0.237s, episode steps: 54, steps per second: 228, episode reward: 0.709, mean reward: 0.013 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.195 [-0.240, 0.897], loss: 0.000301, mean_absolute_error: 0.482778, mean_q: 0.721747\n",
      " 31357/50000: episode: 754, duration: 0.287s, episode steps: 67, steps per second: 234, episode reward: 0.734, mean reward: 0.011 [-0.008, 1.000], mean action: 1.030 [0.000, 2.000], mean observation: -0.146 [-0.808, 0.230], loss: 0.000241, mean_absolute_error: 0.487095, mean_q: 0.730072\n",
      " 31399/50000: episode: 755, duration: 0.248s, episode steps: 42, steps per second: 169, episode reward: 0.887, mean reward: 0.021 [-0.004, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.087 [-0.160, 0.448], loss: 0.000037, mean_absolute_error: 0.481575, mean_q: 0.725020\n",
      " 31446/50000: episode: 756, duration: 0.207s, episode steps: 47, steps per second: 227, episode reward: 0.841, mean reward: 0.018 [-0.006, 1.000], mean action: 1.064 [0.000, 2.000], mean observation: -0.118 [-0.585, 0.190], loss: 0.000042, mean_absolute_error: 0.483368, mean_q: 0.724369\n",
      " 31504/50000: episode: 757, duration: 0.258s, episode steps: 58, steps per second: 225, episode reward: 0.720, mean reward: 0.012 [-0.009, 1.000], mean action: 1.052 [0.000, 2.000], mean observation: -0.176 [-0.869, 0.220], loss: 0.000030, mean_absolute_error: 0.487421, mean_q: 0.734458\n",
      " 31562/50000: episode: 758, duration: 0.277s, episode steps: 58, steps per second: 209, episode reward: 0.717, mean reward: 0.012 [-0.009, 1.000], mean action: 1.069 [0.000, 2.000], mean observation: -0.179 [-0.865, 0.230], loss: 0.000031, mean_absolute_error: 0.487272, mean_q: 0.734590\n",
      " 31608/50000: episode: 759, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 0.793, mean reward: 0.017 [-0.008, 1.000], mean action: 1.174 [0.000, 2.000], mean observation: -0.155 [-0.752, 0.220], loss: 0.000033, mean_absolute_error: 0.487515, mean_q: 0.731837\n",
      " 31664/50000: episode: 760, duration: 0.285s, episode steps: 56, steps per second: 196, episode reward: 0.682, mean reward: 0.012 [-0.010, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.206 [-0.240, 0.980], loss: 0.000037, mean_absolute_error: 0.492789, mean_q: 0.743174\n",
      " 31668/50000: episode: 761, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 0.997, mean reward: 0.249 [-0.001, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.041 [-0.106, 0.030], loss: 0.000033, mean_absolute_error: 0.511715, mean_q: 0.772333\n",
      " 31722/50000: episode: 762, duration: 0.283s, episode steps: 54, steps per second: 191, episode reward: 0.713, mean reward: 0.013 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.194 [-0.240, 0.878], loss: 0.000039, mean_absolute_error: 0.486332, mean_q: 0.730373\n",
      " 31760/50000: episode: 763, duration: 0.252s, episode steps: 38, steps per second: 151, episode reward: 0.907, mean reward: 0.024 [-0.004, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.083 [-0.407, 0.140], loss: 0.000033, mean_absolute_error: 0.488116, mean_q: 0.733627\n",
      " 31804/50000: episode: 764, duration: 0.185s, episode steps: 44, steps per second: 238, episode reward: 0.835, mean reward: 0.019 [-0.006, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.123 [-0.220, 0.642], loss: 0.000040, mean_absolute_error: 0.484298, mean_q: 0.726812\n",
      " 31852/50000: episode: 765, duration: 0.307s, episode steps: 48, steps per second: 157, episode reward: 0.775, mean reward: 0.016 [-0.008, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.164 [-0.240, 0.780], loss: 0.000042, mean_absolute_error: 0.487485, mean_q: 0.732018\n",
      " 31872/50000: episode: 766, duration: 0.084s, episode steps: 20, steps per second: 239, episode reward: 0.967, mean reward: 0.048 [-0.002, 1.000], mean action: 1.350 [0.000, 2.000], mean observation: -0.053 [-0.226, 0.110], loss: 0.000057, mean_absolute_error: 0.486095, mean_q: 0.731103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31925/50000: episode: 767, duration: 0.230s, episode steps: 53, steps per second: 230, episode reward: 0.774, mean reward: 0.015 [-0.007, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.154 [-0.240, 0.727], loss: 0.000094, mean_absolute_error: 0.482886, mean_q: 0.725638\n",
      " 31926/50000: episode: 768, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [-0.010, 0.076], loss: 0.000059, mean_absolute_error: 0.444779, mean_q: 0.673022\n",
      " 31975/50000: episode: 769, duration: 0.259s, episode steps: 49, steps per second: 189, episode reward: 0.822, mean reward: 0.017 [-0.006, 1.000], mean action: 1.082 [0.000, 2.000], mean observation: -0.128 [-0.632, 0.180], loss: 0.000026, mean_absolute_error: 0.482219, mean_q: 0.726092\n",
      " 32024/50000: episode: 770, duration: 0.241s, episode steps: 49, steps per second: 204, episode reward: 0.816, mean reward: 0.017 [-0.007, 1.000], mean action: 1.122 [0.000, 2.000], mean observation: -0.131 [-0.653, 0.180], loss: 0.000083, mean_absolute_error: 0.485708, mean_q: 0.726886\n",
      " 32079/50000: episode: 771, duration: 0.255s, episode steps: 55, steps per second: 216, episode reward: 0.762, mean reward: 0.014 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.156 [-0.774, 0.180], loss: 0.000055, mean_absolute_error: 0.485619, mean_q: 0.729694\n",
      " 32106/50000: episode: 772, duration: 0.106s, episode steps: 27, steps per second: 255, episode reward: 0.951, mean reward: 0.035 [-0.003, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.062 [-0.265, 0.100], loss: 0.000030, mean_absolute_error: 0.481278, mean_q: 0.723777\n",
      " 32141/50000: episode: 773, duration: 0.217s, episode steps: 35, steps per second: 162, episode reward: 0.907, mean reward: 0.026 [-0.004, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.088 [-0.140, 0.419], loss: 0.000044, mean_absolute_error: 0.486231, mean_q: 0.728909\n",
      " 32192/50000: episode: 774, duration: 0.243s, episode steps: 51, steps per second: 209, episode reward: 0.788, mean reward: 0.015 [-0.007, 1.000], mean action: 0.863 [0.000, 2.000], mean observation: 0.146 [-0.240, 0.728], loss: 0.000033, mean_absolute_error: 0.485330, mean_q: 0.730061\n",
      " 32224/50000: episode: 775, duration: 0.142s, episode steps: 32, steps per second: 226, episode reward: 0.944, mean reward: 0.030 [-0.003, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.062 [-0.090, 0.258], loss: 0.000073, mean_absolute_error: 0.491974, mean_q: 0.737383\n",
      " 32275/50000: episode: 776, duration: 0.208s, episode steps: 51, steps per second: 246, episode reward: 0.787, mean reward: 0.015 [-0.007, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.147 [-0.737, 0.210], loss: 0.000033, mean_absolute_error: 0.484946, mean_q: 0.728826\n",
      " 32301/50000: episode: 777, duration: 0.123s, episode steps: 26, steps per second: 212, episode reward: 0.951, mean reward: 0.037 [-0.003, 1.000], mean action: 0.692 [0.000, 2.000], mean observation: 0.063 [-0.100, 0.259], loss: 0.000032, mean_absolute_error: 0.484881, mean_q: 0.729026\n",
      " 32344/50000: episode: 778, duration: 0.195s, episode steps: 43, steps per second: 221, episode reward: 0.847, mean reward: 0.020 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.115 [-0.230, 0.618], loss: 0.000066, mean_absolute_error: 0.482422, mean_q: 0.720800\n",
      " 32399/50000: episode: 779, duration: 0.231s, episode steps: 55, steps per second: 238, episode reward: 0.820, mean reward: 0.015 [-0.006, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.110 [-0.190, 0.586], loss: 0.000096, mean_absolute_error: 0.485917, mean_q: 0.729055\n",
      " 32455/50000: episode: 780, duration: 0.308s, episode steps: 56, steps per second: 182, episode reward: 0.802, mean reward: 0.014 [-0.006, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.126 [-0.627, 0.160], loss: 0.000277, mean_absolute_error: 0.487916, mean_q: 0.733275\n",
      " 32495/50000: episode: 781, duration: 0.182s, episode steps: 40, steps per second: 220, episode reward: 0.866, mean reward: 0.022 [-0.005, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.113 [-0.546, 0.170], loss: 0.000038, mean_absolute_error: 0.486465, mean_q: 0.730440\n",
      " 32540/50000: episode: 782, duration: 0.196s, episode steps: 45, steps per second: 230, episode reward: 0.829, mean reward: 0.018 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.131 [-0.636, 0.180], loss: 0.000339, mean_absolute_error: 0.483082, mean_q: 0.724176\n",
      " 32571/50000: episode: 783, duration: 0.143s, episode steps: 31, steps per second: 218, episode reward: 0.936, mean reward: 0.030 [-0.003, 1.000], mean action: 0.774 [0.000, 2.000], mean observation: 0.072 [-0.110, 0.304], loss: 0.000047, mean_absolute_error: 0.482696, mean_q: 0.725512\n",
      " 32617/50000: episode: 784, duration: 0.255s, episode steps: 46, steps per second: 181, episode reward: 0.832, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.120 [-0.623, 0.180], loss: 0.000066, mean_absolute_error: 0.485720, mean_q: 0.731127\n",
      " 32631/50000: episode: 785, duration: 0.103s, episode steps: 14, steps per second: 136, episode reward: 0.982, mean reward: 0.070 [-0.002, 1.000], mean action: 1.643 [0.000, 2.000], mean observation: -0.041 [-0.170, 0.090], loss: 0.000030, mean_absolute_error: 0.478103, mean_q: 0.717214\n",
      " 32653/50000: episode: 786, duration: 0.089s, episode steps: 22, steps per second: 247, episode reward: 0.958, mean reward: 0.044 [-0.003, 1.000], mean action: 1.318 [0.000, 2.000], mean observation: -0.059 [-0.266, 0.120], loss: 0.000046, mean_absolute_error: 0.494047, mean_q: 0.741007\n",
      " 32693/50000: episode: 787, duration: 0.194s, episode steps: 40, steps per second: 206, episode reward: 0.917, mean reward: 0.023 [-0.004, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.059 [-0.374, 0.120], loss: 0.000042, mean_absolute_error: 0.488779, mean_q: 0.733626\n",
      " 32735/50000: episode: 788, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 0.848, mean reward: 0.020 [-0.006, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.121 [-0.210, 0.607], loss: 0.000049, mean_absolute_error: 0.482210, mean_q: 0.722121\n",
      " 32758/50000: episode: 789, duration: 0.088s, episode steps: 23, steps per second: 263, episode reward: 0.962, mean reward: 0.042 [-0.002, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.043 [-0.120, 0.248], loss: 0.000036, mean_absolute_error: 0.481188, mean_q: 0.724494\n",
      " 32793/50000: episode: 790, duration: 0.161s, episode steps: 35, steps per second: 218, episode reward: 0.927, mean reward: 0.026 [-0.003, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.071 [-0.110, 0.334], loss: 0.000046, mean_absolute_error: 0.479914, mean_q: 0.718923\n",
      " 32805/50000: episode: 791, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 0.986, mean reward: 0.082 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.040 [-0.150, 0.060], loss: 0.000038, mean_absolute_error: 0.479998, mean_q: 0.713392\n",
      " 32856/50000: episode: 792, duration: 0.248s, episode steps: 51, steps per second: 206, episode reward: 0.798, mean reward: 0.016 [-0.007, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.141 [-0.685, 0.180], loss: 0.000058, mean_absolute_error: 0.484971, mean_q: 0.727119\n",
      " 32909/50000: episode: 793, duration: 0.263s, episode steps: 53, steps per second: 201, episode reward: 0.769, mean reward: 0.015 [-0.007, 1.000], mean action: 1.151 [0.000, 2.000], mean observation: -0.157 [-0.749, 0.190], loss: 0.000063, mean_absolute_error: 0.488186, mean_q: 0.732520\n",
      " 32954/50000: episode: 794, duration: 0.293s, episode steps: 45, steps per second: 154, episode reward: 0.854, mean reward: 0.019 [-0.006, 1.000], mean action: 0.911 [0.000, 2.000], mean observation: 0.110 [-0.190, 0.575], loss: 0.000045, mean_absolute_error: 0.496484, mean_q: 0.745100\n",
      " 32999/50000: episode: 795, duration: 0.219s, episode steps: 45, steps per second: 206, episode reward: 0.850, mean reward: 0.019 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.111 [-0.190, 0.579], loss: 0.000053, mean_absolute_error: 0.489754, mean_q: 0.733301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33062/50000: episode: 796, duration: 0.320s, episode steps: 63, steps per second: 197, episode reward: 0.922, mean reward: 0.015 [-0.002, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.041 [-0.175, 0.206], loss: 0.000550, mean_absolute_error: 0.488221, mean_q: 0.733556\n",
      " 33072/50000: episode: 797, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.039 [-0.060, 0.142], loss: 0.000372, mean_absolute_error: 0.488236, mean_q: 0.733197\n",
      " 33073/50000: episode: 798, duration: 0.008s, episode steps: 1, steps per second: 119, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.018 [0.000, 0.036], loss: 0.000124, mean_absolute_error: 0.482873, mean_q: 0.728505\n",
      " 33074/50000: episode: 799, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.005 [0.000, 0.009], loss: 0.000108, mean_absolute_error: 0.451487, mean_q: 0.680095\n",
      " 33075/50000: episode: 800, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.007 [-0.014, 0.000], loss: 0.000061, mean_absolute_error: 0.496594, mean_q: 0.750931\n",
      " 33130/50000: episode: 801, duration: 0.283s, episode steps: 55, steps per second: 194, episode reward: 0.709, mean reward: 0.013 [-0.009, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.190 [-0.914, 0.230], loss: 0.000080, mean_absolute_error: 0.486365, mean_q: 0.731207\n",
      " 33170/50000: episode: 802, duration: 0.190s, episode steps: 40, steps per second: 210, episode reward: 0.914, mean reward: 0.023 [-0.004, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.060 [-0.120, 0.389], loss: 0.000375, mean_absolute_error: 0.487730, mean_q: 0.733196\n",
      " 33199/50000: episode: 803, duration: 0.180s, episode steps: 29, steps per second: 161, episode reward: 0.926, mean reward: 0.032 [-0.004, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.076 [-0.160, 0.390], loss: 0.000072, mean_absolute_error: 0.487804, mean_q: 0.735138\n",
      " 33264/50000: episode: 804, duration: 0.389s, episode steps: 65, steps per second: 167, episode reward: 0.660, mean reward: 0.010 [-0.009, 1.000], mean action: 1.108 [0.000, 2.000], mean observation: -0.200 [-0.908, 0.230], loss: 0.000240, mean_absolute_error: 0.486072, mean_q: 0.729367\n",
      " 33325/50000: episode: 805, duration: 0.338s, episode steps: 61, steps per second: 181, episode reward: 0.651, mean reward: 0.011 [-0.010, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.209 [-0.989, 0.210], loss: 0.000256, mean_absolute_error: 0.484862, mean_q: 0.729332\n",
      " 33352/50000: episode: 806, duration: 0.112s, episode steps: 27, steps per second: 241, episode reward: 0.955, mean reward: 0.035 [-0.002, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.057 [-0.249, 0.110], loss: 0.000089, mean_absolute_error: 0.493408, mean_q: 0.742176\n",
      " 33353/50000: episode: 807, duration: 0.008s, episode steps: 1, steps per second: 128, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [-0.010, 0.049], loss: 0.000039, mean_absolute_error: 0.508318, mean_q: 0.742352\n",
      " 33412/50000: episode: 808, duration: 0.337s, episode steps: 59, steps per second: 175, episode reward: 0.683, mean reward: 0.012 [-0.010, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.192 [-0.270, 0.965], loss: 0.000100, mean_absolute_error: 0.486932, mean_q: 0.732990\n",
      " 33454/50000: episode: 809, duration: 0.203s, episode steps: 42, steps per second: 207, episode reward: 0.877, mean reward: 0.021 [-0.005, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.099 [-0.160, 0.500], loss: 0.000041, mean_absolute_error: 0.479388, mean_q: 0.722979\n",
      " 33482/50000: episode: 810, duration: 0.170s, episode steps: 28, steps per second: 165, episode reward: 0.943, mean reward: 0.034 [-0.003, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.066 [-0.110, 0.307], loss: 0.000064, mean_absolute_error: 0.480698, mean_q: 0.722026\n",
      " 33538/50000: episode: 811, duration: 0.230s, episode steps: 56, steps per second: 244, episode reward: 0.676, mean reward: 0.012 [-0.010, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.209 [-0.270, 0.979], loss: 0.000312, mean_absolute_error: 0.487866, mean_q: 0.730320\n",
      " 33598/50000: episode: 812, duration: 0.267s, episode steps: 60, steps per second: 225, episode reward: 0.768, mean reward: 0.013 [-0.007, 1.000], mean action: 0.917 [0.000, 2.000], mean observation: 0.140 [-0.200, 0.739], loss: 0.000062, mean_absolute_error: 0.487660, mean_q: 0.732323\n",
      " 33630/50000: episode: 813, duration: 0.146s, episode steps: 32, steps per second: 219, episode reward: 0.950, mean reward: 0.030 [-0.002, 1.000], mean action: 1.156 [0.000, 2.000], mean observation: -0.072 [-0.191, 0.080], loss: 0.000062, mean_absolute_error: 0.487145, mean_q: 0.730912\n",
      " 33657/50000: episode: 814, duration: 0.133s, episode steps: 27, steps per second: 202, episode reward: 0.954, mean reward: 0.035 [-0.003, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.059 [-0.090, 0.252], loss: 0.000058, mean_absolute_error: 0.475886, mean_q: 0.712963\n",
      " 33694/50000: episode: 815, duration: 0.221s, episode steps: 37, steps per second: 167, episode reward: 0.935, mean reward: 0.025 [-0.002, 1.000], mean action: 0.892 [0.000, 2.000], mean observation: 0.069 [-0.080, 0.239], loss: 0.000058, mean_absolute_error: 0.481594, mean_q: 0.723103\n",
      " 33722/50000: episode: 816, duration: 0.118s, episode steps: 28, steps per second: 237, episode reward: 0.974, mean reward: 0.035 [-0.002, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: 0.008 [-0.186, 0.140], loss: 0.000042, mean_absolute_error: 0.487503, mean_q: 0.729198\n",
      " 33723/50000: episode: 817, duration: 0.008s, episode steps: 1, steps per second: 125, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.009 [-0.010, -0.007], loss: 0.000038, mean_absolute_error: 0.452388, mean_q: 0.685198\n",
      " 33745/50000: episode: 818, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 0.969, mean reward: 0.044 [-0.002, 1.000], mean action: 1.409 [0.000, 2.000], mean observation: -0.031 [-0.219, 0.120], loss: 0.000048, mean_absolute_error: 0.487511, mean_q: 0.733148\n",
      " 33779/50000: episode: 819, duration: 0.152s, episode steps: 34, steps per second: 224, episode reward: 0.914, mean reward: 0.027 [-0.004, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.082 [-0.160, 0.403], loss: 0.000439, mean_absolute_error: 0.484945, mean_q: 0.726681\n",
      " 33836/50000: episode: 820, duration: 0.269s, episode steps: 57, steps per second: 212, episode reward: 0.700, mean reward: 0.012 [-0.010, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.177 [-0.962, 0.280], loss: 0.000055, mean_absolute_error: 0.486312, mean_q: 0.727414\n",
      " 33888/50000: episode: 821, duration: 0.234s, episode steps: 52, steps per second: 223, episode reward: 0.827, mean reward: 0.016 [-0.006, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.093 [-0.628, 0.220], loss: 0.000038, mean_absolute_error: 0.485738, mean_q: 0.725990\n",
      " 33937/50000: episode: 822, duration: 0.218s, episode steps: 49, steps per second: 225, episode reward: 0.798, mean reward: 0.016 [-0.007, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.144 [-0.230, 0.706], loss: 0.000052, mean_absolute_error: 0.488606, mean_q: 0.732744\n",
      " 34082/50000: episode: 823, duration: 0.627s, episode steps: 145, steps per second: 231, episode reward: 0.560, mean reward: 0.004 [-0.010, 1.000], mean action: 1.062 [0.000, 2.000], mean observation: 0.029 [-0.243, 0.973], loss: 0.000115, mean_absolute_error: 0.484518, mean_q: 0.727462\n",
      " 34100/50000: episode: 824, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 0.976, mean reward: 0.054 [-0.002, 1.000], mean action: 0.722 [0.000, 2.000], mean observation: 0.047 [-0.060, 0.177], loss: 0.000042, mean_absolute_error: 0.481468, mean_q: 0.720892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34159/50000: episode: 825, duration: 0.338s, episode steps: 59, steps per second: 175, episode reward: 0.651, mean reward: 0.011 [-0.010, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.221 [-0.990, 0.250], loss: 0.000059, mean_absolute_error: 0.487358, mean_q: 0.729382\n",
      " 34199/50000: episode: 826, duration: 0.237s, episode steps: 40, steps per second: 169, episode reward: 0.864, mean reward: 0.022 [-0.006, 1.000], mean action: 0.825 [0.000, 2.000], mean observation: 0.113 [-0.210, 0.568], loss: 0.000061, mean_absolute_error: 0.482182, mean_q: 0.724486\n",
      " 34232/50000: episode: 827, duration: 0.231s, episode steps: 33, steps per second: 143, episode reward: 0.924, mean reward: 0.028 [-0.003, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.079 [-0.110, 0.338], loss: 0.000453, mean_absolute_error: 0.480170, mean_q: 0.719079\n",
      " 34285/50000: episode: 828, duration: 0.240s, episode steps: 53, steps per second: 221, episode reward: 0.715, mean reward: 0.013 [-0.009, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.192 [-0.250, 0.921], loss: 0.000054, mean_absolute_error: 0.475329, mean_q: 0.715048\n",
      " 34334/50000: episode: 829, duration: 0.235s, episode steps: 49, steps per second: 209, episode reward: 0.796, mean reward: 0.016 [-0.008, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.131 [-0.753, 0.250], loss: 0.000052, mean_absolute_error: 0.482215, mean_q: 0.724902\n",
      " 34391/50000: episode: 830, duration: 0.254s, episode steps: 57, steps per second: 225, episode reward: 0.736, mean reward: 0.013 [-0.009, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.153 [-0.863, 0.230], loss: 0.000213, mean_absolute_error: 0.480783, mean_q: 0.721749\n",
      " 34481/50000: episode: 831, duration: 0.403s, episode steps: 90, steps per second: 223, episode reward: 0.756, mean reward: 0.008 [-0.006, 1.000], mean action: 0.922 [0.000, 2.000], mean observation: -0.035 [-0.586, 0.210], loss: 0.000074, mean_absolute_error: 0.484501, mean_q: 0.727476\n",
      " 34508/50000: episode: 832, duration: 0.128s, episode steps: 27, steps per second: 211, episode reward: 0.950, mean reward: 0.035 [-0.003, 1.000], mean action: 0.741 [0.000, 2.000], mean observation: 0.063 [-0.090, 0.266], loss: 0.000061, mean_absolute_error: 0.480015, mean_q: 0.718880\n",
      " 34515/50000: episode: 833, duration: 0.029s, episode steps: 7, steps per second: 239, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.286 [0.000, 2.000], mean observation: 0.038 [-0.060, 0.124], loss: 0.000032, mean_absolute_error: 0.481831, mean_q: 0.720413\n",
      " 34575/50000: episode: 834, duration: 0.270s, episode steps: 60, steps per second: 222, episode reward: 0.896, mean reward: 0.015 [-0.004, 1.000], mean action: 0.900 [0.000, 2.000], mean observation: 0.005 [-0.392, 0.180], loss: 0.000111, mean_absolute_error: 0.486866, mean_q: 0.730112\n",
      " 34605/50000: episode: 835, duration: 0.185s, episode steps: 30, steps per second: 162, episode reward: 0.928, mean reward: 0.031 [-0.004, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.065 [-0.389, 0.190], loss: 0.000061, mean_absolute_error: 0.482921, mean_q: 0.727509\n",
      " 34626/50000: episode: 836, duration: 0.084s, episode steps: 21, steps per second: 250, episode reward: 0.967, mean reward: 0.046 [-0.002, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.053 [-0.080, 0.219], loss: 0.000055, mean_absolute_error: 0.471414, mean_q: 0.710899\n",
      " 34667/50000: episode: 837, duration: 0.176s, episode steps: 41, steps per second: 233, episode reward: 0.885, mean reward: 0.022 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.072 [-0.521, 0.210], loss: 0.000038, mean_absolute_error: 0.481723, mean_q: 0.725128\n",
      " 34687/50000: episode: 838, duration: 0.081s, episode steps: 20, steps per second: 247, episode reward: 0.970, mean reward: 0.049 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.047 [-0.197, 0.100], loss: 0.000060, mean_absolute_error: 0.474340, mean_q: 0.713242\n",
      " 34731/50000: episode: 839, duration: 0.262s, episode steps: 44, steps per second: 168, episode reward: 0.891, mean reward: 0.020 [-0.004, 1.000], mean action: 0.886 [0.000, 2.000], mean observation: 0.089 [-0.110, 0.415], loss: 0.000064, mean_absolute_error: 0.489859, mean_q: 0.737831\n",
      " 34779/50000: episode: 840, duration: 0.200s, episode steps: 48, steps per second: 240, episode reward: 0.788, mean reward: 0.016 [-0.008, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.144 [-0.773, 0.240], loss: 0.000060, mean_absolute_error: 0.480216, mean_q: 0.720494\n",
      " 34836/50000: episode: 841, duration: 0.356s, episode steps: 57, steps per second: 160, episode reward: 0.700, mean reward: 0.012 [-0.009, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.187 [-0.897, 0.280], loss: 0.000119, mean_absolute_error: 0.481562, mean_q: 0.721953\n",
      " 34887/50000: episode: 842, duration: 0.213s, episode steps: 51, steps per second: 239, episode reward: 0.808, mean reward: 0.016 [-0.007, 1.000], mean action: 0.882 [0.000, 2.000], mean observation: 0.131 [-0.210, 0.682], loss: 0.000118, mean_absolute_error: 0.482544, mean_q: 0.725855\n",
      " 34943/50000: episode: 843, duration: 0.288s, episode steps: 56, steps per second: 195, episode reward: 0.754, mean reward: 0.013 [-0.008, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.158 [-0.190, 0.797], loss: 0.000110, mean_absolute_error: 0.481570, mean_q: 0.722391\n",
      " 34988/50000: episode: 844, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 0.798, mean reward: 0.018 [-0.007, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.153 [-0.735, 0.250], loss: 0.000059, mean_absolute_error: 0.481839, mean_q: 0.723861\n",
      " 34989/50000: episode: 845, duration: 0.008s, episode steps: 1, steps per second: 125, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.010, 0.082], loss: 0.000025, mean_absolute_error: 0.465055, mean_q: 0.702893\n",
      " 35045/50000: episode: 846, duration: 0.304s, episode steps: 56, steps per second: 184, episode reward: 0.773, mean reward: 0.014 [-0.007, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.141 [-0.724, 0.200], loss: 0.000113, mean_absolute_error: 0.478724, mean_q: 0.716846\n",
      " 35046/50000: episode: 847, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [-0.010, 0.074], loss: 0.000067, mean_absolute_error: 0.506407, mean_q: 0.763859\n",
      " 35075/50000: episode: 848, duration: 0.195s, episode steps: 29, steps per second: 149, episode reward: 0.954, mean reward: 0.033 [-0.002, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.061 [-0.060, 0.217], loss: 0.000060, mean_absolute_error: 0.481517, mean_q: 0.725374\n",
      " 35124/50000: episode: 849, duration: 0.336s, episode steps: 49, steps per second: 146, episode reward: 0.783, mean reward: 0.016 [-0.007, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.154 [-0.746, 0.200], loss: 0.000074, mean_absolute_error: 0.474723, mean_q: 0.712789\n",
      " 35138/50000: episode: 850, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 0.983, mean reward: 0.070 [-0.002, 1.000], mean action: 0.643 [0.000, 2.000], mean observation: 0.043 [-0.060, 0.158], loss: 0.000072, mean_absolute_error: 0.487498, mean_q: 0.730621\n",
      " 35192/50000: episode: 851, duration: 0.259s, episode steps: 54, steps per second: 208, episode reward: 0.714, mean reward: 0.013 [-0.009, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.192 [-0.260, 0.886], loss: 0.000059, mean_absolute_error: 0.483260, mean_q: 0.727307\n",
      " 35211/50000: episode: 852, duration: 0.118s, episode steps: 19, steps per second: 160, episode reward: 0.980, mean reward: 0.052 [-0.002, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.010 [-0.169, 0.140], loss: 0.000117, mean_absolute_error: 0.493152, mean_q: 0.743338\n",
      " 35258/50000: episode: 853, duration: 0.286s, episode steps: 47, steps per second: 164, episode reward: 0.779, mean reward: 0.017 [-0.008, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.164 [-0.210, 0.773], loss: 0.000062, mean_absolute_error: 0.480978, mean_q: 0.722262\n",
      " 35289/50000: episode: 854, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 0.945, mean reward: 0.030 [-0.003, 1.000], mean action: 0.806 [0.000, 2.000], mean observation: 0.066 [-0.060, 0.251], loss: 0.000049, mean_absolute_error: 0.487009, mean_q: 0.732755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35320/50000: episode: 855, duration: 0.140s, episode steps: 31, steps per second: 222, episode reward: 0.936, mean reward: 0.030 [-0.003, 1.000], mean action: 0.742 [0.000, 2.000], mean observation: 0.074 [-0.110, 0.287], loss: 0.000084, mean_absolute_error: 0.469106, mean_q: 0.705035\n",
      " 35379/50000: episode: 856, duration: 0.337s, episode steps: 59, steps per second: 175, episode reward: 0.665, mean reward: 0.011 [-0.010, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.210 [-0.250, 0.980], loss: 0.000127, mean_absolute_error: 0.487451, mean_q: 0.735084\n",
      " 35416/50000: episode: 857, duration: 0.180s, episode steps: 37, steps per second: 206, episode reward: 0.913, mean reward: 0.025 [-0.004, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.075 [-0.391, 0.150], loss: 0.000101, mean_absolute_error: 0.485092, mean_q: 0.730590\n",
      " 35479/50000: episode: 858, duration: 0.322s, episode steps: 63, steps per second: 195, episode reward: 0.706, mean reward: 0.011 [-0.009, 1.000], mean action: 0.921 [0.000, 2.000], mean observation: 0.172 [-0.210, 0.875], loss: 0.000089, mean_absolute_error: 0.482854, mean_q: 0.724500\n",
      " 35513/50000: episode: 859, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 0.891, mean reward: 0.026 [-0.005, 1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.100 [-0.504, 0.200], loss: 0.000050, mean_absolute_error: 0.485565, mean_q: 0.729973\n",
      " 35536/50000: episode: 860, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 0.961, mean reward: 0.042 [-0.002, 1.000], mean action: 0.696 [0.000, 2.000], mean observation: 0.056 [-0.080, 0.243], loss: 0.000049, mean_absolute_error: 0.482584, mean_q: 0.724823\n",
      " 35537/50000: episode: 861, duration: 0.008s, episode steps: 1, steps per second: 131, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.031 [-0.010, 0.073], loss: 0.000075, mean_absolute_error: 0.487212, mean_q: 0.736513\n",
      " 35579/50000: episode: 862, duration: 0.207s, episode steps: 42, steps per second: 203, episode reward: 0.857, mean reward: 0.020 [-0.005, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.118 [-0.150, 0.548], loss: 0.000073, mean_absolute_error: 0.486709, mean_q: 0.730063\n",
      " 35606/50000: episode: 863, duration: 0.150s, episode steps: 27, steps per second: 181, episode reward: 0.940, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.066 [-0.339, 0.150], loss: 0.000065, mean_absolute_error: 0.479801, mean_q: 0.720030\n",
      " 35713/50000: episode: 864, duration: 0.544s, episode steps: 107, steps per second: 197, episode reward: 0.621, mean reward: 0.006 [-0.010, 1.000], mean action: 1.084 [0.000, 2.000], mean observation: 0.076 [-0.270, 0.976], loss: 0.000176, mean_absolute_error: 0.484323, mean_q: 0.726938\n",
      " 35754/50000: episode: 865, duration: 0.178s, episode steps: 41, steps per second: 230, episode reward: 0.869, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.108 [-0.527, 0.170], loss: 0.000088, mean_absolute_error: 0.483461, mean_q: 0.725266\n",
      " 35813/50000: episode: 866, duration: 0.249s, episode steps: 59, steps per second: 237, episode reward: 0.791, mean reward: 0.013 [-0.007, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.121 [-0.200, 0.682], loss: 0.000060, mean_absolute_error: 0.478656, mean_q: 0.719287\n",
      " 35861/50000: episode: 867, duration: 0.231s, episode steps: 48, steps per second: 208, episode reward: 0.848, mean reward: 0.018 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.102 [-0.566, 0.160], loss: 0.000242, mean_absolute_error: 0.481447, mean_q: 0.725210\n",
      " 35862/50000: episode: 868, duration: 0.007s, episode steps: 1, steps per second: 141, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.013 [-0.037, 0.010], loss: 0.000012, mean_absolute_error: 0.496651, mean_q: 0.730687\n",
      " 35894/50000: episode: 869, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 0.927, mean reward: 0.029 [-0.004, 1.000], mean action: 1.281 [0.000, 2.000], mean observation: -0.066 [-0.365, 0.150], loss: 0.000094, mean_absolute_error: 0.480515, mean_q: 0.719642\n",
      " 35950/50000: episode: 870, duration: 0.229s, episode steps: 56, steps per second: 245, episode reward: 0.721, mean reward: 0.013 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.182 [-0.859, 0.260], loss: 0.000096, mean_absolute_error: 0.481690, mean_q: 0.723576\n",
      " 36002/50000: episode: 871, duration: 0.223s, episode steps: 52, steps per second: 233, episode reward: 0.870, mean reward: 0.017 [-0.005, 1.000], mean action: 0.904 [0.000, 2.000], mean observation: 0.092 [-0.130, 0.453], loss: 0.000056, mean_absolute_error: 0.484587, mean_q: 0.727892\n",
      " 36052/50000: episode: 872, duration: 0.197s, episode steps: 50, steps per second: 254, episode reward: 0.806, mean reward: 0.016 [-0.007, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.136 [-0.210, 0.686], loss: 0.000077, mean_absolute_error: 0.483396, mean_q: 0.725548\n",
      " 36071/50000: episode: 873, duration: 0.081s, episode steps: 19, steps per second: 236, episode reward: 0.965, mean reward: 0.051 [-0.003, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.053 [-0.257, 0.140], loss: 0.000035, mean_absolute_error: 0.475713, mean_q: 0.712141\n",
      " 36130/50000: episode: 874, duration: 0.292s, episode steps: 59, steps per second: 202, episode reward: 0.745, mean reward: 0.013 [-0.008, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.151 [-0.809, 0.220], loss: 0.000054, mean_absolute_error: 0.479516, mean_q: 0.721067\n",
      " 36131/50000: episode: 875, duration: 0.008s, episode steps: 1, steps per second: 132, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.016 [-0.043, 0.010], loss: 0.000064, mean_absolute_error: 0.525899, mean_q: 0.789657\n",
      " 36166/50000: episode: 876, duration: 0.163s, episode steps: 35, steps per second: 214, episode reward: 0.923, mean reward: 0.026 [-0.004, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.073 [-0.360, 0.140], loss: 0.000048, mean_absolute_error: 0.482579, mean_q: 0.726812\n",
      " 36214/50000: episode: 877, duration: 0.237s, episode steps: 48, steps per second: 202, episode reward: 0.816, mean reward: 0.017 [-0.007, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.122 [-0.240, 0.683], loss: 0.000047, mean_absolute_error: 0.484527, mean_q: 0.728330\n",
      " 36244/50000: episode: 878, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 0.938, mean reward: 0.031 [-0.003, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.052 [-0.343, 0.160], loss: 0.000080, mean_absolute_error: 0.479117, mean_q: 0.719865\n",
      " 36306/50000: episode: 879, duration: 0.331s, episode steps: 62, steps per second: 187, episode reward: 0.664, mean reward: 0.011 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.200 [-0.943, 0.240], loss: 0.000303, mean_absolute_error: 0.487247, mean_q: 0.732535\n",
      " 36348/50000: episode: 880, duration: 0.180s, episode steps: 42, steps per second: 233, episode reward: 0.879, mean reward: 0.021 [-0.005, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.099 [-0.160, 0.479], loss: 0.000045, mean_absolute_error: 0.483168, mean_q: 0.727643\n",
      " 36399/50000: episode: 881, duration: 0.215s, episode steps: 51, steps per second: 237, episode reward: 0.761, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.166 [-0.230, 0.803], loss: 0.000060, mean_absolute_error: 0.476813, mean_q: 0.718409\n",
      " 36459/50000: episode: 882, duration: 0.328s, episode steps: 60, steps per second: 183, episode reward: 0.696, mean reward: 0.012 [-0.009, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.187 [-0.870, 0.230], loss: 0.000056, mean_absolute_error: 0.481735, mean_q: 0.723874\n",
      " 36460/50000: episode: 883, duration: 0.007s, episode steps: 1, steps per second: 149, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.033 [-0.076, 0.010], loss: 0.000058, mean_absolute_error: 0.481541, mean_q: 0.706341\n",
      " 36491/50000: episode: 884, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 0.927, mean reward: 0.030 [-0.004, 1.000], mean action: 1.194 [0.000, 2.000], mean observation: -0.075 [-0.372, 0.150], loss: 0.000064, mean_absolute_error: 0.480768, mean_q: 0.721372\n",
      " 36492/50000: episode: 885, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.082, 0.010], loss: 0.000098, mean_absolute_error: 0.501814, mean_q: 0.738160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36530/50000: episode: 886, duration: 0.279s, episode steps: 38, steps per second: 136, episode reward: 0.885, mean reward: 0.023 [-0.005, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.103 [-0.467, 0.160], loss: 0.000064, mean_absolute_error: 0.487076, mean_q: 0.734193\n",
      " 36586/50000: episode: 887, duration: 0.292s, episode steps: 56, steps per second: 192, episode reward: 0.719, mean reward: 0.013 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.184 [-0.250, 0.858], loss: 0.000058, mean_absolute_error: 0.484832, mean_q: 0.727696\n",
      " 36614/50000: episode: 888, duration: 0.117s, episode steps: 28, steps per second: 240, episode reward: 0.945, mean reward: 0.034 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.056 [-0.309, 0.130], loss: 0.000061, mean_absolute_error: 0.475950, mean_q: 0.716257\n",
      " 36635/50000: episode: 889, duration: 0.089s, episode steps: 21, steps per second: 237, episode reward: 0.960, mean reward: 0.046 [-0.003, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.056 [-0.271, 0.130], loss: 0.000050, mean_absolute_error: 0.477595, mean_q: 0.719376\n",
      " 36690/50000: episode: 890, duration: 0.299s, episode steps: 55, steps per second: 184, episode reward: 0.711, mean reward: 0.013 [-0.009, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.183 [-0.270, 0.914], loss: 0.000050, mean_absolute_error: 0.487492, mean_q: 0.731269\n",
      " 36734/50000: episode: 891, duration: 0.171s, episode steps: 44, steps per second: 257, episode reward: 0.868, mean reward: 0.020 [-0.005, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.096 [-0.529, 0.170], loss: 0.000067, mean_absolute_error: 0.484045, mean_q: 0.726984\n",
      " 36745/50000: episode: 892, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 0.364 [0.000, 2.000], mean observation: 0.040 [-0.080, 0.155], loss: 0.000066, mean_absolute_error: 0.482052, mean_q: 0.726788\n",
      " 36785/50000: episode: 893, duration: 0.194s, episode steps: 40, steps per second: 207, episode reward: 0.862, mean reward: 0.022 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.116 [-0.560, 0.180], loss: 0.000050, mean_absolute_error: 0.481906, mean_q: 0.723598\n",
      " 36815/50000: episode: 894, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 0.939, mean reward: 0.031 [-0.003, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.065 [-0.110, 0.306], loss: 0.000053, mean_absolute_error: 0.483603, mean_q: 0.728912\n",
      " 36862/50000: episode: 895, duration: 0.199s, episode steps: 47, steps per second: 236, episode reward: 0.792, mean reward: 0.017 [-0.007, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.155 [-0.240, 0.709], loss: 0.000048, mean_absolute_error: 0.483973, mean_q: 0.726233\n",
      " 36907/50000: episode: 896, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 0.839, mean reward: 0.019 [-0.006, 1.000], mean action: 1.156 [0.000, 2.000], mean observation: -0.124 [-0.603, 0.180], loss: 0.000040, mean_absolute_error: 0.481048, mean_q: 0.724049\n",
      " 36966/50000: episode: 897, duration: 0.257s, episode steps: 59, steps per second: 230, episode reward: 0.694, mean reward: 0.012 [-0.009, 1.000], mean action: 1.102 [0.000, 2.000], mean observation: -0.190 [-0.918, 0.210], loss: 0.000060, mean_absolute_error: 0.481747, mean_q: 0.724961\n",
      " 37000/50000: episode: 898, duration: 0.155s, episode steps: 34, steps per second: 219, episode reward: 0.912, mean reward: 0.027 [-0.004, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.088 [-0.140, 0.388], loss: 0.000059, mean_absolute_error: 0.481423, mean_q: 0.723872\n",
      " 37042/50000: episode: 899, duration: 0.169s, episode steps: 42, steps per second: 249, episode reward: 0.861, mean reward: 0.020 [-0.005, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.113 [-0.170, 0.546], loss: 0.000063, mean_absolute_error: 0.486734, mean_q: 0.731954\n",
      " 37061/50000: episode: 900, duration: 0.072s, episode steps: 19, steps per second: 263, episode reward: 0.972, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.046 [-0.100, 0.199], loss: 0.000062, mean_absolute_error: 0.483608, mean_q: 0.726139\n",
      " 37115/50000: episode: 901, duration: 0.242s, episode steps: 54, steps per second: 223, episode reward: 0.787, mean reward: 0.015 [-0.007, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.136 [-0.707, 0.210], loss: 0.000040, mean_absolute_error: 0.481278, mean_q: 0.725977\n",
      " 37161/50000: episode: 902, duration: 0.190s, episode steps: 46, steps per second: 242, episode reward: 0.821, mean reward: 0.018 [-0.007, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.135 [-0.220, 0.660], loss: 0.000043, mean_absolute_error: 0.484212, mean_q: 0.727886\n",
      " 37162/50000: episode: 903, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [-0.010, 0.059], loss: 0.000022, mean_absolute_error: 0.493921, mean_q: 0.746943\n",
      " 37221/50000: episode: 904, duration: 0.292s, episode steps: 59, steps per second: 202, episode reward: 0.720, mean reward: 0.012 [-0.009, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.151 [-0.250, 0.921], loss: 0.000054, mean_absolute_error: 0.476669, mean_q: 0.715857\n",
      " 37277/50000: episode: 905, duration: 0.227s, episode steps: 56, steps per second: 247, episode reward: 0.745, mean reward: 0.013 [-0.008, 1.000], mean action: 0.911 [0.000, 2.000], mean observation: 0.162 [-0.250, 0.838], loss: 0.000048, mean_absolute_error: 0.481911, mean_q: 0.723155\n",
      " 37278/50000: episode: 906, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [-0.010, 0.041], loss: 0.000021, mean_absolute_error: 0.486941, mean_q: 0.722411\n",
      " 37342/50000: episode: 907, duration: 0.265s, episode steps: 64, steps per second: 242, episode reward: 0.679, mean reward: 0.011 [-0.010, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.179 [-0.961, 0.230], loss: 0.000036, mean_absolute_error: 0.481480, mean_q: 0.724189\n",
      " 37386/50000: episode: 908, duration: 0.207s, episode steps: 44, steps per second: 213, episode reward: 0.831, mean reward: 0.019 [-0.006, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.127 [-0.240, 0.638], loss: 0.000046, mean_absolute_error: 0.490967, mean_q: 0.737319\n",
      " 37445/50000: episode: 909, duration: 0.284s, episode steps: 59, steps per second: 208, episode reward: 0.718, mean reward: 0.012 [-0.008, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.175 [-0.841, 0.220], loss: 0.000049, mean_absolute_error: 0.483931, mean_q: 0.727970\n",
      " 37464/50000: episode: 910, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 0.973, mean reward: 0.051 [-0.002, 1.000], mean action: 0.632 [0.000, 2.000], mean observation: 0.048 [-0.080, 0.194], loss: 0.000041, mean_absolute_error: 0.479787, mean_q: 0.719481\n",
      " 37511/50000: episode: 911, duration: 0.233s, episode steps: 47, steps per second: 201, episode reward: 0.821, mean reward: 0.017 [-0.006, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.132 [-0.638, 0.200], loss: 0.000044, mean_absolute_error: 0.479869, mean_q: 0.722821\n",
      " 37548/50000: episode: 912, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 0.878, mean reward: 0.024 [-0.005, 1.000], mean action: 0.838 [0.000, 2.000], mean observation: 0.108 [-0.200, 0.531], loss: 0.000053, mean_absolute_error: 0.485240, mean_q: 0.728995\n",
      " 37574/50000: episode: 913, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 0.959, mean reward: 0.037 [-0.002, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.048 [-0.110, 0.236], loss: 0.000051, mean_absolute_error: 0.492032, mean_q: 0.738567\n",
      " 37575/50000: episode: 914, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.023 [-0.056, 0.010], loss: 0.000103, mean_absolute_error: 0.499272, mean_q: 0.734727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37624/50000: episode: 915, duration: 0.187s, episode steps: 49, steps per second: 262, episode reward: 0.816, mean reward: 0.017 [-0.006, 1.000], mean action: 1.061 [0.000, 2.000], mean observation: -0.134 [-0.640, 0.200], loss: 0.000085, mean_absolute_error: 0.483226, mean_q: 0.724534\n",
      " 37669/50000: episode: 916, duration: 0.214s, episode steps: 45, steps per second: 210, episode reward: 0.827, mean reward: 0.018 [-0.007, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.127 [-0.200, 0.651], loss: 0.000038, mean_absolute_error: 0.489177, mean_q: 0.735571\n",
      " 37722/50000: episode: 917, duration: 0.228s, episode steps: 53, steps per second: 232, episode reward: 0.708, mean reward: 0.013 [-0.009, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.196 [-0.260, 0.941], loss: 0.000043, mean_absolute_error: 0.483584, mean_q: 0.727986\n",
      " 37788/50000: episode: 918, duration: 0.314s, episode steps: 66, steps per second: 210, episode reward: 0.630, mean reward: 0.010 [-0.010, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.212 [-0.966, 0.240], loss: 0.000045, mean_absolute_error: 0.481191, mean_q: 0.721831\n",
      " 37839/50000: episode: 919, duration: 0.383s, episode steps: 51, steps per second: 133, episode reward: 0.759, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.159 [-0.270, 0.835], loss: 0.000103, mean_absolute_error: 0.484767, mean_q: 0.728794\n",
      " 37865/50000: episode: 920, duration: 0.101s, episode steps: 26, steps per second: 258, episode reward: 0.948, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.054 [-0.310, 0.140], loss: 0.000045, mean_absolute_error: 0.472812, mean_q: 0.711008\n",
      " 37916/50000: episode: 921, duration: 0.220s, episode steps: 51, steps per second: 232, episode reward: 0.781, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.136 [-0.260, 0.792], loss: 0.000387, mean_absolute_error: 0.485552, mean_q: 0.731662\n",
      " 37932/50000: episode: 922, duration: 0.068s, episode steps: 16, steps per second: 234, episode reward: 0.975, mean reward: 0.061 [-0.002, 1.000], mean action: 0.562 [0.000, 2.000], mean observation: 0.046 [-0.110, 0.207], loss: 0.000103, mean_absolute_error: 0.491708, mean_q: 0.738960\n",
      " 37988/50000: episode: 923, duration: 0.263s, episode steps: 56, steps per second: 213, episode reward: 0.772, mean reward: 0.014 [-0.008, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.146 [-0.230, 0.755], loss: 0.000071, mean_absolute_error: 0.484646, mean_q: 0.730796\n",
      " 38021/50000: episode: 924, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 0.910, mean reward: 0.028 [-0.004, 1.000], mean action: 1.212 [0.000, 2.000], mean observation: -0.086 [-0.431, 0.170], loss: 0.000055, mean_absolute_error: 0.491414, mean_q: 0.742898\n",
      " 38035/50000: episode: 925, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 0.984, mean reward: 0.070 [-0.001, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.042 [-0.060, 0.150], loss: 0.000057, mean_absolute_error: 0.475899, mean_q: 0.716843\n",
      " 38052/50000: episode: 926, duration: 0.108s, episode steps: 17, steps per second: 158, episode reward: 0.973, mean reward: 0.057 [-0.002, 1.000], mean action: 1.529 [0.000, 2.000], mean observation: -0.043 [-0.218, 0.130], loss: 0.000035, mean_absolute_error: 0.475909, mean_q: 0.714187\n",
      " 38107/50000: episode: 927, duration: 0.253s, episode steps: 55, steps per second: 217, episode reward: 0.714, mean reward: 0.013 [-0.009, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.185 [-0.270, 0.913], loss: 0.000045, mean_absolute_error: 0.489186, mean_q: 0.736037\n",
      " 38161/50000: episode: 928, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 0.722, mean reward: 0.013 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.185 [-0.260, 0.874], loss: 0.000052, mean_absolute_error: 0.482121, mean_q: 0.723765\n",
      " 38206/50000: episode: 929, duration: 0.194s, episode steps: 45, steps per second: 232, episode reward: 0.839, mean reward: 0.019 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.119 [-0.614, 0.190], loss: 0.000062, mean_absolute_error: 0.481228, mean_q: 0.723193\n",
      " 38266/50000: episode: 930, duration: 0.292s, episode steps: 60, steps per second: 206, episode reward: 0.796, mean reward: 0.013 [-0.007, 1.000], mean action: 0.933 [0.000, 2.000], mean observation: 0.125 [-0.170, 0.654], loss: 0.000049, mean_absolute_error: 0.482614, mean_q: 0.727792\n",
      " 38328/50000: episode: 931, duration: 0.275s, episode steps: 62, steps per second: 226, episode reward: 0.663, mean reward: 0.011 [-0.010, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.202 [-0.969, 0.210], loss: 0.000059, mean_absolute_error: 0.485007, mean_q: 0.730243\n",
      " 38347/50000: episode: 932, duration: 0.082s, episode steps: 19, steps per second: 232, episode reward: 0.973, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.037 [-0.100, 0.200], loss: 0.000065, mean_absolute_error: 0.482982, mean_q: 0.727368\n",
      " 38393/50000: episode: 933, duration: 0.187s, episode steps: 46, steps per second: 245, episode reward: 0.879, mean reward: 0.019 [-0.005, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.080 [-0.160, 0.482], loss: 0.000045, mean_absolute_error: 0.479910, mean_q: 0.721854\n",
      " 38394/50000: episode: 934, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [-0.010, 0.061], loss: 0.000035, mean_absolute_error: 0.531319, mean_q: 0.798497\n",
      " 38422/50000: episode: 935, duration: 0.117s, episode steps: 28, steps per second: 240, episode reward: 0.940, mean reward: 0.034 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.064 [-0.140, 0.319], loss: 0.000048, mean_absolute_error: 0.481175, mean_q: 0.721495\n",
      " 38477/50000: episode: 936, duration: 0.261s, episode steps: 55, steps per second: 211, episode reward: 0.748, mean reward: 0.014 [-0.008, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.165 [-0.200, 0.811], loss: 0.000053, mean_absolute_error: 0.490388, mean_q: 0.738247\n",
      " 38530/50000: episode: 937, duration: 0.250s, episode steps: 53, steps per second: 212, episode reward: 0.741, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.171 [-0.260, 0.830], loss: 0.000050, mean_absolute_error: 0.480209, mean_q: 0.722037\n",
      " 38531/50000: episode: 938, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [-0.010, 0.096], loss: 0.000098, mean_absolute_error: 0.488716, mean_q: 0.737533\n",
      " 38577/50000: episode: 939, duration: 0.233s, episode steps: 46, steps per second: 197, episode reward: 0.848, mean reward: 0.018 [-0.006, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.104 [-0.210, 0.596], loss: 0.000045, mean_absolute_error: 0.485571, mean_q: 0.732108\n",
      " 38593/50000: episode: 940, duration: 0.067s, episode steps: 16, steps per second: 238, episode reward: 0.976, mean reward: 0.061 [-0.002, 1.000], mean action: 0.562 [0.000, 2.000], mean observation: 0.046 [-0.100, 0.199], loss: 0.000051, mean_absolute_error: 0.471877, mean_q: 0.712236\n",
      " 38600/50000: episode: 941, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.127, 0.070], loss: 0.000058, mean_absolute_error: 0.491789, mean_q: 0.740982\n",
      " 38648/50000: episode: 942, duration: 0.225s, episode steps: 48, steps per second: 213, episode reward: 0.804, mean reward: 0.017 [-0.007, 1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.144 [-0.685, 0.180], loss: 0.000066, mean_absolute_error: 0.479874, mean_q: 0.721944\n",
      " 38669/50000: episode: 943, duration: 0.134s, episode steps: 21, steps per second: 157, episode reward: 0.962, mean reward: 0.046 [-0.003, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.051 [-0.130, 0.257], loss: 0.000060, mean_absolute_error: 0.476272, mean_q: 0.715528\n",
      " 38677/50000: episode: 944, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 1.875 [1.000, 2.000], mean observation: -0.037 [-0.131, 0.070], loss: 0.000064, mean_absolute_error: 0.493849, mean_q: 0.741459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38688/50000: episode: 945, duration: 0.051s, episode steps: 11, steps per second: 216, episode reward: 0.988, mean reward: 0.090 [-0.001, 1.000], mean action: 1.636 [0.000, 2.000], mean observation: -0.043 [-0.138, 0.070], loss: 0.000069, mean_absolute_error: 0.494096, mean_q: 0.742138\n",
      " 38736/50000: episode: 946, duration: 0.232s, episode steps: 48, steps per second: 207, episode reward: 0.820, mean reward: 0.017 [-0.006, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.129 [-0.200, 0.639], loss: 0.000049, mean_absolute_error: 0.486890, mean_q: 0.733771\n",
      " 38777/50000: episode: 947, duration: 0.204s, episode steps: 41, steps per second: 201, episode reward: 0.872, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.098 [-0.536, 0.200], loss: 0.000052, mean_absolute_error: 0.493357, mean_q: 0.742905\n",
      " 38778/50000: episode: 948, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.036 [0.000, 0.071], loss: 0.000134, mean_absolute_error: 0.484710, mean_q: 0.733241\n",
      " 38837/50000: episode: 949, duration: 0.301s, episode steps: 59, steps per second: 196, episode reward: 0.696, mean reward: 0.012 [-0.009, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.182 [-0.260, 0.935], loss: 0.000052, mean_absolute_error: 0.485407, mean_q: 0.728905\n",
      " 38838/50000: episode: 950, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.082, 0.010], loss: 0.000052, mean_absolute_error: 0.471607, mean_q: 0.713041\n",
      " 38892/50000: episode: 951, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 0.739, mean reward: 0.014 [-0.008, 1.000], mean action: 1.093 [0.000, 2.000], mean observation: -0.174 [-0.840, 0.210], loss: 0.000072, mean_absolute_error: 0.482052, mean_q: 0.724268\n",
      " 38944/50000: episode: 952, duration: 0.304s, episode steps: 52, steps per second: 171, episode reward: 0.794, mean reward: 0.015 [-0.007, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.141 [-0.210, 0.698], loss: 0.000059, mean_absolute_error: 0.485750, mean_q: 0.730753\n",
      " 38945/50000: episode: 953, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.024 [-0.058, 0.010], loss: 0.000154, mean_absolute_error: 0.515574, mean_q: 0.774777\n",
      " 39003/50000: episode: 954, duration: 0.298s, episode steps: 58, steps per second: 195, episode reward: 0.730, mean reward: 0.013 [-0.008, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.174 [-0.240, 0.789], loss: 0.000042, mean_absolute_error: 0.484181, mean_q: 0.729018\n",
      " 39051/50000: episode: 955, duration: 0.332s, episode steps: 48, steps per second: 145, episode reward: 0.793, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.149 [-0.730, 0.190], loss: 0.000041, mean_absolute_error: 0.483299, mean_q: 0.728497\n",
      " 39072/50000: episode: 956, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 0.970, mean reward: 0.046 [-0.002, 1.000], mean action: 0.619 [0.000, 2.000], mean observation: 0.050 [-0.080, 0.193], loss: 0.000045, mean_absolute_error: 0.480975, mean_q: 0.725414\n",
      " 39099/50000: episode: 957, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 0.953, mean reward: 0.035 [-0.003, 1.000], mean action: 1.296 [0.000, 2.000], mean observation: -0.059 [-0.251, 0.100], loss: 0.000079, mean_absolute_error: 0.482963, mean_q: 0.726257\n",
      " 39132/50000: episode: 958, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 0.928, mean reward: 0.028 [-0.003, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.070 [-0.332, 0.110], loss: 0.000057, mean_absolute_error: 0.477874, mean_q: 0.717495\n",
      " 39177/50000: episode: 959, duration: 0.239s, episode steps: 45, steps per second: 188, episode reward: 0.852, mean reward: 0.019 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.105 [-0.571, 0.190], loss: 0.000035, mean_absolute_error: 0.479894, mean_q: 0.721671\n",
      " 39183/50000: episode: 960, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.118, 0.060], loss: 0.000080, mean_absolute_error: 0.475826, mean_q: 0.719361\n",
      " 39184/50000: episode: 961, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.044 [-0.099, 0.010], loss: 0.000042, mean_absolute_error: 0.477775, mean_q: 0.723019\n",
      " 39241/50000: episode: 962, duration: 0.353s, episode steps: 57, steps per second: 162, episode reward: 0.714, mean reward: 0.013 [-0.009, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.182 [-0.895, 0.250], loss: 0.000046, mean_absolute_error: 0.487596, mean_q: 0.730782\n",
      " 39276/50000: episode: 963, duration: 0.259s, episode steps: 35, steps per second: 135, episode reward: 0.893, mean reward: 0.026 [-0.005, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.097 [-0.190, 0.487], loss: 0.000045, mean_absolute_error: 0.481877, mean_q: 0.726691\n",
      " 39301/50000: episode: 964, duration: 0.198s, episode steps: 25, steps per second: 126, episode reward: 0.947, mean reward: 0.038 [-0.003, 1.000], mean action: 0.720 [0.000, 2.000], mean observation: 0.065 [-0.140, 0.308], loss: 0.000044, mean_absolute_error: 0.492958, mean_q: 0.740730\n",
      " 39354/50000: episode: 965, duration: 0.355s, episode steps: 53, steps per second: 149, episode reward: 0.715, mean reward: 0.013 [-0.009, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.189 [-0.250, 0.920], loss: 0.000050, mean_absolute_error: 0.482830, mean_q: 0.725612\n",
      " 39403/50000: episode: 966, duration: 0.275s, episode steps: 49, steps per second: 178, episode reward: 0.808, mean reward: 0.016 [-0.007, 1.000], mean action: 1.082 [0.000, 2.000], mean observation: -0.138 [-0.670, 0.210], loss: 0.000038, mean_absolute_error: 0.484857, mean_q: 0.728145\n",
      " 39446/50000: episode: 967, duration: 0.273s, episode steps: 43, steps per second: 157, episode reward: 0.871, mean reward: 0.020 [-0.005, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.095 [-0.495, 0.190], loss: 0.000106, mean_absolute_error: 0.479955, mean_q: 0.720126\n",
      " 39451/50000: episode: 968, duration: 0.058s, episode steps: 5, steps per second: 87, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [-0.050, 0.112], loss: 0.000076, mean_absolute_error: 0.499791, mean_q: 0.751122\n",
      " 39499/50000: episode: 969, duration: 0.385s, episode steps: 48, steps per second: 125, episode reward: 0.840, mean reward: 0.017 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.112 [-0.570, 0.170], loss: 0.000060, mean_absolute_error: 0.486276, mean_q: 0.729990\n",
      " 39523/50000: episode: 970, duration: 0.175s, episode steps: 24, steps per second: 137, episode reward: 0.958, mean reward: 0.040 [-0.002, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.057 [-0.100, 0.243], loss: 0.000043, mean_absolute_error: 0.477508, mean_q: 0.718090\n",
      " 39551/50000: episode: 971, duration: 0.212s, episode steps: 28, steps per second: 132, episode reward: 0.946, mean reward: 0.034 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.059 [-0.110, 0.294], loss: 0.000047, mean_absolute_error: 0.488995, mean_q: 0.733596\n",
      " 39598/50000: episode: 972, duration: 0.264s, episode steps: 47, steps per second: 178, episode reward: 0.770, mean reward: 0.016 [-0.008, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.168 [-0.250, 0.815], loss: 0.000059, mean_absolute_error: 0.483963, mean_q: 0.726023\n",
      " 39609/50000: episode: 973, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 0.988, mean reward: 0.090 [-0.001, 1.000], mean action: 0.182 [0.000, 2.000], mean observation: 0.031 [-0.100, 0.144], loss: 0.000038, mean_absolute_error: 0.476607, mean_q: 0.717794\n",
      " 39637/50000: episode: 974, duration: 0.138s, episode steps: 28, steps per second: 202, episode reward: 0.957, mean reward: 0.034 [-0.002, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.053 [-0.110, 0.236], loss: 0.000057, mean_absolute_error: 0.479189, mean_q: 0.722537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39638/50000: episode: 975, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.048 [-0.087, -0.010], loss: 0.000019, mean_absolute_error: 0.510771, mean_q: 0.771421\n",
      " 39653/50000: episode: 976, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 0.983, mean reward: 0.066 [-0.001, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.044 [-0.060, 0.148], loss: 0.000065, mean_absolute_error: 0.487670, mean_q: 0.734621\n",
      " 39690/50000: episode: 977, duration: 0.302s, episode steps: 37, steps per second: 123, episode reward: 0.883, mean reward: 0.024 [-0.005, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.099 [-0.521, 0.200], loss: 0.000063, mean_absolute_error: 0.489976, mean_q: 0.733225\n",
      " 39719/50000: episode: 978, duration: 0.208s, episode steps: 29, steps per second: 139, episode reward: 0.928, mean reward: 0.032 [-0.004, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.079 [-0.130, 0.373], loss: 0.000049, mean_absolute_error: 0.484047, mean_q: 0.726851\n",
      " 39720/50000: episode: 979, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.001 [-0.010, 0.011], loss: 0.000129, mean_absolute_error: 0.443390, mean_q: 0.669815\n",
      " 39749/50000: episode: 980, duration: 0.182s, episode steps: 29, steps per second: 159, episode reward: 0.946, mean reward: 0.033 [-0.003, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.052 [-0.289, 0.140], loss: 0.000045, mean_absolute_error: 0.481172, mean_q: 0.723686\n",
      " 39792/50000: episode: 981, duration: 0.229s, episode steps: 43, steps per second: 188, episode reward: 0.833, mean reward: 0.019 [-0.007, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.131 [-0.651, 0.210], loss: 0.000257, mean_absolute_error: 0.477720, mean_q: 0.715754\n",
      " 39852/50000: episode: 982, duration: 0.353s, episode steps: 60, steps per second: 170, episode reward: 0.744, mean reward: 0.012 [-0.008, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.147 [-0.808, 0.230], loss: 0.000076, mean_absolute_error: 0.490092, mean_q: 0.735498\n",
      " 39882/50000: episode: 983, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 0.943, mean reward: 0.031 [-0.003, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.058 [-0.120, 0.293], loss: 0.000033, mean_absolute_error: 0.481914, mean_q: 0.724934\n",
      " 39927/50000: episode: 984, duration: 0.301s, episode steps: 45, steps per second: 149, episode reward: 0.842, mean reward: 0.019 [-0.006, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.122 [-0.170, 0.593], loss: 0.000041, mean_absolute_error: 0.484908, mean_q: 0.727392\n",
      " 39963/50000: episode: 985, duration: 0.175s, episode steps: 36, steps per second: 206, episode reward: 0.898, mean reward: 0.025 [-0.004, 1.000], mean action: 0.806 [0.000, 2.000], mean observation: 0.095 [-0.140, 0.440], loss: 0.000044, mean_absolute_error: 0.488843, mean_q: 0.736494\n",
      " 39969/50000: episode: 986, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.060, 0.118], loss: 0.000086, mean_absolute_error: 0.490848, mean_q: 0.731820\n",
      " 40010/50000: episode: 987, duration: 0.241s, episode steps: 41, steps per second: 170, episode reward: 0.863, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.109 [-0.180, 0.553], loss: 0.000060, mean_absolute_error: 0.473676, mean_q: 0.709039\n",
      " 40070/50000: episode: 988, duration: 0.252s, episode steps: 60, steps per second: 238, episode reward: 0.681, mean reward: 0.011 [-0.009, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.195 [-0.930, 0.250], loss: 0.000049, mean_absolute_error: 0.483364, mean_q: 0.726406\n",
      " 40125/50000: episode: 989, duration: 0.291s, episode steps: 55, steps per second: 189, episode reward: 0.754, mean reward: 0.014 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.156 [-0.795, 0.230], loss: 0.000033, mean_absolute_error: 0.486817, mean_q: 0.732939\n",
      " 40126/50000: episode: 990, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.018 [-0.046, 0.010], loss: 0.000005, mean_absolute_error: 0.501533, mean_q: 0.759618\n",
      " 40156/50000: episode: 991, duration: 0.195s, episode steps: 30, steps per second: 154, episode reward: 0.924, mean reward: 0.031 [-0.004, 1.000], mean action: 1.267 [0.000, 2.000], mean observation: -0.079 [-0.386, 0.150], loss: 0.000048, mean_absolute_error: 0.486649, mean_q: 0.732844\n",
      " 40200/50000: episode: 992, duration: 0.230s, episode steps: 44, steps per second: 191, episode reward: 0.810, mean reward: 0.018 [-0.007, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.147 [-0.210, 0.707], loss: 0.000062, mean_absolute_error: 0.483450, mean_q: 0.724706\n",
      " 40201/50000: episode: 993, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.044 [-0.098, 0.010], loss: 0.000014, mean_absolute_error: 0.463055, mean_q: 0.703011\n",
      " 40265/50000: episode: 994, duration: 0.398s, episode steps: 64, steps per second: 161, episode reward: 0.735, mean reward: 0.011 [-0.008, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.145 [-0.778, 0.240], loss: 0.000049, mean_absolute_error: 0.486062, mean_q: 0.732477\n",
      " 40328/50000: episode: 995, duration: 0.302s, episode steps: 63, steps per second: 208, episode reward: 0.652, mean reward: 0.010 [-0.010, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.203 [-0.260, 0.990], loss: 0.000050, mean_absolute_error: 0.483665, mean_q: 0.724774\n",
      " 40373/50000: episode: 996, duration: 0.220s, episode steps: 45, steps per second: 204, episode reward: 0.830, mean reward: 0.018 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.126 [-0.636, 0.220], loss: 0.000077, mean_absolute_error: 0.477612, mean_q: 0.714844\n",
      " 40412/50000: episode: 997, duration: 0.218s, episode steps: 39, steps per second: 179, episode reward: 0.877, mean reward: 0.022 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.101 [-0.519, 0.170], loss: 0.000047, mean_absolute_error: 0.486038, mean_q: 0.730221\n",
      " 40455/50000: episode: 998, duration: 0.203s, episode steps: 43, steps per second: 212, episode reward: 0.851, mean reward: 0.020 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.108 [-0.190, 0.595], loss: 0.000055, mean_absolute_error: 0.491655, mean_q: 0.738141\n",
      " 40456/50000: episode: 999, duration: 0.007s, episode steps: 1, steps per second: 140, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [-0.010, 0.059], loss: 0.000024, mean_absolute_error: 0.502162, mean_q: 0.740085\n",
      " 40507/50000: episode: 1000, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 0.780, mean reward: 0.015 [-0.007, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.151 [-0.210, 0.744], loss: 0.000063, mean_absolute_error: 0.487396, mean_q: 0.733614\n",
      " 40508/50000: episode: 1001, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.019 [-0.049, 0.010], loss: 0.000019, mean_absolute_error: 0.482759, mean_q: 0.732262\n",
      " 40557/50000: episode: 1002, duration: 0.221s, episode steps: 49, steps per second: 221, episode reward: 0.800, mean reward: 0.016 [-0.007, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.133 [-0.240, 0.735], loss: 0.000124, mean_absolute_error: 0.485344, mean_q: 0.728301\n",
      " 40609/50000: episode: 1003, duration: 0.241s, episode steps: 52, steps per second: 215, episode reward: 0.760, mean reward: 0.015 [-0.008, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.165 [-0.762, 0.220], loss: 0.000060, mean_absolute_error: 0.489410, mean_q: 0.733923\n",
      " 40610/50000: episode: 1004, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.029 [-0.068, 0.010], loss: 0.000020, mean_absolute_error: 0.497701, mean_q: 0.750481\n",
      " 40630/50000: episode: 1005, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: 0.968, mean reward: 0.048 [-0.002, 1.000], mean action: 1.350 [0.000, 2.000], mean observation: -0.052 [-0.220, 0.090], loss: 0.000069, mean_absolute_error: 0.490602, mean_q: 0.732795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40686/50000: episode: 1006, duration: 0.258s, episode steps: 56, steps per second: 217, episode reward: 0.774, mean reward: 0.014 [-0.007, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.145 [-0.200, 0.725], loss: 0.000065, mean_absolute_error: 0.489731, mean_q: 0.735392\n",
      " 40687/50000: episode: 1007, duration: 0.008s, episode steps: 1, steps per second: 133, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.012 [-0.013, -0.010], loss: 0.000105, mean_absolute_error: 0.532481, mean_q: 0.802661\n",
      " 40764/50000: episode: 1008, duration: 0.324s, episode steps: 77, steps per second: 238, episode reward: 0.552, mean reward: 0.007 [-0.010, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.232 [-0.983, 0.240], loss: 0.000054, mean_absolute_error: 0.484355, mean_q: 0.726564\n",
      " 40765/50000: episode: 1009, duration: 0.008s, episode steps: 1, steps per second: 132, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.040 [-0.090, 0.010], loss: 0.000133, mean_absolute_error: 0.537658, mean_q: 0.805662\n",
      " 40822/50000: episode: 1010, duration: 0.285s, episode steps: 57, steps per second: 200, episode reward: 0.710, mean reward: 0.012 [-0.009, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.189 [-0.855, 0.220], loss: 0.000045, mean_absolute_error: 0.481054, mean_q: 0.721748\n",
      " 40852/50000: episode: 1011, duration: 0.230s, episode steps: 30, steps per second: 130, episode reward: 0.934, mean reward: 0.031 [-0.003, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.072 [-0.110, 0.332], loss: 0.000050, mean_absolute_error: 0.491028, mean_q: 0.736625\n",
      " 40887/50000: episode: 1012, duration: 0.169s, episode steps: 35, steps per second: 207, episode reward: 0.905, mean reward: 0.026 [-0.004, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.083 [-0.150, 0.438], loss: 0.000054, mean_absolute_error: 0.484940, mean_q: 0.728004\n",
      " 40933/50000: episode: 1013, duration: 0.223s, episode steps: 46, steps per second: 206, episode reward: 0.831, mean reward: 0.018 [-0.006, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.128 [-0.180, 0.625], loss: 0.000057, mean_absolute_error: 0.486945, mean_q: 0.728261\n",
      " 40982/50000: episode: 1014, duration: 0.213s, episode steps: 49, steps per second: 230, episode reward: 0.809, mean reward: 0.017 [-0.007, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.131 [-0.686, 0.200], loss: 0.000087, mean_absolute_error: 0.488243, mean_q: 0.731707\n",
      " 41014/50000: episode: 1015, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 0.914, mean reward: 0.029 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.084 [-0.422, 0.170], loss: 0.000057, mean_absolute_error: 0.489076, mean_q: 0.734185\n",
      " 41043/50000: episode: 1016, duration: 0.122s, episode steps: 29, steps per second: 237, episode reward: 0.935, mean reward: 0.032 [-0.004, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.063 [-0.160, 0.353], loss: 0.000043, mean_absolute_error: 0.493571, mean_q: 0.741455\n",
      " 41066/50000: episode: 1017, duration: 0.105s, episode steps: 23, steps per second: 220, episode reward: 0.955, mean reward: 0.042 [-0.003, 1.000], mean action: 1.348 [0.000, 2.000], mean observation: -0.059 [-0.280, 0.120], loss: 0.000051, mean_absolute_error: 0.482331, mean_q: 0.724995\n",
      " 41124/50000: episode: 1018, duration: 0.275s, episode steps: 58, steps per second: 211, episode reward: 0.682, mean reward: 0.012 [-0.010, 1.000], mean action: 1.103 [0.000, 2.000], mean observation: -0.201 [-0.965, 0.270], loss: 0.000054, mean_absolute_error: 0.487247, mean_q: 0.732470\n",
      " 41183/50000: episode: 1019, duration: 0.352s, episode steps: 59, steps per second: 168, episode reward: 0.677, mean reward: 0.011 [-0.010, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.197 [-0.260, 0.982], loss: 0.000068, mean_absolute_error: 0.486627, mean_q: 0.729572\n",
      " 41200/50000: episode: 1020, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 0.973, mean reward: 0.057 [-0.002, 1.000], mean action: 1.471 [0.000, 2.000], mean observation: -0.047 [-0.213, 0.110], loss: 0.000040, mean_absolute_error: 0.491618, mean_q: 0.738701\n",
      " 41222/50000: episode: 1021, duration: 0.137s, episode steps: 22, steps per second: 161, episode reward: 0.963, mean reward: 0.044 [-0.002, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.054 [-0.230, 0.090], loss: 0.000051, mean_absolute_error: 0.484587, mean_q: 0.728433\n",
      " 41265/50000: episode: 1022, duration: 0.344s, episode steps: 43, steps per second: 125, episode reward: 0.858, mean reward: 0.020 [-0.005, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.115 [-0.534, 0.180], loss: 0.000055, mean_absolute_error: 0.483949, mean_q: 0.725642\n",
      " 41275/50000: episode: 1023, duration: 0.045s, episode steps: 10, steps per second: 224, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.038 [-0.090, 0.143], loss: 0.000042, mean_absolute_error: 0.488271, mean_q: 0.736006\n",
      " 41333/50000: episode: 1024, duration: 0.453s, episode steps: 58, steps per second: 128, episode reward: 0.805, mean reward: 0.014 [-0.006, 1.000], mean action: 1.086 [0.000, 2.000], mean observation: -0.121 [-0.646, 0.200], loss: 0.000051, mean_absolute_error: 0.486191, mean_q: 0.729456\n",
      " 41359/50000: episode: 1025, duration: 0.235s, episode steps: 26, steps per second: 111, episode reward: 0.946, mean reward: 0.036 [-0.003, 1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.067 [-0.291, 0.120], loss: 0.000058, mean_absolute_error: 0.483244, mean_q: 0.723447\n",
      " 41378/50000: episode: 1026, duration: 0.165s, episode steps: 19, steps per second: 115, episode reward: 0.966, mean reward: 0.051 [-0.002, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.052 [-0.242, 0.130], loss: 0.000050, mean_absolute_error: 0.480733, mean_q: 0.720646\n",
      " 41394/50000: episode: 1027, duration: 0.163s, episode steps: 16, steps per second: 98, episode reward: 0.978, mean reward: 0.061 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.044 [-0.183, 0.080], loss: 0.000080, mean_absolute_error: 0.492232, mean_q: 0.737454\n",
      " 41447/50000: episode: 1028, duration: 0.360s, episode steps: 53, steps per second: 147, episode reward: 0.767, mean reward: 0.014 [-0.008, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.158 [-0.759, 0.220], loss: 0.000306, mean_absolute_error: 0.492283, mean_q: 0.737597\n",
      " 41501/50000: episode: 1029, duration: 0.408s, episode steps: 54, steps per second: 132, episode reward: 0.743, mean reward: 0.014 [-0.007, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.173 [-0.746, 0.250], loss: 0.000136, mean_absolute_error: 0.486022, mean_q: 0.728932\n",
      " 41529/50000: episode: 1030, duration: 0.227s, episode steps: 28, steps per second: 123, episode reward: 0.932, mean reward: 0.033 [-0.004, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: -0.076 [-0.359, 0.150], loss: 0.000069, mean_absolute_error: 0.482207, mean_q: 0.723348\n",
      " 41564/50000: episode: 1031, duration: 0.307s, episode steps: 35, steps per second: 114, episode reward: 0.917, mean reward: 0.026 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.068 [-0.401, 0.160], loss: 0.000059, mean_absolute_error: 0.482774, mean_q: 0.726051\n",
      " 41592/50000: episode: 1032, duration: 0.213s, episode steps: 28, steps per second: 132, episode reward: 0.963, mean reward: 0.034 [-0.002, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.029 [-0.140, 0.207], loss: 0.000578, mean_absolute_error: 0.482404, mean_q: 0.723055\n",
      " 41655/50000: episode: 1033, duration: 0.333s, episode steps: 63, steps per second: 189, episode reward: 0.714, mean reward: 0.011 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.163 [-0.190, 0.860], loss: 0.000136, mean_absolute_error: 0.486098, mean_q: 0.729667\n",
      " 41678/50000: episode: 1034, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 0.959, mean reward: 0.042 [-0.003, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.052 [-0.120, 0.253], loss: 0.000069, mean_absolute_error: 0.489739, mean_q: 0.732747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41712/50000: episode: 1035, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 0.916, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.068 [-0.160, 0.413], loss: 0.000056, mean_absolute_error: 0.482054, mean_q: 0.725105\n",
      " 41762/50000: episode: 1036, duration: 0.238s, episode steps: 50, steps per second: 210, episode reward: 0.804, mean reward: 0.016 [-0.007, 1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.137 [-0.699, 0.210], loss: 0.000066, mean_absolute_error: 0.484672, mean_q: 0.725220\n",
      " 41823/50000: episode: 1037, duration: 0.273s, episode steps: 61, steps per second: 223, episode reward: 0.675, mean reward: 0.011 [-0.010, 1.000], mean action: 0.869 [0.000, 2.000], mean observation: 0.196 [-0.240, 0.964], loss: 0.000063, mean_absolute_error: 0.485819, mean_q: 0.727696\n",
      " 41851/50000: episode: 1038, duration: 0.172s, episode steps: 28, steps per second: 163, episode reward: 0.942, mean reward: 0.034 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.062 [-0.130, 0.309], loss: 0.000071, mean_absolute_error: 0.486364, mean_q: 0.728689\n",
      " 41904/50000: episode: 1039, duration: 0.247s, episode steps: 53, steps per second: 215, episode reward: 0.790, mean reward: 0.015 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.136 [-0.190, 0.715], loss: 0.000059, mean_absolute_error: 0.481611, mean_q: 0.723055\n",
      " 41937/50000: episode: 1040, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 0.906, mean reward: 0.027 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.090 [-0.443, 0.160], loss: 0.000067, mean_absolute_error: 0.489213, mean_q: 0.733593\n",
      " 41938/50000: episode: 1041, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.023 [-0.010, 0.057], loss: 0.000029, mean_absolute_error: 0.517675, mean_q: 0.780568\n",
      " 41991/50000: episode: 1042, duration: 0.233s, episode steps: 53, steps per second: 228, episode reward: 0.769, mean reward: 0.015 [-0.007, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.156 [-0.742, 0.230], loss: 0.000053, mean_absolute_error: 0.489537, mean_q: 0.736397\n",
      " 42049/50000: episode: 1043, duration: 0.253s, episode steps: 58, steps per second: 229, episode reward: 0.725, mean reward: 0.012 [-0.008, 1.000], mean action: 1.103 [0.000, 2.000], mean observation: -0.182 [-0.753, 0.230], loss: 0.000065, mean_absolute_error: 0.486978, mean_q: 0.731119\n",
      " 42071/50000: episode: 1044, duration: 0.095s, episode steps: 22, steps per second: 231, episode reward: 0.959, mean reward: 0.044 [-0.003, 1.000], mean action: 1.409 [0.000, 2.000], mean observation: -0.059 [-0.255, 0.100], loss: 0.000033, mean_absolute_error: 0.480467, mean_q: 0.723850\n",
      " 42128/50000: episode: 1045, duration: 0.314s, episode steps: 57, steps per second: 181, episode reward: 0.732, mean reward: 0.013 [-0.008, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.169 [-0.220, 0.820], loss: 0.000053, mean_absolute_error: 0.486003, mean_q: 0.728567\n",
      " 42173/50000: episode: 1046, duration: 0.235s, episode steps: 45, steps per second: 191, episode reward: 0.858, mean reward: 0.019 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.102 [-0.170, 0.553], loss: 0.000050, mean_absolute_error: 0.486710, mean_q: 0.726994\n",
      " 42224/50000: episode: 1047, duration: 0.326s, episode steps: 51, steps per second: 157, episode reward: 0.788, mean reward: 0.015 [-0.007, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.148 [-0.717, 0.200], loss: 0.000052, mean_absolute_error: 0.484842, mean_q: 0.729028\n",
      " 42284/50000: episode: 1048, duration: 0.299s, episode steps: 60, steps per second: 201, episode reward: 0.689, mean reward: 0.011 [-0.009, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.189 [-0.946, 0.260], loss: 0.000058, mean_absolute_error: 0.482433, mean_q: 0.723979\n",
      " 42285/50000: episode: 1049, duration: 0.008s, episode steps: 1, steps per second: 131, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.011 [-0.012, -0.010], loss: 0.000061, mean_absolute_error: 0.502014, mean_q: 0.758082\n",
      " 42341/50000: episode: 1050, duration: 0.244s, episode steps: 56, steps per second: 230, episode reward: 0.714, mean reward: 0.013 [-0.009, 1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.186 [-0.881, 0.250], loss: 0.000049, mean_absolute_error: 0.490810, mean_q: 0.735577\n",
      " 42373/50000: episode: 1051, duration: 0.175s, episode steps: 32, steps per second: 182, episode reward: 0.930, mean reward: 0.029 [-0.004, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.058 [-0.170, 0.355], loss: 0.000073, mean_absolute_error: 0.494732, mean_q: 0.744576\n",
      " 42396/50000: episode: 1052, duration: 0.160s, episode steps: 23, steps per second: 144, episode reward: 0.951, mean reward: 0.041 [-0.003, 1.000], mean action: 0.652 [0.000, 2.000], mean observation: 0.063 [-0.150, 0.303], loss: 0.000045, mean_absolute_error: 0.497057, mean_q: 0.747645\n",
      " 42454/50000: episode: 1053, duration: 0.236s, episode steps: 58, steps per second: 246, episode reward: 0.738, mean reward: 0.013 [-0.008, 1.000], mean action: 1.086 [0.000, 2.000], mean observation: -0.163 [-0.836, 0.240], loss: 0.000054, mean_absolute_error: 0.485917, mean_q: 0.727968\n",
      " 42496/50000: episode: 1054, duration: 0.169s, episode steps: 42, steps per second: 248, episode reward: 0.860, mean reward: 0.020 [-0.006, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.101 [-0.200, 0.570], loss: 0.000370, mean_absolute_error: 0.485414, mean_q: 0.728205\n",
      " 42531/50000: episode: 1055, duration: 0.152s, episode steps: 35, steps per second: 231, episode reward: 0.892, mean reward: 0.025 [-0.005, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.100 [-0.180, 0.488], loss: 0.000333, mean_absolute_error: 0.482881, mean_q: 0.723753\n",
      " 42552/50000: episode: 1056, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 0.974, mean reward: 0.046 [-0.001, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.043 [-0.148, 0.110], loss: 0.000103, mean_absolute_error: 0.492986, mean_q: 0.739765\n",
      " 42558/50000: episode: 1057, duration: 0.028s, episode steps: 6, steps per second: 213, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.060, 0.117], loss: 0.000031, mean_absolute_error: 0.467276, mean_q: 0.703847\n",
      " 42597/50000: episode: 1058, duration: 0.219s, episode steps: 39, steps per second: 178, episode reward: 0.893, mean reward: 0.023 [-0.004, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.093 [-0.449, 0.160], loss: 0.000064, mean_absolute_error: 0.485632, mean_q: 0.726006\n",
      " 42640/50000: episode: 1059, duration: 0.184s, episode steps: 43, steps per second: 234, episode reward: 0.872, mean reward: 0.020 [-0.005, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.104 [-0.493, 0.170], loss: 0.000089, mean_absolute_error: 0.489779, mean_q: 0.733208\n",
      " 42641/50000: episode: 1060, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [-0.010, 0.041], loss: 0.000036, mean_absolute_error: 0.492965, mean_q: 0.739933\n",
      " 42704/50000: episode: 1061, duration: 0.278s, episode steps: 63, steps per second: 226, episode reward: 0.645, mean reward: 0.010 [-0.009, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.215 [-0.948, 0.240], loss: 0.000063, mean_absolute_error: 0.490093, mean_q: 0.733199\n",
      " 42742/50000: episode: 1062, duration: 0.179s, episode steps: 38, steps per second: 212, episode reward: 0.878, mean reward: 0.023 [-0.005, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.105 [-0.160, 0.508], loss: 0.000055, mean_absolute_error: 0.484463, mean_q: 0.724159\n",
      " 42783/50000: episode: 1063, duration: 0.194s, episode steps: 41, steps per second: 212, episode reward: 0.875, mean reward: 0.021 [-0.005, 1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.105 [-0.495, 0.160], loss: 0.000052, mean_absolute_error: 0.484968, mean_q: 0.728565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42832/50000: episode: 1064, duration: 0.207s, episode steps: 49, steps per second: 237, episode reward: 0.799, mean reward: 0.016 [-0.007, 1.000], mean action: 1.122 [0.000, 2.000], mean observation: -0.143 [-0.704, 0.210], loss: 0.000058, mean_absolute_error: 0.490443, mean_q: 0.736278\n",
      " 42858/50000: episode: 1065, duration: 0.108s, episode steps: 26, steps per second: 240, episode reward: 0.941, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.069 [-0.331, 0.130], loss: 0.000059, mean_absolute_error: 0.495115, mean_q: 0.741730\n",
      " 42921/50000: episode: 1066, duration: 0.266s, episode steps: 63, steps per second: 237, episode reward: 0.682, mean reward: 0.011 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.185 [-0.230, 0.914], loss: 0.000063, mean_absolute_error: 0.488637, mean_q: 0.733513\n",
      " 42922/50000: episode: 1067, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.010 [-0.010, -0.009], loss: 0.000269, mean_absolute_error: 0.486107, mean_q: 0.735497\n",
      " 42980/50000: episode: 1068, duration: 0.229s, episode steps: 58, steps per second: 253, episode reward: 0.772, mean reward: 0.013 [-0.007, 1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.144 [-0.716, 0.210], loss: 0.000049, mean_absolute_error: 0.484155, mean_q: 0.727998\n",
      " 43038/50000: episode: 1069, duration: 0.281s, episode steps: 58, steps per second: 207, episode reward: 0.699, mean reward: 0.012 [-0.009, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.192 [-0.200, 0.889], loss: 0.000055, mean_absolute_error: 0.487146, mean_q: 0.731389\n",
      " 43039/50000: episode: 1070, duration: 0.008s, episode steps: 1, steps per second: 131, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.092, 0.010], loss: 0.000021, mean_absolute_error: 0.502686, mean_q: 0.759891\n",
      " 43100/50000: episode: 1071, duration: 0.307s, episode steps: 61, steps per second: 199, episode reward: 0.702, mean reward: 0.012 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.179 [-0.894, 0.240], loss: 0.000045, mean_absolute_error: 0.484631, mean_q: 0.727237\n",
      " 43145/50000: episode: 1072, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 0.856, mean reward: 0.019 [-0.006, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.110 [-0.557, 0.170], loss: 0.000259, mean_absolute_error: 0.491497, mean_q: 0.739862\n",
      " 43194/50000: episode: 1073, duration: 0.251s, episode steps: 49, steps per second: 196, episode reward: 0.819, mean reward: 0.017 [-0.006, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.127 [-0.642, 0.190], loss: 0.000068, mean_absolute_error: 0.485820, mean_q: 0.729423\n",
      " 43204/50000: episode: 1074, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.040 [-0.080, 0.136], loss: 0.000039, mean_absolute_error: 0.490603, mean_q: 0.736531\n",
      " 43246/50000: episode: 1075, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 0.864, mean reward: 0.021 [-0.005, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.108 [-0.170, 0.537], loss: 0.000056, mean_absolute_error: 0.496117, mean_q: 0.748675\n",
      " 43267/50000: episode: 1076, duration: 0.087s, episode steps: 21, steps per second: 241, episode reward: 0.965, mean reward: 0.046 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.050 [-0.110, 0.235], loss: 0.000071, mean_absolute_error: 0.493259, mean_q: 0.738512\n",
      " 43314/50000: episode: 1077, duration: 0.221s, episode steps: 47, steps per second: 212, episode reward: 0.809, mean reward: 0.017 [-0.007, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.136 [-0.210, 0.700], loss: 0.000123, mean_absolute_error: 0.482307, mean_q: 0.721454\n",
      " 43319/50000: episode: 1078, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.050, 0.110], loss: 0.000089, mean_absolute_error: 0.487017, mean_q: 0.738375\n",
      " 43320/50000: episode: 1079, duration: 0.008s, episode steps: 1, steps per second: 120, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [-0.010, 0.016], loss: 0.000055, mean_absolute_error: 0.493736, mean_q: 0.738589\n",
      " 43332/50000: episode: 1080, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 0.985, mean reward: 0.082 [-0.002, 1.000], mean action: 1.667 [0.000, 2.000], mean observation: -0.037 [-0.164, 0.100], loss: 0.000049, mean_absolute_error: 0.493240, mean_q: 0.746239\n",
      " 43383/50000: episode: 1081, duration: 0.244s, episode steps: 51, steps per second: 209, episode reward: 0.785, mean reward: 0.015 [-0.007, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.149 [-0.717, 0.180], loss: 0.000062, mean_absolute_error: 0.487922, mean_q: 0.727917\n",
      " 43436/50000: episode: 1082, duration: 0.231s, episode steps: 53, steps per second: 229, episode reward: 0.754, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.167 [-0.200, 0.790], loss: 0.000035, mean_absolute_error: 0.484353, mean_q: 0.725475\n",
      " 43486/50000: episode: 1083, duration: 0.241s, episode steps: 50, steps per second: 207, episode reward: 0.807, mean reward: 0.016 [-0.007, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.135 [-0.668, 0.180], loss: 0.000058, mean_absolute_error: 0.488762, mean_q: 0.733853\n",
      " 43543/50000: episode: 1084, duration: 0.244s, episode steps: 57, steps per second: 234, episode reward: 0.713, mean reward: 0.013 [-0.009, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.183 [-0.895, 0.240], loss: 0.000044, mean_absolute_error: 0.485639, mean_q: 0.729414\n",
      " 43606/50000: episode: 1085, duration: 0.362s, episode steps: 63, steps per second: 174, episode reward: 0.654, mean reward: 0.010 [-0.010, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.207 [-0.952, 0.230], loss: 0.000038, mean_absolute_error: 0.485518, mean_q: 0.729007\n",
      " 43663/50000: episode: 1086, duration: 0.265s, episode steps: 57, steps per second: 215, episode reward: 0.715, mean reward: 0.013 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.179 [-0.250, 0.895], loss: 0.000059, mean_absolute_error: 0.486803, mean_q: 0.731756\n",
      " 43664/50000: episode: 1087, duration: 0.008s, episode steps: 1, steps per second: 119, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.083, 0.010], loss: 0.000123, mean_absolute_error: 0.433980, mean_q: 0.653547\n",
      " 43734/50000: episode: 1088, duration: 0.377s, episode steps: 70, steps per second: 186, episode reward: 0.651, mean reward: 0.009 [-0.010, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.182 [-0.962, 0.210], loss: 0.000047, mean_absolute_error: 0.485626, mean_q: 0.727843\n",
      " 43767/50000: episode: 1089, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 0.920, mean reward: 0.028 [-0.004, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.072 [-0.150, 0.385], loss: 0.000046, mean_absolute_error: 0.496503, mean_q: 0.744392\n",
      " 43796/50000: episode: 1090, duration: 0.139s, episode steps: 29, steps per second: 208, episode reward: 0.934, mean reward: 0.032 [-0.003, 1.000], mean action: 0.759 [0.000, 2.000], mean observation: 0.072 [-0.150, 0.346], loss: 0.000076, mean_absolute_error: 0.487362, mean_q: 0.727985\n",
      " 43856/50000: episode: 1091, duration: 0.246s, episode steps: 60, steps per second: 244, episode reward: 0.663, mean reward: 0.011 [-0.010, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.202 [-0.250, 0.989], loss: 0.000054, mean_absolute_error: 0.488510, mean_q: 0.732538\n",
      " 43901/50000: episode: 1092, duration: 0.198s, episode steps: 45, steps per second: 227, episode reward: 0.857, mean reward: 0.019 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.112 [-0.150, 0.527], loss: 0.000051, mean_absolute_error: 0.489684, mean_q: 0.734987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 43962/50000: episode: 1093, duration: 0.296s, episode steps: 61, steps per second: 206, episode reward: 0.677, mean reward: 0.011 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.197 [-0.936, 0.210], loss: 0.000054, mean_absolute_error: 0.483782, mean_q: 0.728134\n",
      " 44016/50000: episode: 1094, duration: 0.261s, episode steps: 54, steps per second: 207, episode reward: 0.769, mean reward: 0.014 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.152 [-0.765, 0.200], loss: 0.000052, mean_absolute_error: 0.492260, mean_q: 0.736559\n",
      " 44061/50000: episode: 1095, duration: 0.195s, episode steps: 45, steps per second: 231, episode reward: 0.837, mean reward: 0.019 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.125 [-0.607, 0.180], loss: 0.000050, mean_absolute_error: 0.488989, mean_q: 0.733208\n",
      " 44100/50000: episode: 1096, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 0.887, mean reward: 0.023 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.097 [-0.470, 0.160], loss: 0.000055, mean_absolute_error: 0.490635, mean_q: 0.737450\n",
      " 44145/50000: episode: 1097, duration: 0.221s, episode steps: 45, steps per second: 204, episode reward: 0.886, mean reward: 0.020 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.069 [-0.200, 0.496], loss: 0.000380, mean_absolute_error: 0.491529, mean_q: 0.736169\n",
      " 44177/50000: episode: 1098, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 0.932, mean reward: 0.029 [-0.003, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.060 [-0.150, 0.333], loss: 0.000111, mean_absolute_error: 0.486361, mean_q: 0.725076\n",
      " 44234/50000: episode: 1099, duration: 0.302s, episode steps: 57, steps per second: 189, episode reward: 0.731, mean reward: 0.013 [-0.008, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.175 [-0.801, 0.200], loss: 0.000063, mean_absolute_error: 0.495071, mean_q: 0.743400\n",
      " 44295/50000: episode: 1100, duration: 0.264s, episode steps: 61, steps per second: 231, episode reward: 0.658, mean reward: 0.011 [-0.010, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.202 [-0.260, 0.991], loss: 0.000065, mean_absolute_error: 0.488833, mean_q: 0.734568\n",
      " 44350/50000: episode: 1101, duration: 0.252s, episode steps: 55, steps per second: 218, episode reward: 0.725, mean reward: 0.013 [-0.009, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.177 [-0.230, 0.884], loss: 0.000051, mean_absolute_error: 0.486724, mean_q: 0.733230\n",
      " 44404/50000: episode: 1102, duration: 0.258s, episode steps: 54, steps per second: 209, episode reward: 0.732, mean reward: 0.014 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.177 [-0.230, 0.871], loss: 0.000046, mean_absolute_error: 0.495850, mean_q: 0.746304\n",
      " 44421/50000: episode: 1103, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 0.973, mean reward: 0.057 [-0.002, 1.000], mean action: 0.471 [0.000, 2.000], mean observation: 0.046 [-0.120, 0.218], loss: 0.000044, mean_absolute_error: 0.484303, mean_q: 0.728958\n",
      " 44460/50000: episode: 1104, duration: 0.234s, episode steps: 39, steps per second: 167, episode reward: 0.904, mean reward: 0.023 [-0.004, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.073 [-0.428, 0.150], loss: 0.000053, mean_absolute_error: 0.497135, mean_q: 0.744033\n",
      " 44497/50000: episode: 1105, duration: 0.255s, episode steps: 37, steps per second: 145, episode reward: 0.900, mean reward: 0.024 [-0.004, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.084 [-0.442, 0.150], loss: 0.000038, mean_absolute_error: 0.486099, mean_q: 0.729942\n",
      " 44519/50000: episode: 1106, duration: 0.098s, episode steps: 22, steps per second: 224, episode reward: 0.963, mean reward: 0.044 [-0.002, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.050 [-0.130, 0.236], loss: 0.000033, mean_absolute_error: 0.490969, mean_q: 0.737301\n",
      " 44535/50000: episode: 1107, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 1.562 [0.000, 2.000], mean observation: -0.046 [-0.185, 0.090], loss: 0.000041, mean_absolute_error: 0.490635, mean_q: 0.732458\n",
      " 44587/50000: episode: 1108, duration: 0.341s, episode steps: 52, steps per second: 152, episode reward: 0.780, mean reward: 0.015 [-0.007, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.148 [-0.210, 0.737], loss: 0.000062, mean_absolute_error: 0.488290, mean_q: 0.730431\n",
      " 44610/50000: episode: 1109, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 0.959, mean reward: 0.042 [-0.003, 1.000], mean action: 1.391 [0.000, 2.000], mean observation: -0.055 [-0.252, 0.110], loss: 0.000035, mean_absolute_error: 0.486602, mean_q: 0.732677\n",
      " 44611/50000: episode: 1110, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.013 [-0.037, 0.010], loss: 0.000148, mean_absolute_error: 0.490622, mean_q: 0.740090\n",
      " 44612/50000: episode: 1111, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.009 [-0.010, -0.007], loss: 0.000045, mean_absolute_error: 0.486210, mean_q: 0.735964\n",
      " 44665/50000: episode: 1112, duration: 0.367s, episode steps: 53, steps per second: 144, episode reward: 0.778, mean reward: 0.015 [-0.007, 1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.149 [-0.747, 0.200], loss: 0.000050, mean_absolute_error: 0.494071, mean_q: 0.741153\n",
      " 44704/50000: episode: 1113, duration: 0.195s, episode steps: 39, steps per second: 200, episode reward: 0.874, mean reward: 0.022 [-0.005, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.106 [-0.200, 0.509], loss: 0.000061, mean_absolute_error: 0.491409, mean_q: 0.735162\n",
      " 44735/50000: episode: 1114, duration: 0.130s, episode steps: 31, steps per second: 239, episode reward: 0.925, mean reward: 0.030 [-0.004, 1.000], mean action: 1.258 [0.000, 2.000], mean observation: -0.080 [-0.367, 0.120], loss: 0.000036, mean_absolute_error: 0.486368, mean_q: 0.727293\n",
      " 44787/50000: episode: 1115, duration: 0.226s, episode steps: 52, steps per second: 230, episode reward: 0.783, mean reward: 0.015 [-0.007, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.151 [-0.707, 0.180], loss: 0.000052, mean_absolute_error: 0.488682, mean_q: 0.730843\n",
      " 44841/50000: episode: 1116, duration: 0.234s, episode steps: 54, steps per second: 231, episode reward: 0.744, mean reward: 0.014 [-0.008, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.160 [-0.250, 0.842], loss: 0.000110, mean_absolute_error: 0.496538, mean_q: 0.746290\n",
      " 44877/50000: episode: 1117, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 0.903, mean reward: 0.025 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.075 [-0.180, 0.454], loss: 0.000306, mean_absolute_error: 0.491063, mean_q: 0.739789\n",
      " 44918/50000: episode: 1118, duration: 0.195s, episode steps: 41, steps per second: 211, episode reward: 0.863, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.114 [-0.538, 0.150], loss: 0.000053, mean_absolute_error: 0.490753, mean_q: 0.738507\n",
      " 44938/50000: episode: 1119, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 0.965, mean reward: 0.048 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.051 [-0.242, 0.110], loss: 0.000035, mean_absolute_error: 0.493412, mean_q: 0.742868\n",
      " 44959/50000: episode: 1120, duration: 0.147s, episode steps: 21, steps per second: 143, episode reward: 0.962, mean reward: 0.046 [-0.003, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.052 [-0.130, 0.251], loss: 0.000080, mean_absolute_error: 0.496308, mean_q: 0.747190\n",
      " 44997/50000: episode: 1121, duration: 0.172s, episode steps: 38, steps per second: 221, episode reward: 0.867, mean reward: 0.023 [-0.006, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.115 [-0.200, 0.559], loss: 0.000067, mean_absolute_error: 0.486986, mean_q: 0.730285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45029/50000: episode: 1122, duration: 0.128s, episode steps: 32, steps per second: 251, episode reward: 0.919, mean reward: 0.029 [-0.004, 1.000], mean action: 1.281 [0.000, 2.000], mean observation: -0.079 [-0.394, 0.140], loss: 0.000043, mean_absolute_error: 0.487562, mean_q: 0.731915\n",
      " 45071/50000: episode: 1123, duration: 0.211s, episode steps: 42, steps per second: 199, episode reward: 0.845, mean reward: 0.020 [-0.006, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.124 [-0.190, 0.607], loss: 0.000050, mean_absolute_error: 0.490924, mean_q: 0.739002\n",
      " 45112/50000: episode: 1124, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 0.865, mean reward: 0.021 [-0.005, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.111 [-0.170, 0.524], loss: 0.000042, mean_absolute_error: 0.493927, mean_q: 0.740554\n",
      " 45113/50000: episode: 1125, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [-0.010, 0.098], loss: 0.000038, mean_absolute_error: 0.499581, mean_q: 0.754342\n",
      " 45114/50000: episode: 1126, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.017 [-0.044, 0.010], loss: 0.000053, mean_absolute_error: 0.493599, mean_q: 0.748554\n",
      " 45140/50000: episode: 1127, duration: 0.106s, episode steps: 26, steps per second: 245, episode reward: 0.947, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.065 [-0.289, 0.120], loss: 0.000058, mean_absolute_error: 0.498596, mean_q: 0.749478\n",
      " 45193/50000: episode: 1128, duration: 0.225s, episode steps: 53, steps per second: 235, episode reward: 0.716, mean reward: 0.014 [-0.009, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.192 [-0.250, 0.894], loss: 0.000042, mean_absolute_error: 0.490367, mean_q: 0.734542\n",
      " 45244/50000: episode: 1129, duration: 0.227s, episode steps: 51, steps per second: 225, episode reward: 0.774, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.144 [-0.250, 0.799], loss: 0.000047, mean_absolute_error: 0.496067, mean_q: 0.745660\n",
      " 45250/50000: episode: 1130, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.060, 0.118], loss: 0.000028, mean_absolute_error: 0.491782, mean_q: 0.741848\n",
      " 45277/50000: episode: 1131, duration: 0.123s, episode steps: 27, steps per second: 219, episode reward: 0.943, mean reward: 0.035 [-0.003, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.067 [-0.120, 0.316], loss: 0.000034, mean_absolute_error: 0.489013, mean_q: 0.729393\n",
      " 45278/50000: episode: 1132, duration: 0.008s, episode steps: 1, steps per second: 124, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.037 [-0.083, 0.010], loss: 0.000114, mean_absolute_error: 0.483485, mean_q: 0.704877\n",
      " 45279/50000: episode: 1133, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.081, 0.010], loss: 0.000015, mean_absolute_error: 0.499128, mean_q: 0.756796\n",
      " 45339/50000: episode: 1134, duration: 0.245s, episode steps: 60, steps per second: 245, episode reward: 0.716, mean reward: 0.012 [-0.009, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.171 [-0.871, 0.220], loss: 0.000051, mean_absolute_error: 0.490203, mean_q: 0.735986\n",
      " 45389/50000: episode: 1135, duration: 0.251s, episode steps: 50, steps per second: 199, episode reward: 0.811, mean reward: 0.016 [-0.006, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.132 [-0.180, 0.638], loss: 0.000062, mean_absolute_error: 0.497720, mean_q: 0.747456\n",
      " 45390/50000: episode: 1136, duration: 0.007s, episode steps: 1, steps per second: 149, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.022 [-0.053, 0.010], loss: 0.000039, mean_absolute_error: 0.510879, mean_q: 0.773533\n",
      " 45438/50000: episode: 1137, duration: 0.252s, episode steps: 48, steps per second: 190, episode reward: 0.808, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.142 [-0.663, 0.160], loss: 0.000056, mean_absolute_error: 0.497598, mean_q: 0.746715\n",
      " 45478/50000: episode: 1138, duration: 0.176s, episode steps: 40, steps per second: 227, episode reward: 0.888, mean reward: 0.022 [-0.005, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.089 [-0.160, 0.462], loss: 0.000047, mean_absolute_error: 0.487052, mean_q: 0.729750\n",
      " 45523/50000: episode: 1139, duration: 0.211s, episode steps: 45, steps per second: 213, episode reward: 0.846, mean reward: 0.019 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.118 [-0.573, 0.140], loss: 0.000064, mean_absolute_error: 0.497421, mean_q: 0.746883\n",
      " 45553/50000: episode: 1140, duration: 0.129s, episode steps: 30, steps per second: 233, episode reward: 0.930, mean reward: 0.031 [-0.004, 1.000], mean action: 0.767 [0.000, 2.000], mean observation: 0.075 [-0.150, 0.356], loss: 0.000081, mean_absolute_error: 0.484021, mean_q: 0.725571\n",
      " 45596/50000: episode: 1141, duration: 0.214s, episode steps: 43, steps per second: 201, episode reward: 0.831, mean reward: 0.019 [-0.006, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.133 [-0.190, 0.649], loss: 0.000054, mean_absolute_error: 0.497411, mean_q: 0.747596\n",
      " 45597/50000: episode: 1142, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.091, 0.010], loss: 0.000156, mean_absolute_error: 0.495003, mean_q: 0.733234\n",
      " 45624/50000: episode: 1143, duration: 0.135s, episode steps: 27, steps per second: 199, episode reward: 0.952, mean reward: 0.035 [-0.002, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.062 [-0.100, 0.250], loss: 0.000048, mean_absolute_error: 0.493684, mean_q: 0.744616\n",
      " 45625/50000: episode: 1144, duration: 0.008s, episode steps: 1, steps per second: 119, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.010, 0.084], loss: 0.000039, mean_absolute_error: 0.448311, mean_q: 0.672796\n",
      " 45626/50000: episode: 1145, duration: 0.008s, episode steps: 1, steps per second: 132, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.041 [-0.010, 0.093], loss: 0.000039, mean_absolute_error: 0.475276, mean_q: 0.712162\n",
      " 45641/50000: episode: 1146, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 0.980, mean reward: 0.065 [-0.002, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.039 [-0.172, 0.100], loss: 0.000027, mean_absolute_error: 0.491146, mean_q: 0.738922\n",
      " 45699/50000: episode: 1147, duration: 0.307s, episode steps: 58, steps per second: 189, episode reward: 0.722, mean reward: 0.012 [-0.008, 1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.176 [-0.843, 0.190], loss: 0.000050, mean_absolute_error: 0.496232, mean_q: 0.744303\n",
      " 45743/50000: episode: 1148, duration: 0.207s, episode steps: 44, steps per second: 212, episode reward: 0.874, mean reward: 0.020 [-0.005, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.102 [-0.461, 0.140], loss: 0.000073, mean_absolute_error: 0.497162, mean_q: 0.748339\n",
      " 45809/50000: episode: 1149, duration: 0.340s, episode steps: 66, steps per second: 194, episode reward: 0.640, mean reward: 0.010 [-0.010, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.207 [-0.974, 0.210], loss: 0.000045, mean_absolute_error: 0.492968, mean_q: 0.742238\n",
      " 45855/50000: episode: 1150, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 0.813, mean reward: 0.018 [-0.007, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.137 [-0.220, 0.689], loss: 0.000059, mean_absolute_error: 0.490370, mean_q: 0.738768\n",
      " 45860/50000: episode: 1151, duration: 0.023s, episode steps: 5, steps per second: 220, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.038 [-0.040, 0.110], loss: 0.000041, mean_absolute_error: 0.510402, mean_q: 0.768719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45910/50000: episode: 1152, duration: 0.261s, episode steps: 50, steps per second: 191, episode reward: 0.812, mean reward: 0.016 [-0.006, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.134 [-0.646, 0.170], loss: 0.000066, mean_absolute_error: 0.495185, mean_q: 0.743962\n",
      " 45951/50000: episode: 1153, duration: 0.193s, episode steps: 41, steps per second: 213, episode reward: 0.852, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.125 [-0.200, 0.569], loss: 0.000032, mean_absolute_error: 0.493640, mean_q: 0.743271\n",
      " 45996/50000: episode: 1154, duration: 0.211s, episode steps: 45, steps per second: 214, episode reward: 0.826, mean reward: 0.018 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.135 [-0.180, 0.623], loss: 0.000050, mean_absolute_error: 0.494643, mean_q: 0.742496\n",
      " 46047/50000: episode: 1155, duration: 0.238s, episode steps: 51, steps per second: 214, episode reward: 0.795, mean reward: 0.016 [-0.007, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.147 [-0.650, 0.170], loss: 0.000066, mean_absolute_error: 0.493516, mean_q: 0.743121\n",
      " 46109/50000: episode: 1156, duration: 0.353s, episode steps: 62, steps per second: 176, episode reward: 0.678, mean reward: 0.011 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.196 [-0.900, 0.190], loss: 0.000061, mean_absolute_error: 0.500129, mean_q: 0.751991\n",
      " 46139/50000: episode: 1157, duration: 0.124s, episode steps: 30, steps per second: 241, episode reward: 0.922, mean reward: 0.031 [-0.004, 1.000], mean action: 1.267 [0.000, 2.000], mean observation: -0.082 [-0.399, 0.160], loss: 0.000046, mean_absolute_error: 0.491360, mean_q: 0.736385\n",
      " 46183/50000: episode: 1158, duration: 0.212s, episode steps: 44, steps per second: 208, episode reward: 0.862, mean reward: 0.020 [-0.005, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.112 [-0.481, 0.140], loss: 0.000041, mean_absolute_error: 0.497896, mean_q: 0.749000\n",
      " 46209/50000: episode: 1159, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 0.945, mean reward: 0.036 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.063 [-0.130, 0.312], loss: 0.000030, mean_absolute_error: 0.497253, mean_q: 0.745834\n",
      " 46230/50000: episode: 1160, duration: 0.100s, episode steps: 21, steps per second: 210, episode reward: 0.973, mean reward: 0.046 [-0.002, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.026 [-0.198, 0.110], loss: 0.000197, mean_absolute_error: 0.502180, mean_q: 0.752797\n",
      " 46289/50000: episode: 1161, duration: 0.279s, episode steps: 59, steps per second: 211, episode reward: 0.719, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.172 [-0.860, 0.200], loss: 0.000047, mean_absolute_error: 0.496240, mean_q: 0.746825\n",
      " 46318/50000: episode: 1162, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 0.938, mean reward: 0.032 [-0.003, 1.000], mean action: 1.276 [0.000, 2.000], mean observation: -0.069 [-0.326, 0.120], loss: 0.000048, mean_absolute_error: 0.501866, mean_q: 0.748175\n",
      " 46359/50000: episode: 1163, duration: 0.227s, episode steps: 41, steps per second: 181, episode reward: 0.848, mean reward: 0.021 [-0.006, 1.000], mean action: 0.805 [0.000, 2.000], mean observation: 0.124 [-0.210, 0.609], loss: 0.000037, mean_absolute_error: 0.489162, mean_q: 0.734802\n",
      " 46400/50000: episode: 1164, duration: 0.175s, episode steps: 41, steps per second: 235, episode reward: 0.853, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.116 [-0.190, 0.593], loss: 0.000043, mean_absolute_error: 0.490725, mean_q: 0.737335\n",
      " 46458/50000: episode: 1165, duration: 0.256s, episode steps: 58, steps per second: 227, episode reward: 0.703, mean reward: 0.012 [-0.008, 1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.194 [-0.826, 0.190], loss: 0.000050, mean_absolute_error: 0.496090, mean_q: 0.743853\n",
      " 46507/50000: episode: 1166, duration: 0.218s, episode steps: 49, steps per second: 225, episode reward: 0.796, mean reward: 0.016 [-0.007, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.143 [-0.210, 0.706], loss: 0.000048, mean_absolute_error: 0.489746, mean_q: 0.736565\n",
      " 46508/50000: episode: 1167, duration: 0.008s, episode steps: 1, steps per second: 123, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.009 [-0.010, -0.009], loss: 0.000059, mean_absolute_error: 0.506756, mean_q: 0.744322\n",
      " 46543/50000: episode: 1168, duration: 0.177s, episode steps: 35, steps per second: 197, episode reward: 0.905, mean reward: 0.026 [-0.004, 1.000], mean action: 0.771 [0.000, 2.000], mean observation: 0.092 [-0.130, 0.415], loss: 0.000081, mean_absolute_error: 0.504402, mean_q: 0.759761\n",
      " 46597/50000: episode: 1169, duration: 0.252s, episode steps: 54, steps per second: 215, episode reward: 0.699, mean reward: 0.013 [-0.010, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.199 [-0.250, 0.956], loss: 0.000061, mean_absolute_error: 0.492739, mean_q: 0.741732\n",
      " 46648/50000: episode: 1170, duration: 0.257s, episode steps: 51, steps per second: 198, episode reward: 0.757, mean reward: 0.015 [-0.008, 1.000], mean action: 0.882 [0.000, 2.000], mean observation: 0.169 [-0.240, 0.806], loss: 0.000059, mean_absolute_error: 0.496202, mean_q: 0.746108\n",
      " 46649/50000: episode: 1171, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.027 [0.000, 0.053], loss: 0.000028, mean_absolute_error: 0.518219, mean_q: 0.781501\n",
      " 46686/50000: episode: 1172, duration: 0.149s, episode steps: 37, steps per second: 249, episode reward: 0.919, mean reward: 0.025 [-0.003, 1.000], mean action: 1.162 [0.000, 2.000], mean observation: -0.078 [-0.329, 0.140], loss: 0.000296, mean_absolute_error: 0.487953, mean_q: 0.734486\n",
      " 46687/50000: episode: 1173, duration: 0.008s, episode steps: 1, steps per second: 131, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.042 [-0.010, 0.094], loss: 0.000145, mean_absolute_error: 0.521569, mean_q: 0.791711\n",
      " 46688/50000: episode: 1174, duration: 0.008s, episode steps: 1, steps per second: 127, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.015 [-0.039, 0.010], loss: 0.000120, mean_absolute_error: 0.433030, mean_q: 0.625745\n",
      " 46740/50000: episode: 1175, duration: 0.273s, episode steps: 52, steps per second: 190, episode reward: 0.821, mean reward: 0.016 [-0.006, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: -0.125 [-0.598, 0.180], loss: 0.000070, mean_absolute_error: 0.492886, mean_q: 0.741644\n",
      " 46777/50000: episode: 1176, duration: 0.150s, episode steps: 37, steps per second: 247, episode reward: 0.895, mean reward: 0.024 [-0.004, 1.000], mean action: 1.135 [0.000, 2.000], mean observation: -0.096 [-0.447, 0.140], loss: 0.000052, mean_absolute_error: 0.494310, mean_q: 0.742718\n",
      " 46785/50000: episode: 1177, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.035 [-0.129, 0.080], loss: 0.000058, mean_absolute_error: 0.496913, mean_q: 0.751016\n",
      " 46819/50000: episode: 1178, duration: 0.168s, episode steps: 34, steps per second: 202, episode reward: 0.900, mean reward: 0.026 [-0.005, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.095 [-0.170, 0.461], loss: 0.000057, mean_absolute_error: 0.489268, mean_q: 0.736629\n",
      " 46828/50000: episode: 1179, duration: 0.044s, episode steps: 9, steps per second: 205, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 1.889 [1.000, 2.000], mean observation: -0.036 [-0.133, 0.080], loss: 0.000054, mean_absolute_error: 0.506678, mean_q: 0.764933\n",
      " 46875/50000: episode: 1180, duration: 0.241s, episode steps: 47, steps per second: 195, episode reward: 0.824, mean reward: 0.018 [-0.006, 1.000], mean action: 1.128 [0.000, 2.000], mean observation: -0.131 [-0.635, 0.180], loss: 0.000058, mean_absolute_error: 0.495639, mean_q: 0.745627\n",
      " 46876/50000: episode: 1181, duration: 0.008s, episode steps: 1, steps per second: 133, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.008 [-0.010, -0.006], loss: 0.000046, mean_absolute_error: 0.427551, mean_q: 0.643847\n",
      " 46896/50000: episode: 1182, duration: 0.091s, episode steps: 20, steps per second: 221, episode reward: 0.963, mean reward: 0.048 [-0.003, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.055 [-0.257, 0.120], loss: 0.000053, mean_absolute_error: 0.499609, mean_q: 0.751525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46946/50000: episode: 1183, duration: 0.260s, episode steps: 50, steps per second: 193, episode reward: 0.802, mean reward: 0.016 [-0.007, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.142 [-0.671, 0.180], loss: 0.000076, mean_absolute_error: 0.493689, mean_q: 0.739157\n",
      " 47002/50000: episode: 1184, duration: 0.300s, episode steps: 56, steps per second: 187, episode reward: 0.720, mean reward: 0.013 [-0.009, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.174 [-0.270, 0.867], loss: 0.000229, mean_absolute_error: 0.490956, mean_q: 0.738694\n",
      " 47050/50000: episode: 1185, duration: 0.310s, episode steps: 48, steps per second: 155, episode reward: 0.858, mean reward: 0.018 [-0.005, 1.000], mean action: 1.083 [0.000, 2.000], mean observation: -0.104 [-0.524, 0.150], loss: 0.000086, mean_absolute_error: 0.499157, mean_q: 0.750268\n",
      " 47096/50000: episode: 1186, duration: 0.259s, episode steps: 46, steps per second: 178, episode reward: 0.821, mean reward: 0.018 [-0.006, 1.000], mean action: 0.848 [0.000, 2.000], mean observation: 0.137 [-0.200, 0.630], loss: 0.000054, mean_absolute_error: 0.495402, mean_q: 0.744490\n",
      " 47115/50000: episode: 1187, duration: 0.087s, episode steps: 19, steps per second: 218, episode reward: 0.967, mean reward: 0.051 [-0.002, 1.000], mean action: 0.579 [0.000, 2.000], mean observation: 0.050 [-0.130, 0.236], loss: 0.000039, mean_absolute_error: 0.493049, mean_q: 0.741184\n",
      " 47169/50000: episode: 1188, duration: 0.278s, episode steps: 54, steps per second: 194, episode reward: 0.796, mean reward: 0.015 [-0.007, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.133 [-0.679, 0.180], loss: 0.000050, mean_absolute_error: 0.496580, mean_q: 0.745989\n",
      " 47171/50000: episode: 1189, duration: 0.011s, episode steps: 2, steps per second: 177, episode reward: 0.999, mean reward: 0.499 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.042 [-0.101, 0.020], loss: 0.000062, mean_absolute_error: 0.475409, mean_q: 0.719587\n",
      " 47228/50000: episode: 1190, duration: 0.325s, episode steps: 57, steps per second: 176, episode reward: 0.712, mean reward: 0.012 [-0.008, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.187 [-0.220, 0.843], loss: 0.000042, mean_absolute_error: 0.494551, mean_q: 0.743330\n",
      " 47261/50000: episode: 1191, duration: 0.188s, episode steps: 33, steps per second: 176, episode reward: 0.927, mean reward: 0.028 [-0.003, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.072 [-0.347, 0.120], loss: 0.000060, mean_absolute_error: 0.496860, mean_q: 0.744604\n",
      " 47311/50000: episode: 1192, duration: 0.282s, episode steps: 50, steps per second: 177, episode reward: 0.817, mean reward: 0.016 [-0.006, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.133 [-0.615, 0.180], loss: 0.000070, mean_absolute_error: 0.495613, mean_q: 0.746124\n",
      " 47354/50000: episode: 1193, duration: 0.203s, episode steps: 43, steps per second: 212, episode reward: 0.852, mean reward: 0.020 [-0.005, 1.000], mean action: 0.814 [0.000, 2.000], mean observation: 0.120 [-0.190, 0.548], loss: 0.000086, mean_absolute_error: 0.496234, mean_q: 0.745339\n",
      " 47396/50000: episode: 1194, duration: 0.182s, episode steps: 42, steps per second: 230, episode reward: 0.856, mean reward: 0.020 [-0.006, 1.000], mean action: 0.929 [0.000, 2.000], mean observation: 0.117 [-0.210, 0.572], loss: 0.000074, mean_absolute_error: 0.496179, mean_q: 0.742776\n",
      " 47397/50000: episode: 1195, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.035 [-0.081, 0.010], loss: 0.000047, mean_absolute_error: 0.492422, mean_q: 0.742633\n",
      " 47398/50000: episode: 1196, duration: 0.008s, episode steps: 1, steps per second: 130, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.001 [-0.010, 0.008], loss: 0.000011, mean_absolute_error: 0.457444, mean_q: 0.670120\n",
      " 47453/50000: episode: 1197, duration: 0.345s, episode steps: 55, steps per second: 160, episode reward: 0.710, mean reward: 0.013 [-0.009, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.192 [-0.250, 0.890], loss: 0.000064, mean_absolute_error: 0.492705, mean_q: 0.740829\n",
      " 47516/50000: episode: 1198, duration: 0.344s, episode steps: 63, steps per second: 183, episode reward: 0.661, mean reward: 0.010 [-0.010, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.202 [-0.953, 0.180], loss: 0.000050, mean_absolute_error: 0.493665, mean_q: 0.743096\n",
      " 47556/50000: episode: 1199, duration: 0.171s, episode steps: 40, steps per second: 234, episode reward: 0.847, mean reward: 0.021 [-0.006, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.128 [-0.220, 0.607], loss: 0.000054, mean_absolute_error: 0.491132, mean_q: 0.739538\n",
      " 47609/50000: episode: 1200, duration: 0.251s, episode steps: 53, steps per second: 211, episode reward: 0.789, mean reward: 0.015 [-0.007, 1.000], mean action: 1.057 [0.000, 2.000], mean observation: -0.144 [-0.688, 0.170], loss: 0.000057, mean_absolute_error: 0.491461, mean_q: 0.737969\n",
      " 47667/50000: episode: 1201, duration: 0.249s, episode steps: 58, steps per second: 233, episode reward: 0.761, mean reward: 0.013 [-0.007, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.155 [-0.699, 0.180], loss: 0.000060, mean_absolute_error: 0.491598, mean_q: 0.738774\n",
      " 47742/50000: episode: 1202, duration: 0.351s, episode steps: 75, steps per second: 214, episode reward: 0.609, mean reward: 0.008 [-0.010, 1.000], mean action: 1.040 [0.000, 2.000], mean observation: -0.204 [-0.956, 0.190], loss: 0.000051, mean_absolute_error: 0.495362, mean_q: 0.743709\n",
      " 47782/50000: episode: 1203, duration: 0.217s, episode steps: 40, steps per second: 184, episode reward: 0.869, mean reward: 0.022 [-0.005, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.112 [-0.522, 0.180], loss: 0.000053, mean_absolute_error: 0.491955, mean_q: 0.739766\n",
      " 47783/50000: episode: 1204, duration: 0.007s, episode steps: 1, steps per second: 150, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.009 [-0.028, 0.010], loss: 0.000012, mean_absolute_error: 0.478340, mean_q: 0.723849\n",
      " 47822/50000: episode: 1205, duration: 0.162s, episode steps: 39, steps per second: 241, episode reward: 0.877, mean reward: 0.022 [-0.005, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.109 [-0.160, 0.489], loss: 0.000063, mean_absolute_error: 0.494169, mean_q: 0.740498\n",
      " 47841/50000: episode: 1206, duration: 0.088s, episode steps: 19, steps per second: 217, episode reward: 0.970, mean reward: 0.051 [-0.002, 1.000], mean action: 1.316 [0.000, 2.000], mean observation: -0.050 [-0.220, 0.090], loss: 0.000052, mean_absolute_error: 0.496297, mean_q: 0.748933\n",
      " 47842/50000: episode: 1207, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.003 [-0.017, 0.010], loss: 0.000047, mean_absolute_error: 0.508040, mean_q: 0.768472\n",
      " 47902/50000: episode: 1208, duration: 0.297s, episode steps: 60, steps per second: 202, episode reward: 0.745, mean reward: 0.012 [-0.008, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.156 [-0.782, 0.180], loss: 0.000056, mean_absolute_error: 0.495750, mean_q: 0.744433\n",
      " 47956/50000: episode: 1209, duration: 0.428s, episode steps: 54, steps per second: 126, episode reward: 0.706, mean reward: 0.013 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.194 [-0.260, 0.921], loss: 0.000058, mean_absolute_error: 0.494052, mean_q: 0.743149\n",
      " 48005/50000: episode: 1210, duration: 0.283s, episode steps: 49, steps per second: 173, episode reward: 0.828, mean reward: 0.017 [-0.006, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.120 [-0.615, 0.190], loss: 0.000057, mean_absolute_error: 0.493404, mean_q: 0.740898\n",
      " 48046/50000: episode: 1211, duration: 0.240s, episode steps: 41, steps per second: 171, episode reward: 0.909, mean reward: 0.022 [-0.003, 1.000], mean action: 1.171 [0.000, 2.000], mean observation: -0.082 [-0.339, 0.130], loss: 0.000065, mean_absolute_error: 0.492101, mean_q: 0.740261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48103/50000: episode: 1212, duration: 0.280s, episode steps: 57, steps per second: 204, episode reward: 0.750, mean reward: 0.013 [-0.008, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.159 [-0.765, 0.180], loss: 0.000129, mean_absolute_error: 0.493287, mean_q: 0.740625\n",
      " 48161/50000: episode: 1213, duration: 0.280s, episode steps: 58, steps per second: 207, episode reward: 0.682, mean reward: 0.012 [-0.010, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.197 [-0.250, 0.984], loss: 0.000068, mean_absolute_error: 0.489790, mean_q: 0.737001\n",
      " 48207/50000: episode: 1214, duration: 0.314s, episode steps: 46, steps per second: 147, episode reward: 0.820, mean reward: 0.018 [-0.007, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.125 [-0.220, 0.686], loss: 0.000273, mean_absolute_error: 0.497461, mean_q: 0.747316\n",
      " 48248/50000: episode: 1215, duration: 0.166s, episode steps: 41, steps per second: 247, episode reward: 0.847, mean reward: 0.021 [-0.006, 1.000], mean action: 0.829 [0.000, 2.000], mean observation: 0.125 [-0.210, 0.607], loss: 0.000089, mean_absolute_error: 0.489159, mean_q: 0.734939\n",
      " 48297/50000: episode: 1216, duration: 0.264s, episode steps: 49, steps per second: 185, episode reward: 0.868, mean reward: 0.018 [-0.005, 1.000], mean action: 1.102 [0.000, 2.000], mean observation: -0.096 [-0.489, 0.140], loss: 0.000043, mean_absolute_error: 0.498949, mean_q: 0.749202\n",
      " 48345/50000: episode: 1217, duration: 0.238s, episode steps: 48, steps per second: 201, episode reward: 0.797, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.152 [-0.672, 0.160], loss: 0.000052, mean_absolute_error: 0.487593, mean_q: 0.730778\n",
      " 48354/50000: episode: 1218, duration: 0.072s, episode steps: 9, steps per second: 124, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.090, 0.139], loss: 0.000038, mean_absolute_error: 0.494994, mean_q: 0.745484\n",
      " 48395/50000: episode: 1219, duration: 0.212s, episode steps: 41, steps per second: 194, episode reward: 0.864, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.104 [-0.220, 0.571], loss: 0.000057, mean_absolute_error: 0.491769, mean_q: 0.739102\n",
      " 48443/50000: episode: 1220, duration: 0.263s, episode steps: 48, steps per second: 183, episode reward: 0.772, mean reward: 0.016 [-0.008, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.167 [-0.240, 0.778], loss: 0.000075, mean_absolute_error: 0.490206, mean_q: 0.736630\n",
      " 48504/50000: episode: 1221, duration: 0.274s, episode steps: 61, steps per second: 223, episode reward: 0.727, mean reward: 0.012 [-0.008, 1.000], mean action: 1.082 [0.000, 2.000], mean observation: -0.167 [-0.800, 0.200], loss: 0.000124, mean_absolute_error: 0.497739, mean_q: 0.745492\n",
      " 48562/50000: episode: 1222, duration: 0.267s, episode steps: 58, steps per second: 217, episode reward: 0.680, mean reward: 0.012 [-0.010, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.200 [-0.270, 0.954], loss: 0.000055, mean_absolute_error: 0.495026, mean_q: 0.742830\n",
      " 48607/50000: episode: 1223, duration: 0.185s, episode steps: 45, steps per second: 243, episode reward: 0.813, mean reward: 0.018 [-0.007, 1.000], mean action: 0.822 [0.000, 2.000], mean observation: 0.143 [-0.230, 0.691], loss: 0.000076, mean_absolute_error: 0.495277, mean_q: 0.743311\n",
      " 48626/50000: episode: 1224, duration: 0.091s, episode steps: 19, steps per second: 210, episode reward: 0.970, mean reward: 0.051 [-0.002, 1.000], mean action: 1.211 [0.000, 2.000], mean observation: -0.048 [-0.219, 0.110], loss: 0.000043, mean_absolute_error: 0.489858, mean_q: 0.737360\n",
      " 48636/50000: episode: 1225, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 0.990, mean reward: 0.099 [-0.001, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.043 [-0.124, 0.050], loss: 0.000071, mean_absolute_error: 0.484556, mean_q: 0.730436\n",
      " 48637/50000: episode: 1226, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.009 [-0.010, -0.008], loss: 0.000028, mean_absolute_error: 0.478135, mean_q: 0.721938\n",
      " 48684/50000: episode: 1227, duration: 0.234s, episode steps: 47, steps per second: 201, episode reward: 0.768, mean reward: 0.016 [-0.008, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.171 [-0.240, 0.820], loss: 0.000059, mean_absolute_error: 0.495691, mean_q: 0.745972\n",
      " 48738/50000: episode: 1228, duration: 0.224s, episode steps: 54, steps per second: 241, episode reward: 0.714, mean reward: 0.013 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.190 [-0.240, 0.909], loss: 0.000043, mean_absolute_error: 0.491095, mean_q: 0.738555\n",
      " 48739/50000: episode: 1229, duration: 0.007s, episode steps: 1, steps per second: 134, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.009 [-0.010, -0.008], loss: 0.000008, mean_absolute_error: 0.496415, mean_q: 0.749208\n",
      " 48763/50000: episode: 1230, duration: 0.100s, episode steps: 24, steps per second: 241, episode reward: 0.967, mean reward: 0.040 [-0.002, 1.000], mean action: 1.083 [0.000, 2.000], mean observation: -0.054 [-0.182, 0.050], loss: 0.000049, mean_absolute_error: 0.499934, mean_q: 0.748865\n",
      " 48808/50000: episode: 1231, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 0.856, mean reward: 0.019 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.102 [-0.210, 0.576], loss: 0.000051, mean_absolute_error: 0.488166, mean_q: 0.736300\n",
      " 48846/50000: episode: 1232, duration: 0.178s, episode steps: 38, steps per second: 213, episode reward: 0.878, mean reward: 0.023 [-0.005, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.101 [-0.532, 0.180], loss: 0.000059, mean_absolute_error: 0.492933, mean_q: 0.740345\n",
      " 48880/50000: episode: 1233, duration: 0.150s, episode steps: 34, steps per second: 227, episode reward: 0.913, mean reward: 0.027 [-0.004, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.086 [-0.130, 0.387], loss: 0.000071, mean_absolute_error: 0.489647, mean_q: 0.737315\n",
      " 48886/50000: episode: 1234, duration: 0.028s, episode steps: 6, steps per second: 218, episode reward: 0.995, mean reward: 0.166 [-0.001, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.039 [-0.050, 0.114], loss: 0.000055, mean_absolute_error: 0.480732, mean_q: 0.726999\n",
      " 48920/50000: episode: 1235, duration: 0.153s, episode steps: 34, steps per second: 222, episode reward: 0.894, mean reward: 0.026 [-0.005, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.098 [-0.170, 0.490], loss: 0.000055, mean_absolute_error: 0.497445, mean_q: 0.749095\n",
      " 48959/50000: episode: 1236, duration: 0.174s, episode steps: 39, steps per second: 225, episode reward: 0.912, mean reward: 0.023 [-0.004, 1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.077 [-0.379, 0.140], loss: 0.000074, mean_absolute_error: 0.500509, mean_q: 0.754846\n",
      " 48986/50000: episode: 1237, duration: 0.116s, episode steps: 27, steps per second: 234, episode reward: 0.941, mean reward: 0.035 [-0.003, 1.000], mean action: 0.704 [0.000, 2.000], mean observation: 0.068 [-0.130, 0.320], loss: 0.000047, mean_absolute_error: 0.488540, mean_q: 0.736748\n",
      " 49008/50000: episode: 1238, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 0.966, mean reward: 0.044 [-0.002, 1.000], mean action: 1.409 [0.000, 2.000], mean observation: -0.053 [-0.206, 0.100], loss: 0.000152, mean_absolute_error: 0.494131, mean_q: 0.743348\n",
      " 49009/50000: episode: 1239, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.031 [0.010, 0.052], loss: 0.000058, mean_absolute_error: 0.491471, mean_q: 0.738593\n",
      " 49059/50000: episode: 1240, duration: 0.215s, episode steps: 50, steps per second: 233, episode reward: 0.797, mean reward: 0.016 [-0.007, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.136 [-0.230, 0.718], loss: 0.000065, mean_absolute_error: 0.492573, mean_q: 0.743311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49120/50000: episode: 1241, duration: 0.276s, episode steps: 61, steps per second: 221, episode reward: 0.750, mean reward: 0.012 [-0.008, 1.000], mean action: 1.082 [0.000, 2.000], mean observation: -0.151 [-0.760, 0.170], loss: 0.000059, mean_absolute_error: 0.495751, mean_q: 0.747612\n",
      " 49152/50000: episode: 1242, duration: 0.134s, episode steps: 32, steps per second: 238, episode reward: 0.902, mean reward: 0.028 [-0.005, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.096 [-0.180, 0.470], loss: 0.000054, mean_absolute_error: 0.482246, mean_q: 0.727015\n",
      " 49200/50000: episode: 1243, duration: 0.194s, episode steps: 48, steps per second: 248, episode reward: 0.850, mean reward: 0.018 [-0.006, 1.000], mean action: 1.104 [0.000, 2.000], mean observation: -0.110 [-0.553, 0.180], loss: 0.000057, mean_absolute_error: 0.485653, mean_q: 0.732664\n",
      " 49258/50000: episode: 1244, duration: 0.248s, episode steps: 58, steps per second: 234, episode reward: 0.710, mean reward: 0.012 [-0.009, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.179 [-0.250, 0.867], loss: 0.000119, mean_absolute_error: 0.496142, mean_q: 0.748358\n",
      " 49278/50000: episode: 1245, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 0.963, mean reward: 0.048 [-0.003, 1.000], mean action: 0.650 [0.000, 2.000], mean observation: 0.055 [-0.130, 0.259], loss: 0.000079, mean_absolute_error: 0.494821, mean_q: 0.748705\n",
      " 49312/50000: episode: 1246, duration: 0.144s, episode steps: 34, steps per second: 236, episode reward: 0.929, mean reward: 0.027 [-0.003, 1.000], mean action: 1.118 [0.000, 2.000], mean observation: -0.074 [-0.317, 0.130], loss: 0.000062, mean_absolute_error: 0.499116, mean_q: 0.754140\n",
      " 49358/50000: episode: 1247, duration: 0.229s, episode steps: 46, steps per second: 201, episode reward: 0.794, mean reward: 0.017 [-0.008, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.154 [-0.220, 0.750], loss: 0.000053, mean_absolute_error: 0.498543, mean_q: 0.753022\n",
      " 49376/50000: episode: 1248, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 0.972, mean reward: 0.054 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.049 [-0.100, 0.210], loss: 0.000032, mean_absolute_error: 0.500112, mean_q: 0.754756\n",
      " 49443/50000: episode: 1249, duration: 0.319s, episode steps: 67, steps per second: 210, episode reward: 0.704, mean reward: 0.011 [-0.008, 1.000], mean action: 1.090 [0.000, 2.000], mean observation: -0.167 [-0.834, 0.180], loss: 0.000052, mean_absolute_error: 0.492695, mean_q: 0.743720\n",
      " 49444/50000: episode: 1250, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.002 [-0.010, 0.007], loss: 0.000017, mean_absolute_error: 0.460456, mean_q: 0.698817\n",
      " 49515/50000: episode: 1251, duration: 0.351s, episode steps: 71, steps per second: 202, episode reward: 0.668, mean reward: 0.009 [-0.009, 1.000], mean action: 1.085 [0.000, 2.000], mean observation: -0.179 [-0.890, 0.170], loss: 0.000055, mean_absolute_error: 0.497819, mean_q: 0.751842\n",
      " 49573/50000: episode: 1252, duration: 0.435s, episode steps: 58, steps per second: 133, episode reward: 0.763, mean reward: 0.013 [-0.007, 1.000], mean action: 1.052 [0.000, 2.000], mean observation: -0.149 [-0.750, 0.190], loss: 0.000073, mean_absolute_error: 0.495340, mean_q: 0.748404\n",
      " 49628/50000: episode: 1253, duration: 0.223s, episode steps: 55, steps per second: 247, episode reward: 0.704, mean reward: 0.013 [-0.009, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.193 [-0.250, 0.946], loss: 0.000065, mean_absolute_error: 0.496009, mean_q: 0.748239\n",
      " 49629/50000: episode: 1254, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [-0.010, 0.016], loss: 0.000065, mean_absolute_error: 0.477631, mean_q: 0.718269\n",
      " 49650/50000: episode: 1255, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 0.966, mean reward: 0.046 [-0.002, 1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.052 [-0.218, 0.090], loss: 0.000039, mean_absolute_error: 0.499103, mean_q: 0.753862\n",
      " 49691/50000: episode: 1256, duration: 0.245s, episode steps: 41, steps per second: 167, episode reward: 0.904, mean reward: 0.022 [-0.004, 1.000], mean action: 1.122 [0.000, 2.000], mean observation: -0.086 [-0.370, 0.130], loss: 0.000063, mean_absolute_error: 0.501239, mean_q: 0.756532\n",
      " 49742/50000: episode: 1257, duration: 0.239s, episode steps: 51, steps per second: 213, episode reward: 0.830, mean reward: 0.016 [-0.006, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.117 [-0.608, 0.170], loss: 0.000052, mean_absolute_error: 0.501203, mean_q: 0.756660\n",
      " 49772/50000: episode: 1258, duration: 0.141s, episode steps: 30, steps per second: 213, episode reward: 0.937, mean reward: 0.031 [-0.003, 1.000], mean action: 0.733 [0.000, 2.000], mean observation: 0.072 [-0.110, 0.303], loss: 0.000070, mean_absolute_error: 0.501387, mean_q: 0.757411\n",
      " 49789/50000: episode: 1259, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 0.981, mean reward: 0.058 [-0.001, 1.000], mean action: 0.706 [0.000, 2.000], mean observation: 0.048 [-0.050, 0.130], loss: 0.000046, mean_absolute_error: 0.493459, mean_q: 0.743756\n",
      " 49813/50000: episode: 1260, duration: 0.098s, episode steps: 24, steps per second: 244, episode reward: 0.954, mean reward: 0.040 [-0.003, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.053 [-0.140, 0.290], loss: 0.000069, mean_absolute_error: 0.498804, mean_q: 0.752296\n",
      " 49853/50000: episode: 1261, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 0.872, mean reward: 0.022 [-0.005, 1.000], mean action: 0.825 [0.000, 2.000], mean observation: 0.109 [-0.170, 0.504], loss: 0.000062, mean_absolute_error: 0.494281, mean_q: 0.745999\n",
      " 49886/50000: episode: 1262, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 0.919, mean reward: 0.028 [-0.004, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.079 [-0.120, 0.375], loss: 0.000059, mean_absolute_error: 0.501008, mean_q: 0.756067\n",
      " 49914/50000: episode: 1263, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 0.936, mean reward: 0.033 [-0.003, 1.000], mean action: 0.714 [0.000, 2.000], mean observation: 0.074 [-0.150, 0.325], loss: 0.000052, mean_absolute_error: 0.499492, mean_q: 0.753143\n",
      " 49915/50000: episode: 1264, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.025 [0.000, 0.049], loss: 0.000032, mean_absolute_error: 0.473069, mean_q: 0.715866\n",
      " 49955/50000: episode: 1265, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 0.901, mean reward: 0.023 [-0.004, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.088 [-0.399, 0.130], loss: 0.000066, mean_absolute_error: 0.502114, mean_q: 0.757561\n",
      "done, took 224.685 seconds\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "env = PointOnLine()\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# DQNのネットワーク定義\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# experience replay用のmemory\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "# 行動方策はオーソドックスなepsilon-greedy。ほかに、各行動のQ値によって確率を決定するBoltzmannQPolicyが利用可能\n",
    "policy = EpsGreedyQPolicy(eps=0.1) \n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2, nb_max_episode_steps=300)\n",
    "#学習の様子を描画したいときは、Envに_render()を実装して、visualize=True にします,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 0.932, steps: 32\n",
      "Episode 2: reward: 0.929, steps: 29\n",
      "Episode 3: reward: 0.714, steps: 53\n",
      "Episode 4: reward: 0.697, steps: 55\n",
      "Episode 5: reward: 0.770, steps: 55\n",
      "Episode 6: reward: 0.790, steps: 53\n",
      "Episode 7: reward: 0.693, steps: 55\n",
      "Episode 8: reward: 0.944, steps: 31\n",
      "Episode 9: reward: 0.977, steps: 16\n",
      "Episode 10: reward: 0.921, steps: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1c60944bc88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFBpQlFJKbYr/D9Y5LwewJdchAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c60921de48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import rl.callbacks\n",
    "class EpisodeLogger(rl.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "\n",
    "cb_ep = EpisodeLogger()\n",
    "dqn.test(env, nb_episodes=10, visualize=False, callbacks=[cb_ep])\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for obs in cb_ep.observations.values():\n",
    "    plt.plot([o[0] for o in obs])\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cralybot",
   "language": "python",
   "name": "cralybot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
